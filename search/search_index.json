{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Dynamiq is an orchestration framework for agentic AI and LLM applications </p> <p> </p> <p>Welcome to Dynamiq! \ud83e\udd16</p> <p>Dynamiq is your all-in-one Gen AI framework, designed to streamline the development of AI-powered applications. Dynamiq specializes in orchestrating retrieval-augmented generation (RAG) and large language model (LLM) agents.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Here's how you can get started with Dynamiq:</p>"},{"location":"#installation","title":"Installation","text":"<p>First, let's get Dynamiq installed. You'll need Python, so make sure that's set up on your machine. Then run:</p> <pre><code>pip install dynamiq\n</code></pre> <p>Or build the Python package from the source code: <pre><code>git clone https://github.com/dynamiq-ai/dynamiq.git\ncd dynamiq\npoetry install\n</code></pre></p>"},{"location":"#documentation","title":"Documentation","text":"<p>For more examples and detailed guides, please refer to our documentation.</p>"},{"location":"#examples","title":"Examples","text":""},{"location":"#simple-llm-flow","title":"Simple LLM Flow","text":"<p>Here's a simple example to get you started with Dynamiq:</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq import Workflow\nfrom dynamiq.prompts import Prompt, Message\n\n# Define the prompt template for translation\nprompt_template = \"\"\"\nTranslate the following text into English: {{ text }}\n\"\"\"\n\n# Create a Prompt object with the defined template\nprompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n\n# Setup your LLM (Large Language Model) Node\nllm = OpenAI(\n    id=\"openai\",  # Unique identifier for the node\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),  # Connection using API key\n    model=\"gpt-4o\",  # Model to be used\n    temperature=0.3,  # Sampling temperature for the model\n    max_tokens=1000,  # Maximum number of tokens in the output\n    prompt=prompt  # Prompt to be used for the model\n)\n\n# Create a Workflow object\nworkflow = Workflow()\n\n# Add the LLM node to the workflow\nworkflow.flow.add_nodes(llm)\n\n# Run the workflow with the input data\nresult = workflow.run(\n    input_data={\n        \"text\": \"Hola Mundo!\"  # Text to be translated\n    }\n)\n\n# Print the result of the translation\nprint(result.output)\n</code></pre>"},{"location":"#simple-react-agent","title":"Simple ReAct Agent","text":"<p>An agent that has the access to E2B Code Interpreter and is capable of solving complex coding tasks.</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection, E2B as E2BConnection\nfrom dynamiq.nodes.agents.react import ReActAgent\nfrom dynamiq.nodes.tools.e2b_sandbox import E2BInterpreterTool\n\n# Initialize the E2B tool\ne2b_tool = E2BInterpreterTool(\n    connection=E2BConnection(api_key=\"$API_KEY\")\n)\n\n# Setup your LLM\nllm = OpenAI(\n    id=\"openai\",\n    connection=OpenAIConnection(api_key=\"$API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.3,\n    max_tokens=1000,\n)\n\n# Create the ReAct agent\nagent = ReActAgent(\n    name=\"react-agent\",\n    llm=llm,\n    tools=[e2b_tool],\n    role=\"Senior Data Scientist\",\n    max_loops=10,\n)\n\n# Run the agent with an input\nresult = agent.run(\n    input_data={\n        \"input\": \"Add the first 10 numbers and tell if the result is prime.\",\n    }\n)\n\nprint(result.output.get(\"content\"))\n</code></pre>"},{"location":"#multi-agent-orchestration","title":"Multi-agent orchestration","text":"<pre><code>from dynamiq.connections import (OpenAI as OpenAIConnection,\n                                 ScaleSerp as ScaleSerpConnection,\n                                 E2B as E2BConnection)\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.nodes.agents.orchestrators.adaptive import AdaptiveOrchestrator\nfrom dynamiq.nodes.agents.orchestrators.adaptive_manager import AdaptiveAgentManager\nfrom dynamiq.nodes.agents.react import ReActAgent\nfrom dynamiq.nodes.agents.reflection import ReflectionAgent\nfrom dynamiq.nodes.tools.e2b_sandbox import E2BInterpreterTool\nfrom dynamiq.nodes.tools.scale_serp import ScaleSerpTool\n\n\n# Initialize tools\npython_tool = E2BInterpreterTool(\n    connection=E2BConnection(api_key=\"$E2B_API_KEY\")\n)\nsearch_tool = ScaleSerpTool(\n    connection=ScaleSerpConnection(api_key=\"$SCALESERP_API_KEY\")\n)\n\n# Initialize LLM\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\n# Define agents\ncoding_agent = ReActAgent(\n    name=\"coding-agent\",\n    llm=llm,\n    tools=[python_tool],\n    role=(\"Expert agent with coding skills.\"\n          \"Goal is to provide the solution to the input task\"\n          \"using Python software engineering skills.\"),\n    max_loops=15,\n)\n\nplanner_agent = ReflectionAgent(\n    name=\"planner-agent\",\n    llm=llm,\n    role=(\"Expert agent with planning skills.\"\n          \"Goal is to analyze complex requests\"\n          \"and provide a detailed action plan.\"),\n)\n\nsearch_agent = ReActAgent(\n    name=\"search-agent\",\n    llm=llm,\n    tools=[search_tool],\n    role=(\"Expert agent with web search skills.\"\n          \"Goal is to provide the solution to the input task\"\n          \"using web search and summarization skills.\"),\n    max_loops=10,\n)\n\n# Initialize the adaptive agent manager\nagent_manager = AdaptiveAgentManager(llm=llm)\n\n# Create the orchestrator\norchestrator = AdaptiveOrchestrator(\n    name=\"adaptive-orchestrator\",\n    agents=[coding_agent, planner_agent, search_agent],\n    manager=agent_manager,\n)\n\n# Define the input task\ninput_task = (\n    \"Use coding skills to gather data about Nvidia and Intel stock prices for the last 10 years, \"\n    \"calculate the average per year for each company, and create a table. Then craft a report \"\n    \"and add a conclusion: what would have been better if I had invested $100 ten years ago?\"\n)\n\n# Run the orchestrator\nresult = orchestrator.run(\n    input_data={\"input\": input_task},\n)\n\n# Print the result\nprint(result.output.get(\"content\"))\n</code></pre>"},{"location":"#rag-document-indexing-flow","title":"RAG - document indexing flow","text":"<p>This workflow takes input PDF files, pre-processes them, converts them to vector embeddings, and stores them in the Pinecone vector database. The example provided is for an existing index in Pinecone. You can find examples for index creation on the <code>docs/tutorials/rag</code> page.</p> <pre><code>from io import BytesIO\n\nfrom dynamiq import Workflow\nfrom dynamiq.connections import OpenAI as OpenAIConnection, Pinecone as PineconeConnection\nfrom dynamiq.nodes.converters import PyPDFConverter\nfrom dynamiq.nodes.splitters.document import DocumentSplitter\nfrom dynamiq.nodes.embedders import OpenAIDocumentEmbedder\nfrom dynamiq.nodes.writers import PineconeDocumentWriter\n\nrag_wf = Workflow()\n\n# PyPDF document converter\nconverter = PyPDFConverter(document_creation_mode=\"one-doc-per-page\")\nrag_wf.flow.add_nodes(converter)  # add node to the DAG\n\n# Document splitter\ndocument_splitter = (\n    DocumentSplitter(\n        split_by=\"sentence\",\n        split_length=10,\n        split_overlap=1,\n    )\n    .inputs(documents=converter.outputs.documents)  # map converter node output to the expected input of the current node\n    .depends_on(converter)\n)\nrag_wf.flow.add_nodes(document_splitter)\n\n# OpenAI vector embeddings\nembedder = (\n    OpenAIDocumentEmbedder(\n        connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n        model=\"text-embedding-3-small\",\n    )\n    .inputs(documents=document_splitter.outputs.documents)\n    .depends_on(document_splitter)\n)\nrag_wf.flow.add_nodes(embedder)\n\n# Pinecone vector storage\nvector_store = (\n    PineconeDocumentWriter(\n        connection=PineconeConnection(api_key=\"$PINECONE_API_KEY\"),\n        index_name=\"default\",\n        dimension=1536,\n    )\n    .inputs(documents=embedder.outputs.documents)\n    .depends_on(embedder)\n)\nrag_wf.flow.add_nodes(vector_store)\n\n# Prepare input PDF files\nfile_paths = [\"example.pdf\"]\ninput_data = {\n    \"files\": [\n        BytesIO(open(path, \"rb\").read()) for path in file_paths\n    ],\n    \"metadata\": [\n        {\"filename\": path} for path in file_paths\n    ],\n}\n\n# Run RAG indexing flow\nrag_wf.run(input_data=input_data)\n</code></pre>"},{"location":"#rag-document-retrieval-flow","title":"RAG - document retrieval flow","text":"<p>Simple retrieval RAG flow that searches for relevant documents and answers the original user question using retrieved documents.</p> <pre><code>from dynamiq import Workflow\nfrom dynamiq.nodes import InputTransformer\nfrom dynamiq.connections import OpenAI as OpenAIConnection, Pinecone as PineconeConnection\nfrom dynamiq.nodes.embedders import OpenAITextEmbedder\nfrom dynamiq.nodes.retrievers import PineconeDocumentRetriever\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.prompts import Message, Prompt\n\n# Initialize the RAG retrieval workflow\nretrieval_wf = Workflow()\n\n# Shared OpenAI connection\nopenai_connection = OpenAIConnection(api_key=\"$OPENAI_API_KEY\")\n\n# OpenAI text embedder for query embedding\nembedder = OpenAITextEmbedder(\n    connection=openai_connection,\n    model=\"text-embedding-3-small\",\n)\nretrieval_wf.flow.add_nodes(embedder)\n\n# Pinecone document retriever\ndocument_retriever = (\n    PineconeDocumentRetriever(\n        connection=PineconeConnection(api_key=\"$PINECONE_API_KEY\"),\n        index_name=\"default\",\n        dimension=1536,\n        top_k=5,\n    )\n    .inputs(embedding=embedder.outputs.embedding)\n    .depends_on(embedder)\n)\nretrieval_wf.flow.add_nodes(document_retriever)\n\n# Define the prompt template\nprompt_template = \"\"\"\nPlease answer the question based on the provided context.\n\nQuestion: {{ query }}\n\nContext:\n{% for document in documents %}\n- {{ document.content }}\n{% endfor %}\n\n\"\"\"\n\n# OpenAI LLM for answer generation\nprompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n\nanswer_generator = (\n    OpenAI(\n        connection=openai_connection,\n        model=\"gpt-4o\",\n        prompt=prompt,\n    )\n    .inputs(\n        documents=document_retriever.outputs.documents,\n        query=embedder.outputs.query,\n    )  # take documents from the vector store node and query from the embedder\n    .depends_on([document_retriever, embedder])\n)\nretrieval_wf.flow.add_nodes(answer_generator)\n\n# Run the RAG retrieval flow\nquestion = \"What are the line intems provided in the invoice?\"\nresult = retrieval_wf.run(input_data={\"query\": question})\n\nanswer = result.output.get(answer_generator.id).get(\"output\", {}).get(\"content\")\nprint(answer)\n</code></pre>"},{"location":"#simple-chatbot-with-memory","title":"Simple Chatbot with Memory","text":"<p>A simple chatbot that uses the <code>Memory</code> module to store and retrieve conversation history.</p> <pre><code>from dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq.memory import Memory\nfrom dynamiq.memory.backend.in_memory import InMemory\nfrom dynamiq.nodes.agents.simple import SimpleAgent\nfrom dynamiq.nodes.llms import OpenAI\n\nAGENT_ROLE = \"helpful assistant, goal is to provide useful information and answer questions\"\nllm = OpenAI(\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.1,\n)\n\nmemory = Memory(backend=InMemory())\nagent = SimpleAgent(\n    name=\"Agent\",\n    llm=llm,\n    role=AGENT_ROLE,\n    id=\"agent\",\n    memory=memory,\n)\n\n\ndef main():\n    print(\"Welcome to the AI Chat! (Type 'exit' to end)\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            break\n\n        response = agent.run({\"input\": user_input})\n        response_content = response.output.get(\"content\")\n        print(f\"AI: {response_content}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We love contributions! Whether it's bug reports, feature requests, or pull requests, head over to our CONTRIBUTING.md to see how you can help.</p>"},{"location":"#license","title":"License","text":"<p>Dynamiq is open-source and available under the Apache 2 License.</p> <p>Happy coding! \ud83d\ude80</p>"},{"location":"dynamiq/cache/codecs/","title":"Codecs","text":""},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.Base64Codec","title":"<code>Base64Codec</code>","text":"<p>               Bases: <code>BaseCodec</code></p> <p>Base64 encoding and decoding implementation.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>class Base64Codec(BaseCodec):\n    \"\"\"Base64 encoding and decoding implementation.\"\"\"\n\n    def encode(self, value: str) -&gt; str:\n        \"\"\"Encode a string using Base64.\n\n        Args:\n            value (str): The string to encode.\n\n        Returns:\n            str: The Base64 encoded string.\n        \"\"\"\n        return base64.b64encode(value.encode()).decode()\n\n    def decode(self, value: str | bytes) -&gt; str:\n        \"\"\"Decode a Base64 encoded string or bytes.\n\n        Args:\n            value (str | bytes): The value to decode.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        return base64.b64decode(value).decode()\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.Base64Codec.decode","title":"<code>decode(value)</code>","text":"<p>Decode a Base64 encoded string or bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | bytes</code> <p>The value to decode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def decode(self, value: str | bytes) -&gt; str:\n    \"\"\"Decode a Base64 encoded string or bytes.\n\n    Args:\n        value (str | bytes): The value to decode.\n\n    Returns:\n        str: The decoded string.\n    \"\"\"\n    return base64.b64decode(value).decode()\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.Base64Codec.encode","title":"<code>encode(value)</code>","text":"<p>Encode a string using Base64.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to encode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Base64 encoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def encode(self, value: str) -&gt; str:\n    \"\"\"Encode a string using Base64.\n\n    Args:\n        value (str): The string to encode.\n\n    Returns:\n        str: The Base64 encoded string.\n    \"\"\"\n    return base64.b64encode(value.encode()).decode()\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.BaseCodec","title":"<code>BaseCodec</code>","text":"<p>Abstract base class for encoding and decoding.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>class BaseCodec:\n    \"\"\"Abstract base class for encoding and decoding.\"\"\"\n\n    def encode(self, value: str) -&gt; str:\n        \"\"\"Encode a string value.\n\n        Args:\n            value (str): The string to encode.\n\n        Returns:\n            str: The encoded string.\n        \"\"\"\n        raise NotImplementedError\n\n    def decode(self, value: str | bytes) -&gt; str:\n        \"\"\"Decode a string or bytes value.\n\n        Args:\n            value (str | bytes): The value to decode.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.BaseCodec.decode","title":"<code>decode(value)</code>","text":"<p>Decode a string or bytes value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | bytes</code> <p>The value to decode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def decode(self, value: str | bytes) -&gt; str:\n    \"\"\"Decode a string or bytes value.\n\n    Args:\n        value (str | bytes): The value to decode.\n\n    Returns:\n        str: The decoded string.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/codecs/#dynamiq.cache.codecs.BaseCodec.encode","title":"<code>encode(value)</code>","text":"<p>Encode a string value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to encode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The encoded string.</p> Source code in <code>dynamiq/cache/codecs.py</code> <pre><code>def encode(self, value: str) -&gt; str:\n    \"\"\"Encode a string value.\n\n    Args:\n        value (str): The string to encode.\n\n    Returns:\n        str: The encoded string.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/config/","title":"Config","text":""},{"location":"dynamiq/cache/config/#dynamiq.cache.config.CacheBackend","title":"<code>CacheBackend</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for cache backends.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>class CacheBackend(enum.Enum):\n    \"\"\"Enumeration for cache backends.\"\"\"\n    Redis = \"Redis\"\n</code></pre>"},{"location":"dynamiq/cache/config/#dynamiq.cache.config.CacheConfig","title":"<code>CacheConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for cache settings.</p> <p>Attributes:</p> Name Type Description <code>backend</code> <code>CacheBackend</code> <p>The cache backend to use.</p> <code>namespace</code> <code>str | None</code> <p>Optional namespace for cache keys.</p> <code>ttl</code> <code>int | None</code> <p>Optional time-to-live for cache entries.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>class CacheConfig(BaseModel):\n    \"\"\"Configuration for cache settings.\n\n    Attributes:\n        backend (CacheBackend): The cache backend to use.\n        namespace (str | None): Optional namespace for cache keys.\n        ttl (int | None): Optional time-to-live for cache entries.\n    \"\"\"\n    backend: CacheBackend\n    namespace: str | None = None\n    ttl: int | None = None\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert config to dictionary.\n\n        Args:\n            **kwargs: Additional arguments.\n\n        Returns:\n            dict: Configuration as dictionary.\n        \"\"\"\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/cache/config/#dynamiq.cache.config.CacheConfig.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert config to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Configuration as dictionary.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert config to dictionary.\n\n    Args:\n        **kwargs: Additional arguments.\n\n    Returns:\n        dict: Configuration as dictionary.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/cache/config/#dynamiq.cache.config.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>               Bases: <code>CacheConfig</code>, <code>RedisConnection</code></p> <p>Configuration for Redis cache.</p> <p>Attributes:</p> Name Type Description <code>backend</code> <code>Literal[Redis]</code> <p>The Redis cache backend.</p> Source code in <code>dynamiq/cache/config.py</code> <pre><code>class RedisCacheConfig(CacheConfig, RedisConnection):\n    \"\"\"Configuration for Redis cache.\n\n    Attributes:\n        backend (Literal[CacheBackend.Redis]): The Redis cache backend.\n    \"\"\"\n    backend: Literal[CacheBackend.Redis] = CacheBackend.Redis\n</code></pre>"},{"location":"dynamiq/cache/utils/","title":"Utils","text":""},{"location":"dynamiq/cache/utils/#dynamiq.cache.utils.cache_wf_entity","title":"<code>cache_wf_entity(entity_id, cache_enabled=False, cache_manager_cls=WorkflowCacheManager, cache_config=None, func_kwargs_to_remove=FUNC_KWARGS_TO_REMOVE)</code>","text":"<p>Decorator to cache workflow entity outputs.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Identifier for the entity.</p> required <code>cache_enabled</code> <code>bool</code> <p>Flag to enable caching.</p> <code>False</code> <code>cache_manager_cls</code> <code>type[WorkflowCacheManager]</code> <p>Cache manager class.</p> <code>WorkflowCacheManager</code> <code>cache_config</code> <code>CacheConfig | None</code> <p>Cache configuration.</p> <code>None</code> <code>func_kwargs_to_remove</code> <code>tuple[str]</code> <p>List of params to remove from callable function kwargs.</p> <code>FUNC_KWARGS_TO_REMOVE</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Wrapped function with caching.</p> Source code in <code>dynamiq/cache/utils.py</code> <pre><code>def cache_wf_entity(\n    entity_id: str,\n    cache_enabled: bool = False,\n    cache_manager_cls: type[WorkflowCacheManager] = WorkflowCacheManager,\n    cache_config: CacheConfig | None = None,\n    func_kwargs_to_remove: tuple[str] = FUNC_KWARGS_TO_REMOVE,\n) -&gt; Callable:\n    \"\"\"Decorator to cache workflow entity outputs.\n\n    Args:\n        entity_id (str): Identifier for the entity.\n        cache_enabled (bool): Flag to enable caching.\n        cache_manager_cls (type[WorkflowCacheManager]): Cache manager class.\n        cache_config (CacheConfig | None): Cache configuration.\n        func_kwargs_to_remove (tuple[str]): List of params to remove from callable function kwargs.\n\n    Returns:\n        Callable: Wrapped function with caching.\n    \"\"\"\n    def _cache(func: Callable) -&gt; Callable:\n        \"\"\"Inner cache decorator.\n\n        Args:\n            func (Callable): Function to wrap.\n\n        Returns:\n            Callable: Wrapped function.\n        \"\"\"\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; tuple[Any, bool]:\n            \"\"\"Wrapper function to handle caching.\n\n            Args:\n                *args (Any): Positional arguments.\n                **kwargs (Any): Keyword arguments.\n\n            Returns:\n                tuple[Any, bool]: Function output and cache status.\n            \"\"\"\n            cache_manager = None\n            from_cache = False\n            input_data = kwargs.pop(\"input_data\", args[0] if args else {})\n            cleaned_kwargs = {k: v for k, v in kwargs.items() if k not in func_kwargs_to_remove}\n            if cache_enabled and cache_config:\n                logger.debug(f\"Entity_id {entity_id}: cache used\")\n                cache_manager = cache_manager_cls(config=cache_config)\n                if output := cache_manager.get_entity_output(\n                    entity_id=entity_id, input_data=input_data, **cleaned_kwargs\n                ):\n                    from_cache = True\n                    return output, from_cache\n\n            output = func(*args, **kwargs)\n\n            if cache_manager:\n                cache_manager.set_entity_output(\n                    entity_id=entity_id, input_data=input_data, output_data=output, **cleaned_kwargs\n                )\n\n            return output, from_cache\n\n        return wrapper\n\n    return _cache\n</code></pre>"},{"location":"dynamiq/cache/backends/base/","title":"Base","text":""},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache","title":"<code>BaseCache</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for cache backends.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>CacheClient</code> <p>Cache client instance.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>class BaseCache(ABC):\n    \"\"\"Abstract base class for cache backends.\n\n    Attributes:\n        client (CacheClient): Cache client instance.\n    \"\"\"\n\n    def __init__(self, client: CacheClient):\n        \"\"\"Initialize BaseCache.\n\n        Args:\n            client (CacheClient): Cache client instance.\n        \"\"\"\n        self.client = client\n\n    @classmethod\n    def from_client(cls, client: CacheClient):\n        \"\"\"Create cache instance from client.\n\n        Args:\n            client (CacheClient): Cache client instance.\n\n        Returns:\n            BaseCache: Cache instance.\n        \"\"\"\n        return cls(client=client)\n\n    @classmethod\n    @abstractmethod\n    def from_config(cls, config: CacheConfig):\n        \"\"\"Create cache instance from configuration.\n\n        Args:\n            config (CacheConfig): Cache configuration.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get(self, key: str):\n        \"\"\"Retrieve value from cache.\n\n        Args:\n            key (str): Cache key.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def set(self, key: str, value: dict):\n        \"\"\"Set value in cache.\n\n        Args:\n            key (str): Cache key.\n            value (dict): Value to cache.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def delete(self, key: str):\n        \"\"\"Delete value from cache.\n\n        Args:\n            key (str): Cache key.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.__init__","title":"<code>__init__(client)</code>","text":"<p>Initialize BaseCache.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>CacheClient</code> <p>Cache client instance.</p> required Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>def __init__(self, client: CacheClient):\n    \"\"\"Initialize BaseCache.\n\n    Args:\n        client (CacheClient): Cache client instance.\n    \"\"\"\n    self.client = client\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.delete","title":"<code>delete(key)</code>  <code>abstractmethod</code>","text":"<p>Delete value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@abstractmethod\ndef delete(self, key: str):\n    \"\"\"Delete value from cache.\n\n    Args:\n        key (str): Cache key.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.from_client","title":"<code>from_client(client)</code>  <code>classmethod</code>","text":"<p>Create cache instance from client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>CacheClient</code> <p>Cache client instance.</p> required <p>Returns:</p> Name Type Description <code>BaseCache</code> <p>Cache instance.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@classmethod\ndef from_client(cls, client: CacheClient):\n    \"\"\"Create cache instance from client.\n\n    Args:\n        client (CacheClient): Cache client instance.\n\n    Returns:\n        BaseCache: Cache instance.\n    \"\"\"\n    return cls(client=client)\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.from_config","title":"<code>from_config(config)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create cache instance from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(cls, config: CacheConfig):\n    \"\"\"Create cache instance from configuration.\n\n    Args:\n        config (CacheConfig): Cache configuration.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.get","title":"<code>get(key)</code>  <code>abstractmethod</code>","text":"<p>Retrieve value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@abstractmethod\ndef get(self, key: str):\n    \"\"\"Retrieve value from cache.\n\n    Args:\n        key (str): Cache key.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/base/#dynamiq.cache.backends.base.BaseCache.set","title":"<code>set(key, value)</code>  <code>abstractmethod</code>","text":"<p>Set value in cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>value</code> <code>dict</code> <p>Value to cache.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/cache/backends/base.py</code> <pre><code>@abstractmethod\ndef set(self, key: str, value: dict):\n    \"\"\"Set value in cache.\n\n    Args:\n        key (str): Cache key.\n        value (dict): Value to cache.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/","title":"Redis","text":""},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache","title":"<code>RedisCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>Redis cache backend implementation.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>class RedisCache(BaseCache):\n    \"\"\"Redis cache backend implementation.\"\"\"\n\n    @classmethod\n    def from_config(cls, config: RedisCacheConfig):\n        \"\"\"Create RedisCache instance from configuration.\n\n        Args:\n            config (RedisCacheConfig): Redis cache configuration.\n\n        Returns:\n            RedisCache: Redis cache instance.\n        \"\"\"\n        from redis import Redis\n\n        return cls(client=Redis(**config.to_dict()))\n\n    def get(self, key: str) -&gt; Any:\n        \"\"\"Retrieve value from Redis cache.\n\n        Args:\n            key (str): Cache key.\n\n        Returns:\n            Any: Cached value.\n        \"\"\"\n        return self.client.get(key)\n\n    def set(self, key: str, value: dict, ttl: int | None = None) -&gt; Any:\n        \"\"\"Set value in Redis cache.\n\n        Args:\n            key (str): Cache key.\n            value (dict): Value to cache.\n            ttl (int | None): Time-to-live for cache entry.\n\n        Returns:\n            Any: Result of cache set operation.\n        \"\"\"\n        if ttl is None:\n            return self.client.set(key, value)\n        return self.client.setex(key, ttl, value)\n\n    def delete(self, key: str) -&gt; Any:\n        \"\"\"Delete value from Redis cache.\n\n        Args:\n            key (str): Cache key.\n\n        Returns:\n            Any: Result of cache delete operation.\n        \"\"\"\n        return self.client.delete(key)\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.delete","title":"<code>delete(key)</code>","text":"<p>Delete value from Redis cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache delete operation.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>def delete(self, key: str) -&gt; Any:\n    \"\"\"Delete value from Redis cache.\n\n    Args:\n        key (str): Cache key.\n\n    Returns:\n        Any: Result of cache delete operation.\n    \"\"\"\n    return self.client.delete(key)\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create RedisCache instance from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>Redis cache configuration.</p> required <p>Returns:</p> Name Type Description <code>RedisCache</code> <p>Redis cache instance.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>@classmethod\ndef from_config(cls, config: RedisCacheConfig):\n    \"\"\"Create RedisCache instance from configuration.\n\n    Args:\n        config (RedisCacheConfig): Redis cache configuration.\n\n    Returns:\n        RedisCache: Redis cache instance.\n    \"\"\"\n    from redis import Redis\n\n    return cls(client=Redis(**config.to_dict()))\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.get","title":"<code>get(key)</code>","text":"<p>Retrieve value from Redis cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cached value.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>def get(self, key: str) -&gt; Any:\n    \"\"\"Retrieve value from Redis cache.\n\n    Args:\n        key (str): Cache key.\n\n    Returns:\n        Any: Cached value.\n    \"\"\"\n    return self.client.get(key)\n</code></pre>"},{"location":"dynamiq/cache/backends/redis/#dynamiq.cache.backends.redis.RedisCache.set","title":"<code>set(key, value, ttl=None)</code>","text":"<p>Set value in Redis cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>value</code> <code>dict</code> <p>Value to cache.</p> required <code>ttl</code> <code>int | None</code> <p>Time-to-live for cache entry.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache set operation.</p> Source code in <code>dynamiq/cache/backends/redis.py</code> <pre><code>def set(self, key: str, value: dict, ttl: int | None = None) -&gt; Any:\n    \"\"\"Set value in Redis cache.\n\n    Args:\n        key (str): Cache key.\n        value (dict): Value to cache.\n        ttl (int | None): Time-to-live for cache entry.\n\n    Returns:\n        Any: Result of cache set operation.\n    \"\"\"\n    if ttl is None:\n        return self.client.set(key, value)\n    return self.client.setex(key, ttl, value)\n</code></pre>"},{"location":"dynamiq/cache/managers/base/","title":"Base","text":""},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager","title":"<code>CacheManager</code>","text":"<p>Manager for handling cache operations.</p> <p>Attributes:</p> Name Type Description <code>CACHE_BACKENDS_BY_TYPE</code> <code>dict[CacheBackend, BaseCache]</code> <p>Mapping of backends.</p> <code>cache_backend</code> <code>BaseCache</code> <p>Selected cache backend.</p> <code>cache</code> <code>BaseCache</code> <p>Cache instance.</p> <code>serializer</code> <code>Any</code> <p>Serializer instance.</p> <code>codec</code> <code>Any</code> <p>Codec instance.</p> <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>ttl</code> <code>int | None</code> <p>Time-to-live for cache entries.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>class CacheManager:\n    \"\"\"Manager for handling cache operations.\n\n    Attributes:\n        CACHE_BACKENDS_BY_TYPE (dict[CacheBackend, BaseCache]): Mapping of backends.\n        cache_backend (BaseCache): Selected cache backend.\n        cache (BaseCache): Cache instance.\n        serializer (Any): Serializer instance.\n        codec (Any): Codec instance.\n        namespace (str | None): Cache namespace.\n        ttl (int | None): Time-to-live for cache entries.\n    \"\"\"\n    CACHE_BACKENDS_BY_TYPE: dict[CacheBackend, BaseCache] = {\n        CacheBackend.Redis: RedisCache,\n    }\n\n    def __init__(\n        self,\n        config: CacheConfig,\n        serializer: Any | None = None,\n        codec: Any | None = None,\n    ):\n        \"\"\"Initialize CacheManager.\n\n        Args:\n            config (CacheConfig): Cache configuration.\n            serializer (Any | None): Serializer instance.\n            codec (Any | None): Codec instance.\n        \"\"\"\n        self.cache_backend = self.CACHE_BACKENDS_BY_TYPE.get(config.backend)\n        self.cache = self.cache_backend.from_config(config)\n        self.serializer = serializer or JsonSerializer()\n        self.codec = codec or Base64Codec()\n        self.namespace = config.namespace\n        self.ttl = config.ttl\n\n    def get(\n        self,\n        key: str,\n        namespace: str | None = None,\n        loads_func: Callable[[Any], Any] | None = None,\n        decode_func: Callable[[Any], Any] | None = None,\n    ) -&gt; Any:\n        \"\"\"Retrieve value from cache.\n\n        Args:\n            key (str): Cache key.\n            namespace (str | None): Cache namespace.\n            loads_func (Callable[[Any], Any] | None): Function to deserialize.\n            decode_func (Callable[[Any], Any] | None): Function to decode.\n\n        Returns:\n            Any: Cached value.\n        \"\"\"\n        loads = loads_func or self.serializer.loads\n        decode = decode_func or self.codec.decode\n        ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n\n        if (res := self.cache.get(ns_key)) is not None:\n            res = loads(decode(self.cache.get(key=ns_key)))\n\n        return res\n\n    def set(\n        self,\n        key: str,\n        value: Any,\n        ttl: int | None = None,\n        namespace: str | None = None,\n        dumps_func: Callable[[Any], Any] | None = None,\n        encode_func: Callable[[Any], Any] | None = None,\n    ) -&gt; Any:\n        \"\"\"Set value in cache.\n\n        Args:\n            key (str): Cache key.\n            value (Any): Value to cache.\n            ttl (int | None): Time-to-live for cache entry.\n            namespace (str | None): Cache namespace.\n            dumps_func (Callable[[Any], Any] | None): Function to serialize.\n            encode_func (Callable[[Any], Any] | None): Function to encode.\n\n        Returns:\n            Any: Result of cache set operation.\n        \"\"\"\n        dumps = dumps_func or self.serializer.dumps\n        encode = encode_func or self.codec.encode\n        ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n        ttl = ttl or self.ttl\n\n        res = self.cache.set(key=ns_key, value=encode(dumps(value)), ttl=ttl)\n\n        return res\n\n    def delete(\n        self,\n        key: str,\n        namespace: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Delete value from cache.\n\n        Args:\n            key (str): Cache key.\n            namespace (str | None): Cache namespace.\n\n        Returns:\n            Any: Result of cache delete operation.\n        \"\"\"\n        ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n        res = self.cache.delete(ns_key)\n\n        return res\n\n    def _get_namespace(self, namespace: str | None = None) -&gt; str | None:\n        \"\"\"Get effective namespace.\n\n        Args:\n            namespace (str | None): Provided namespace.\n\n        Returns:\n            str | None: Effective namespace.\n        \"\"\"\n        return namespace if namespace is not None else self.namespace\n\n    @staticmethod\n    def _get_key(key: str, namespace: str | None = None) -&gt; str:\n        \"\"\"Construct cache key with namespace.\n\n        Args:\n            key (str): Cache key.\n            namespace (str | None): Cache namespace.\n\n        Returns:\n            str: Namespaced cache key.\n        \"\"\"\n        if namespace is not None:\n            return f\"{namespace}:{key}\"\n        return key\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.__init__","title":"<code>__init__(config, serializer=None, codec=None)</code>","text":"<p>Initialize CacheManager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> required <code>serializer</code> <code>Any | None</code> <p>Serializer instance.</p> <code>None</code> <code>codec</code> <code>Any | None</code> <p>Codec instance.</p> <code>None</code> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def __init__(\n    self,\n    config: CacheConfig,\n    serializer: Any | None = None,\n    codec: Any | None = None,\n):\n    \"\"\"Initialize CacheManager.\n\n    Args:\n        config (CacheConfig): Cache configuration.\n        serializer (Any | None): Serializer instance.\n        codec (Any | None): Codec instance.\n    \"\"\"\n    self.cache_backend = self.CACHE_BACKENDS_BY_TYPE.get(config.backend)\n    self.cache = self.cache_backend.from_config(config)\n    self.serializer = serializer or JsonSerializer()\n    self.codec = codec or Base64Codec()\n    self.namespace = config.namespace\n    self.ttl = config.ttl\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.delete","title":"<code>delete(key, namespace=None)</code>","text":"<p>Delete value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache delete operation.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def delete(\n    self,\n    key: str,\n    namespace: str | None = None,\n) -&gt; Any:\n    \"\"\"Delete value from cache.\n\n    Args:\n        key (str): Cache key.\n        namespace (str | None): Cache namespace.\n\n    Returns:\n        Any: Result of cache delete operation.\n    \"\"\"\n    ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n    res = self.cache.delete(ns_key)\n\n    return res\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.get","title":"<code>get(key, namespace=None, loads_func=None, decode_func=None)</code>","text":"<p>Retrieve value from cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>None</code> <code>loads_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to deserialize.</p> <code>None</code> <code>decode_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to decode.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cached value.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def get(\n    self,\n    key: str,\n    namespace: str | None = None,\n    loads_func: Callable[[Any], Any] | None = None,\n    decode_func: Callable[[Any], Any] | None = None,\n) -&gt; Any:\n    \"\"\"Retrieve value from cache.\n\n    Args:\n        key (str): Cache key.\n        namespace (str | None): Cache namespace.\n        loads_func (Callable[[Any], Any] | None): Function to deserialize.\n        decode_func (Callable[[Any], Any] | None): Function to decode.\n\n    Returns:\n        Any: Cached value.\n    \"\"\"\n    loads = loads_func or self.serializer.loads\n    decode = decode_func or self.codec.decode\n    ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n\n    if (res := self.cache.get(ns_key)) is not None:\n        res = loads(decode(self.cache.get(key=ns_key)))\n\n    return res\n</code></pre>"},{"location":"dynamiq/cache/managers/base/#dynamiq.cache.managers.base.CacheManager.set","title":"<code>set(key, value, ttl=None, namespace=None, dumps_func=None, encode_func=None)</code>","text":"<p>Set value in cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Cache key.</p> required <code>value</code> <code>Any</code> <p>Value to cache.</p> required <code>ttl</code> <code>int | None</code> <p>Time-to-live for cache entry.</p> <code>None</code> <code>namespace</code> <code>str | None</code> <p>Cache namespace.</p> <code>None</code> <code>dumps_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to serialize.</p> <code>None</code> <code>encode_func</code> <code>Callable[[Any], Any] | None</code> <p>Function to encode.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache set operation.</p> Source code in <code>dynamiq/cache/managers/base.py</code> <pre><code>def set(\n    self,\n    key: str,\n    value: Any,\n    ttl: int | None = None,\n    namespace: str | None = None,\n    dumps_func: Callable[[Any], Any] | None = None,\n    encode_func: Callable[[Any], Any] | None = None,\n) -&gt; Any:\n    \"\"\"Set value in cache.\n\n    Args:\n        key (str): Cache key.\n        value (Any): Value to cache.\n        ttl (int | None): Time-to-live for cache entry.\n        namespace (str | None): Cache namespace.\n        dumps_func (Callable[[Any], Any] | None): Function to serialize.\n        encode_func (Callable[[Any], Any] | None): Function to encode.\n\n    Returns:\n        Any: Result of cache set operation.\n    \"\"\"\n    dumps = dumps_func or self.serializer.dumps\n    encode = encode_func or self.codec.encode\n    ns_key = self._get_key(key, namespace=self._get_namespace(namespace))\n    ttl = ttl or self.ttl\n\n    res = self.cache.set(key=ns_key, value=encode(dumps(value)), ttl=ttl)\n\n    return res\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/","title":"Workflow","text":""},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager","title":"<code>WorkflowCacheManager</code>","text":"<p>               Bases: <code>CacheManager</code></p> <p>Manager for caching workflow entity outputs.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> <code>serializer</code> <code>Any</code> <p>Serializer instance.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>class WorkflowCacheManager(CacheManager):\n    \"\"\"Manager for caching workflow entity outputs.\n\n    Attributes:\n        config (CacheConfig): Cache configuration.\n        serializer (Any): Serializer instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: CacheConfig,\n        serializer: Any | None = None,\n    ):\n        \"\"\"Initialize WorkflowCacheManager.\n\n        Args:\n            config (CacheConfig): Cache configuration.\n            serializer (Any | None): Serializer instance.\n        \"\"\"\n        super().__init__(\n            config=config,\n            serializer=serializer,\n        )\n\n    def get_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n        \"\"\"Retrieve cached entity output.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: Cached output data.\n        \"\"\"\n        key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n        return super().get(key=key)\n\n    def set_entity_output(self, entity_id: str, input_data: dict, output_data: Any, **kwargs) -&gt; Any:\n        \"\"\"Cache entity output.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            output_data (Any): Output data to cache.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: Result of cache set operation.\n        \"\"\"\n        key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n        return super().set(key=key, value=output_data)\n\n    def delete_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n        \"\"\"Delete cached entity output.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: Result of cache delete operation.\n        \"\"\"\n        key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n        return super().delete(key=key)\n\n    def get_key(self, entity_id: str, input_data: dict, **kwargs) -&gt; str:\n        \"\"\"Generate cache key for entity.\n\n        Args:\n            entity_id (str): Entity identifier.\n            input_data (dict): Input data for the entity.\n            kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            str: Generated cache key.\n        \"\"\"\n        input_data_formatted = format_value(self._sort_dict(input_data))\n        input_data_hash = self.hash(self.serializer.dumps(input_data_formatted))\n        kwargs_formatted = format_value(self._sort_dict(kwargs))\n        kwargs_hash = self.hash(self.serializer.dumps(kwargs_formatted))\n        return f\"{entity_id}:{input_data_hash}:{kwargs_hash}\"\n\n    @staticmethod\n    def hash(data: str) -&gt; str:\n        \"\"\"Generate SHA-256 hash of data.\n\n        Args:\n            data (str): Data to hash.\n\n        Returns:\n            str: SHA-256 hash.\n        \"\"\"\n        return hashlib.sha256(data.encode()).hexdigest()\n\n    def _sort_dict(self, d: dict) -&gt; dict:\n        \"\"\"Recursively sort dictionary keys, including nested dictionaries.\n\n        Args:\n            d (dict): Dictionary to sort.\n\n        Returns:\n            dict: Sorted dictionary.\n        \"\"\"\n        return {k: self._sort_dict(v) if isinstance(v, dict) else v for k, v in sorted(d.items())}\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.__init__","title":"<code>__init__(config, serializer=None)</code>","text":"<p>Initialize WorkflowCacheManager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CacheConfig</code> <p>Cache configuration.</p> required <code>serializer</code> <code>Any | None</code> <p>Serializer instance.</p> <code>None</code> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def __init__(\n    self,\n    config: CacheConfig,\n    serializer: Any | None = None,\n):\n    \"\"\"Initialize WorkflowCacheManager.\n\n    Args:\n        config (CacheConfig): Cache configuration.\n        serializer (Any | None): Serializer instance.\n    \"\"\"\n    super().__init__(\n        config=config,\n        serializer=serializer,\n    )\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.delete_entity_output","title":"<code>delete_entity_output(entity_id, input_data, **kwargs)</code>","text":"<p>Delete cached entity output.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache delete operation.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def delete_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n    \"\"\"Delete cached entity output.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        Any: Result of cache delete operation.\n    \"\"\"\n    key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n    return super().delete(key=key)\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.get_entity_output","title":"<code>get_entity_output(entity_id, input_data, **kwargs)</code>","text":"<p>Retrieve cached entity output.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Cached output data.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def get_entity_output(self, entity_id: str, input_data: dict, **kwargs) -&gt; Any:\n    \"\"\"Retrieve cached entity output.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        Any: Cached output data.\n    \"\"\"\n    key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n    return super().get(key=key)\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.get_key","title":"<code>get_key(entity_id, input_data, **kwargs)</code>","text":"<p>Generate cache key for entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated cache key.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def get_key(self, entity_id: str, input_data: dict, **kwargs) -&gt; str:\n    \"\"\"Generate cache key for entity.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        str: Generated cache key.\n    \"\"\"\n    input_data_formatted = format_value(self._sort_dict(input_data))\n    input_data_hash = self.hash(self.serializer.dumps(input_data_formatted))\n    kwargs_formatted = format_value(self._sort_dict(kwargs))\n    kwargs_hash = self.hash(self.serializer.dumps(kwargs_formatted))\n    return f\"{entity_id}:{input_data_hash}:{kwargs_hash}\"\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.hash","title":"<code>hash(data)</code>  <code>staticmethod</code>","text":"<p>Generate SHA-256 hash of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Data to hash.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>SHA-256 hash.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>@staticmethod\ndef hash(data: str) -&gt; str:\n    \"\"\"Generate SHA-256 hash of data.\n\n    Args:\n        data (str): Data to hash.\n\n    Returns:\n        str: SHA-256 hash.\n    \"\"\"\n    return hashlib.sha256(data.encode()).hexdigest()\n</code></pre>"},{"location":"dynamiq/cache/managers/workflow/#dynamiq.cache.managers.workflow.WorkflowCacheManager.set_entity_output","title":"<code>set_entity_output(entity_id, input_data, output_data, **kwargs)</code>","text":"<p>Cache entity output.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier.</p> required <code>input_data</code> <code>dict</code> <p>Input data for the entity.</p> required <code>output_data</code> <code>Any</code> <p>Output data to cache.</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of cache set operation.</p> Source code in <code>dynamiq/cache/managers/workflow.py</code> <pre><code>def set_entity_output(self, entity_id: str, input_data: dict, output_data: Any, **kwargs) -&gt; Any:\n    \"\"\"Cache entity output.\n\n    Args:\n        entity_id (str): Entity identifier.\n        input_data (dict): Input data for the entity.\n        output_data (Any): Output data to cache.\n        kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        Any: Result of cache set operation.\n    \"\"\"\n    key = self.get_key(entity_id=entity_id, input_data=input_data, **kwargs)\n    return super().set(key=key, value=output_data)\n</code></pre>"},{"location":"dynamiq/callbacks/base/","title":"Base","text":""},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler","title":"<code>BaseCallbackHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for callback handlers.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>class BaseCallbackHandler(ABC):\n    \"\"\"Abstract base class for callback handlers.\"\"\"\n\n    def on_workflow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            input_data (dict[str, Any]): Input data for the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_workflow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            output_data (dict[str, Any]): Output data from the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_workflow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the workflow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_flow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            input_data (dict[str, Any]): Input data for the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_flow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            output_data (dict[str, Any]): Output data from the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_flow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the flow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the node errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_skip(\n        self,\n        serialized: dict[str, Any],\n        skip_data: dict[str, Any],\n        input_data: dict[str, Any],\n        **kwargs: Any\n    ):\n        \"\"\"Called when the node skips.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            skip_data (dict[str, Any]): Data related to the skip.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node execute starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node execute ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the node execute errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node execute runs.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n\n    def on_node_execute_stream(self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any):\n        \"\"\"Called when the node execute streams.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            chunk (dict[str, Any] | None): Stream chunk data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_flow_end","title":"<code>on_flow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the flow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_flow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        output_data (dict[str, Any]): Output data from the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_flow_error","title":"<code>on_flow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the flow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_flow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the flow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_flow_start","title":"<code>on_flow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the flow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_flow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        input_data (dict[str, Any]): Input data for the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_end","title":"<code>on_node_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_error","title":"<code>on_node_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the node errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_execute_end","title":"<code>on_node_execute_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node execute ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node execute ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_execute_error","title":"<code>on_node_execute_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node execute errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the node execute errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_execute_run","title":"<code>on_node_execute_run(serialized, **kwargs)</code>","text":"<p>Called when the node execute runs.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node execute runs.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_execute_start","title":"<code>on_node_execute_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node execute starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node execute starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_execute_stream","title":"<code>on_node_execute_stream(serialized, chunk=None, **kwargs)</code>","text":"<p>Called when the node execute streams.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>chunk</code> <code>dict[str, Any] | None</code> <p>Stream chunk data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_execute_stream(self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any):\n    \"\"\"Called when the node execute streams.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        chunk (dict[str, Any] | None): Stream chunk data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_skip","title":"<code>on_node_skip(serialized, skip_data, input_data, **kwargs)</code>","text":"<p>Called when the node skips.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>skip_data</code> <code>dict[str, Any]</code> <p>Data related to the skip.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_skip(\n    self,\n    serialized: dict[str, Any],\n    skip_data: dict[str, Any],\n    input_data: dict[str, Any],\n    **kwargs: Any\n):\n    \"\"\"Called when the node skips.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        skip_data (dict[str, Any]): Data related to the skip.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_node_start","title":"<code>on_node_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_node_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_workflow_end","title":"<code>on_workflow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the workflow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_workflow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        output_data (dict[str, Any]): Output data from the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_workflow_error","title":"<code>on_workflow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the workflow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_workflow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the workflow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.BaseCallbackHandler.on_workflow_start","title":"<code>on_workflow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the workflow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def on_workflow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        input_data (dict[str, Any]): Input data for the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_entity_id","title":"<code>get_entity_id(entity_name, kwargs)</code>","text":"<p>Retrieve entity ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>entity_name</code> <code>str</code> <p>Name of the entity.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Entity ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If entity ID is not found or invalid.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_entity_id(entity_name: str, kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve entity ID from kwargs.\n\n    Args:\n        entity_name (str): Name of the entity.\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Entity ID.\n\n    Raises:\n        ValueError: If entity ID is not found or invalid.\n    \"\"\"\n    entity_id = kwargs.get(entity_name)\n    if not entity_id:\n        raise ValueError(f\"{entity_name} not found\")\n\n    if isinstance(entity_id, UUID):\n        return entity_id\n    elif isinstance(entity_id, str):\n        return UUID(entity_id)\n\n    raise ValueError(f\"{entity_name} is not UUID or str\")\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_execution_run_id","title":"<code>get_execution_run_id(kwargs)</code>","text":"<p>Retrieve execution run ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Execution run ID.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_execution_run_id(kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve execution run ID from kwargs.\n\n    Args:\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Execution run ID.\n    \"\"\"\n    return get_entity_id(\"execution_run_id\", kwargs)\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_parent_run_id","title":"<code>get_parent_run_id(kwargs)</code>","text":"<p>Retrieve parent run ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Parent run ID.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_parent_run_id(kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve parent run ID from kwargs.\n\n    Args:\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Parent run ID.\n    \"\"\"\n    return get_entity_id(\"parent_run_id\", kwargs)\n</code></pre>"},{"location":"dynamiq/callbacks/base/#dynamiq.callbacks.base.get_run_id","title":"<code>get_run_id(kwargs)</code>","text":"<p>Retrieve run ID from kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>UUID</code> <code>UUID</code> <p>Run ID.</p> Source code in <code>dynamiq/callbacks/base.py</code> <pre><code>def get_run_id(kwargs: dict) -&gt; UUID:\n    \"\"\"Retrieve run ID from kwargs.\n\n    Args:\n        kwargs (dict): Keyword arguments.\n\n    Returns:\n        UUID: Run ID.\n    \"\"\"\n    return get_entity_id(\"run_id\", kwargs)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/","title":"Streaming","text":""},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler","title":"<code>AsyncStreamingIteratorCallbackHandler</code>","text":"<p>               Bases: <code>StreamingQueueCallbackHandler</code></p> <p>Callback handler for streaming events using an async iterator.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class AsyncStreamingIteratorCallbackHandler(StreamingQueueCallbackHandler):\n    \"\"\"Callback handler for streaming events using an async iterator.\"\"\"\n\n    def __init__(\n        self,\n        queue: asyncio.Queue | None = None,\n        done_event: asyncio.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize AsyncStreamingIteratorCallbackHandler.\n\n        Args:\n            queue (asyncio.Queue | None): Queue for streaming events.\n            done_event (asyncio.Event | None): Event to signal completion.\n        \"\"\"\n        if queue is None:\n            queue = asyncio.Queue()\n        if done_event is None:\n            done_event = asyncio.Event()\n        super().__init__(queue, done_event)\n        self._iterator = self._iter_queue_events()\n\n    async def _iter_queue_events(self) -&gt; AsyncIterator[StreamingEventMessage]:\n        \"\"\"Async iterate over queue events.\n\n        Returns:\n            AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.\n        \"\"\"\n        try:\n            while not self.queue.empty() or not self.done_event.is_set():\n                event = await self.queue.get()\n                yield event\n        except Exception as e:\n            logger.error(f\"Event streaming failed. Error: {e}\")\n\n    async def __anext__(self) -&gt; StreamingEventMessage:\n        \"\"\"Get the next async streaming event.\n\n        Returns:\n            StreamingEventMessage: Next async streaming event.\n        \"\"\"\n        return await self._iterator.__anext__()\n\n    async def __aiter__(self) -&gt; AsyncIterator[StreamingEventMessage]:\n        \"\"\"Get the async iterator for streaming events.\n\n        Returns:\n            AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.\n        \"\"\"\n        async for item in self._iterator:\n            yield item\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.__aiter__","title":"<code>__aiter__()</code>  <code>async</code>","text":"<p>Get the async iterator for streaming events.</p> <p>Returns:</p> Type Description <code>AsyncIterator[StreamingEventMessage]</code> <p>AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>async def __aiter__(self) -&gt; AsyncIterator[StreamingEventMessage]:\n    \"\"\"Get the async iterator for streaming events.\n\n    Returns:\n        AsyncIterator[StreamingEventMessage]: Async iterator for streaming events.\n    \"\"\"\n    async for item in self._iterator:\n        yield item\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.__anext__","title":"<code>__anext__()</code>  <code>async</code>","text":"<p>Get the next async streaming event.</p> <p>Returns:</p> Name Type Description <code>StreamingEventMessage</code> <code>StreamingEventMessage</code> <p>Next async streaming event.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>async def __anext__(self) -&gt; StreamingEventMessage:\n    \"\"\"Get the next async streaming event.\n\n    Returns:\n        StreamingEventMessage: Next async streaming event.\n    \"\"\"\n    return await self._iterator.__anext__()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.AsyncStreamingIteratorCallbackHandler.__init__","title":"<code>__init__(queue=None, done_event=None)</code>","text":"<p>Initialize AsyncStreamingIteratorCallbackHandler.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue | None</code> <p>Queue for streaming events.</p> <code>None</code> <code>done_event</code> <code>Event | None</code> <p>Event to signal completion.</p> <code>None</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __init__(\n    self,\n    queue: asyncio.Queue | None = None,\n    done_event: asyncio.Event | None = None,\n) -&gt; None:\n    \"\"\"Initialize AsyncStreamingIteratorCallbackHandler.\n\n    Args:\n        queue (asyncio.Queue | None): Queue for streaming events.\n        done_event (asyncio.Event | None): Event to signal completion.\n    \"\"\"\n    if queue is None:\n        queue = asyncio.Queue()\n    if done_event is None:\n        done_event = asyncio.Event()\n    super().__init__(queue, done_event)\n    self._iterator = self._iter_queue_events()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler","title":"<code>StreamingIteratorCallbackHandler</code>","text":"<p>               Bases: <code>StreamingQueueCallbackHandler</code></p> <p>Callback handler for streaming events using an iterator.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class StreamingIteratorCallbackHandler(StreamingQueueCallbackHandler):\n    \"\"\"Callback handler for streaming events using an iterator.\"\"\"\n\n    def __init__(\n        self,\n        queue: Queue | None = None,\n        done_event: threading.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize StreamingIteratorCallbackHandler.\n\n        Args:\n            queue (Queue | None): Queue for streaming events.\n            done_event (threading.Event | None): Event to signal completion.\n        \"\"\"\n        if queue is None:\n            queue = Queue()\n        if done_event is None:\n            done_event = threading.Event()\n        super().__init__(queue, done_event)\n        self._iterator = self._iter_queue_events()\n\n    def _iter_queue_events(self) -&gt; Iterator[StreamingEventMessage]:\n        \"\"\"Iterate over queue events.\n\n        Returns:\n            Iterator[StreamingEventMessage]: Iterator for streaming events.\n        \"\"\"\n        try:\n            while not self.queue.empty() or not self.done_event.is_set():\n                event = self.queue.get()\n                yield event\n        except Exception as e:\n            logger.error(f\"Event streaming failed. Error: {e}\")\n\n    def __next__(self) -&gt; StreamingEventMessage:\n        \"\"\"Get the next streaming event.\n\n        Returns:\n            StreamingEventMessage: Next streaming event.\n        \"\"\"\n        return self._iterator.__next__()\n\n    def __iter__(self) -&gt; Iterator[StreamingEventMessage]:\n        \"\"\"Get the iterator for streaming events.\n\n        Returns:\n            Iterator[StreamingEventMessage]: Iterator for streaming events.\n        \"\"\"\n        yield from self._iterator\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler.__init__","title":"<code>__init__(queue=None, done_event=None)</code>","text":"<p>Initialize StreamingIteratorCallbackHandler.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue | None</code> <p>Queue for streaming events.</p> <code>None</code> <code>done_event</code> <code>Event | None</code> <p>Event to signal completion.</p> <code>None</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __init__(\n    self,\n    queue: Queue | None = None,\n    done_event: threading.Event | None = None,\n) -&gt; None:\n    \"\"\"Initialize StreamingIteratorCallbackHandler.\n\n    Args:\n        queue (Queue | None): Queue for streaming events.\n        done_event (threading.Event | None): Event to signal completion.\n    \"\"\"\n    if queue is None:\n        queue = Queue()\n    if done_event is None:\n        done_event = threading.Event()\n    super().__init__(queue, done_event)\n    self._iterator = self._iter_queue_events()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler.__iter__","title":"<code>__iter__()</code>","text":"<p>Get the iterator for streaming events.</p> <p>Returns:</p> Type Description <code>Iterator[StreamingEventMessage]</code> <p>Iterator[StreamingEventMessage]: Iterator for streaming events.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __iter__(self) -&gt; Iterator[StreamingEventMessage]:\n    \"\"\"Get the iterator for streaming events.\n\n    Returns:\n        Iterator[StreamingEventMessage]: Iterator for streaming events.\n    \"\"\"\n    yield from self._iterator\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingIteratorCallbackHandler.__next__","title":"<code>__next__()</code>","text":"<p>Get the next streaming event.</p> <p>Returns:</p> Name Type Description <code>StreamingEventMessage</code> <code>StreamingEventMessage</code> <p>Next streaming event.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __next__(self) -&gt; StreamingEventMessage:\n    \"\"\"Get the next streaming event.\n\n    Returns:\n        StreamingEventMessage: Next streaming event.\n    \"\"\"\n    return self._iterator.__next__()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler","title":"<code>StreamingQueueCallbackHandler</code>","text":"<p>               Bases: <code>BaseCallbackHandler</code></p> <p>Callback handler for streaming events to a queue.</p> <p>Attributes:</p> Name Type Description <code>queue</code> <code>Queue | Queue | None</code> <p>Queue for streaming events.</p> <code>done_event</code> <code>Event | Event | None</code> <p>Event to signal completion.</p> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>class StreamingQueueCallbackHandler(BaseCallbackHandler):\n    \"\"\"Callback handler for streaming events to a queue.\n\n    Attributes:\n        queue (asyncio.Queue | Queue | None): Queue for streaming events.\n        done_event (asyncio.Event | threading.Event | None): Event to signal completion.\n    \"\"\"\n\n    def __init__(\n        self,\n        queue: asyncio.Queue | Queue | None = None,\n        done_event: asyncio.Event | threading.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize StreamingQueueCallbackHandler.\n\n        Args:\n            queue (asyncio.Queue | Queue | None): Queue for streaming events.\n            done_event (asyncio.Event | threading.Event | None): Event to signal completion.\n        \"\"\"\n        self.queue = queue\n        self.done_event = done_event\n\n    def on_workflow_start(\n        self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the workflow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            prompts (list[str]): List of prompts.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        self.done_event.clear()\n\n    def on_node_execute_stream(\n        self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the node execute streams.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            chunk (dict[str, Any] | None): Stream chunk data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        event = kwargs.get(\"event\") or StreamingEventMessage(\n            run_id=str(get_run_id(kwargs)),\n            wf_run_id=kwargs.get(\"wf_run_id\"),\n            entity_id=serialized.get(\"id\"),\n            data=format_value(chunk),\n            event=serialized.get(\"streaming\", {}).get(\"event\"),\n        )\n        self.queue.put_nowait(event)\n\n    def on_workflow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the workflow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            output_data (dict[str, Any]): Output data from the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        event = StreamingEventMessage(\n            run_id=str(get_run_id(kwargs)),\n            wf_run_id=kwargs.get(\"wf_run_id\"),\n            entity_id=serialized.get(\"id\"),\n            data=format_value(output_data),\n            event=serialized.get(\"streaming\", {}).get(\"event\"),\n        )\n        self.queue.put_nowait(event)\n        self.done_event.set()\n\n    def on_workflow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Called when the workflow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        self.done_event.set()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.__init__","title":"<code>__init__(queue=None, done_event=None)</code>","text":"<p>Initialize StreamingQueueCallbackHandler.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue | Queue | None</code> <p>Queue for streaming events.</p> <code>None</code> <code>done_event</code> <code>Event | Event | None</code> <p>Event to signal completion.</p> <code>None</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def __init__(\n    self,\n    queue: asyncio.Queue | Queue | None = None,\n    done_event: asyncio.Event | threading.Event | None = None,\n) -&gt; None:\n    \"\"\"Initialize StreamingQueueCallbackHandler.\n\n    Args:\n        queue (asyncio.Queue | Queue | None): Queue for streaming events.\n        done_event (asyncio.Event | threading.Event | None): Event to signal completion.\n    \"\"\"\n    self.queue = queue\n    self.done_event = done_event\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_node_execute_stream","title":"<code>on_node_execute_stream(serialized, chunk=None, **kwargs)</code>","text":"<p>Called when the node execute streams.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>chunk</code> <code>dict[str, Any] | None</code> <p>Stream chunk data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_node_execute_stream(\n    self, serialized: dict[str, Any], chunk: dict[str, Any] | None = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the node execute streams.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        chunk (dict[str, Any] | None): Stream chunk data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    event = kwargs.get(\"event\") or StreamingEventMessage(\n        run_id=str(get_run_id(kwargs)),\n        wf_run_id=kwargs.get(\"wf_run_id\"),\n        entity_id=serialized.get(\"id\"),\n        data=format_value(chunk),\n        event=serialized.get(\"streaming\", {}).get(\"event\"),\n    )\n    self.queue.put_nowait(event)\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_workflow_end","title":"<code>on_workflow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the workflow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_workflow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the workflow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        output_data (dict[str, Any]): Output data from the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    event = StreamingEventMessage(\n        run_id=str(get_run_id(kwargs)),\n        wf_run_id=kwargs.get(\"wf_run_id\"),\n        entity_id=serialized.get(\"id\"),\n        data=format_value(output_data),\n        event=serialized.get(\"streaming\", {}).get(\"event\"),\n    )\n    self.queue.put_nowait(event)\n    self.done_event.set()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_workflow_error","title":"<code>on_workflow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the workflow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_workflow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the workflow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    self.done_event.set()\n</code></pre>"},{"location":"dynamiq/callbacks/streaming/#dynamiq.callbacks.streaming.StreamingQueueCallbackHandler.on_workflow_start","title":"<code>on_workflow_start(serialized, prompts, **kwargs)</code>","text":"<p>Called when the workflow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>prompts</code> <code>list[str]</code> <p>List of prompts.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/streaming.py</code> <pre><code>def on_workflow_start(\n    self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any\n) -&gt; None:\n    \"\"\"Called when the workflow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        prompts (list[str]): List of prompts.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    self.done_event.clear()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/","title":"Tracing","text":""},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ExecutionRun","title":"<code>ExecutionRun</code>  <code>dataclass</code>","text":"<p>Data class for execution run details.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>UUID</code> <p>Execution run ID.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the execution.</p> <code>end_time</code> <code>datetime | None</code> <p>End time of the execution.</p> <code>status</code> <code>RunStatus | None</code> <p>Status of the execution.</p> <code>input</code> <code>Any | None</code> <p>Input data for the execution.</p> <code>output</code> <code>Any | None</code> <p>Output data from the execution.</p> <code>error</code> <code>Any | None</code> <p>Error details if any.</p> <code>metadata</code> <code>dict</code> <p>Additional metadata.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>@dataclass\nclass ExecutionRun:\n    \"\"\"Data class for execution run details.\n\n    Attributes:\n        id (UUID): Execution run ID.\n        start_time (datetime): Start time of the execution.\n        end_time (datetime | None): End time of the execution.\n        status (RunStatus | None): Status of the execution.\n        input (Any | None): Input data for the execution.\n        output (Any | None): Output data from the execution.\n        error (Any | None): Error details if any.\n        metadata (dict): Additional metadata.\n    \"\"\"\n    id: UUID\n    start_time: datetime\n    end_time: datetime | None = None\n    status: RunStatus | None = None\n    input: Any | None = None\n    output: Any | None = None\n    error: Any | None = None\n    metadata: dict = field(default_factory=dict)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert ExecutionRun to dictionary.\n\n        Returns:\n            dict: Dictionary representation of ExecutionRun.\n        \"\"\"\n        return asdict(self)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ExecutionRun.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert ExecutionRun to dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of ExecutionRun.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert ExecutionRun to dictionary.\n\n    Returns:\n        dict: Dictionary representation of ExecutionRun.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.Run","title":"<code>Run</code>  <code>dataclass</code>","text":"<p>Data class for run details.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>UUID</code> <p>Run ID.</p> <code>name</code> <code>str</code> <p>Name of the run.</p> <code>type</code> <code>RunType</code> <p>Type of the run.</p> <code>trace_id</code> <code>UUID | str</code> <p>Trace ID.</p> <code>source_id</code> <code>UUID | str</code> <p>Source ID.</p> <code>session_id</code> <code>UUID | str</code> <p>Session ID.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the run.</p> <code>end_time</code> <code>datetime</code> <p>End time of the run.</p> <code>parent_run_id</code> <code>UUID</code> <p>Parent run ID.</p> <code>status</code> <code>RunStatus</code> <p>Status of the run.</p> <code>input</code> <code>Any</code> <p>Input data for the run.</p> <code>output</code> <code>Any</code> <p>Output data from the run.</p> <code>metadata</code> <code>Any</code> <p>Additional metadata.</p> <code>error</code> <code>Any</code> <p>Error details if any.</p> <code>executions</code> <code>list[ExecutionRun]</code> <p>List of execution runs.</p> <code>tags</code> <code>list[str]</code> <p>List of tags.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>@dataclass\nclass Run:\n    \"\"\"Data class for run details.\n\n    Attributes:\n        id (UUID): Run ID.\n        name (str): Name of the run.\n        type (RunType): Type of the run.\n        trace_id (UUID | str): Trace ID.\n        source_id (UUID | str): Source ID.\n        session_id (UUID | str): Session ID.\n        start_time (datetime): Start time of the run.\n        end_time (datetime): End time of the run.\n        parent_run_id (UUID): Parent run ID.\n        status (RunStatus): Status of the run.\n        input (Any): Input data for the run.\n        output (Any): Output data from the run.\n        metadata (Any): Additional metadata.\n        error (Any): Error details if any.\n        executions (list[ExecutionRun]): List of execution runs.\n        tags (list[str]): List of tags.\n    \"\"\"\n    id: UUID\n    name: str\n    type: RunType\n    trace_id: UUID | str\n    source_id: UUID | str\n    session_id: UUID | str\n    start_time: datetime\n    end_time: datetime = None\n    parent_run_id: UUID = None\n    status: RunStatus = None\n    input: Any = None\n    output: Any = None\n    metadata: Any = None\n    error: Any = None\n    executions: list[ExecutionRun] = field(default_factory=list)\n    tags: list[str] = field(default_factory=list)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert Run to dictionary.\n\n        Returns:\n            dict: Dictionary representation of Run.\n        \"\"\"\n        return asdict(self)\n\n    def to_json(self) -&gt; str:\n        \"\"\"Convert Run to JSON string.\n\n        Returns:\n            str: JSON string representation of Run.\n        \"\"\"\n        return json.dumps(self.to_dict(), cls=JsonWorkflowEncoder)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.Run.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert Run to dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of Run.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert Run to dictionary.\n\n    Returns:\n        dict: Dictionary representation of Run.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.Run.to_json","title":"<code>to_json()</code>","text":"<p>Convert Run to JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string representation of Run.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert Run to JSON string.\n\n    Returns:\n        str: JSON string representation of Run.\n    \"\"\"\n    return json.dumps(self.to_dict(), cls=JsonWorkflowEncoder)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.RunStatus","title":"<code>RunStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for run statuses.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>class RunStatus(Enum):\n    \"\"\"Enumeration for run statuses.\"\"\"\n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.RunType","title":"<code>RunType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for run types.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>class RunType(Enum):\n    \"\"\"Enumeration for run types.\"\"\"\n    WORKFLOW = \"workflow\"\n    FLOW = \"flow\"\n    NODE = \"node\"\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler","title":"<code>TracingCallbackHandler</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>BaseCallbackHandler</code></p> <p>Callback handler for tracing workflow events.</p> <p>Attributes:</p> Name Type Description <code>source_id</code> <code>str | None</code> <p>Source ID.</p> <code>trace_id</code> <code>str | None</code> <p>Trace ID.</p> <code>session_id</code> <code>str | None</code> <p>Session ID.</p> <code>client</code> <code>BaseTracingClient | None</code> <p>Tracing client.</p> <code>runs</code> <code>dict[UUID, Run]</code> <p>Dictionary of runs.</p> <code>tags</code> <code>list[str]</code> <p>List of tags.</p> <code>installed_pkgs</code> <code>list[str]</code> <p>List of installed packages.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>class TracingCallbackHandler(BaseModel, BaseCallbackHandler):\n    \"\"\"Callback handler for tracing workflow events.\n\n    Attributes:\n        source_id (str | None): Source ID.\n        trace_id (str | None): Trace ID.\n        session_id (str | None): Session ID.\n        client (BaseTracingClient | None): Tracing client.\n        runs (dict[UUID, Run]): Dictionary of runs.\n        tags (list[str]): List of tags.\n        installed_pkgs (list[str]): List of installed packages.\n    \"\"\"\n    source_id: str | None = Field(default_factory=generate_uuid)\n    trace_id: str | None = Field(default_factory=generate_uuid)\n    session_id: str | None = Field(default_factory=generate_uuid)\n    client: BaseTracingClient | None = None\n    runs: dict[UUID, Run] = {}\n    tags: list[str] = []\n\n    installed_pkgs: list[str] = Field(\n        [\"dynamiq\"],\n        description=\"List of installed packages to include in the host information.\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @cached_property\n    def host(self) -&gt; dict:\n        \"\"\"Get host information.\n\n        Returns:\n            dict: Host information including installed packages.\n        \"\"\"\n        return {\n            \"installed_pkgs\": [\n                {\"name\": dist.metadata[\"Name\"], \"version\": dist.version}\n                for dist in distributions()\n                if dist.metadata.get(\"Name\") in self.installed_pkgs\n            ],\n        }\n\n    def _get_node_base_run(self, serialized: dict[str, Any], **kwargs: Any) -&gt; Run:\n        \"\"\"Get base run details for a node.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            **kwargs (Any): Additional arguments.\n\n        Returns:\n            Run: Base run details for the node.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        parent_run_id = get_parent_run_id(kwargs)\n\n        from dynamiq.nodes import NodeGroup\n\n        # Handle runtime LLM prompt override\n        if serialized.get(\"group\") == NodeGroup.LLMS:\n            prompt = kwargs.get(\"prompt\") or serialized.get(\"prompt\")\n            if isinstance(prompt, BaseModel):\n                prompt = prompt.model_dump()\n            serialized[\"prompt\"] = prompt\n\n        run = Run(\n            id=run_id,\n            name=serialized.get(\"name\"),\n            type=RunType.NODE,\n            trace_id=self.trace_id,\n            source_id=self.source_id,\n            session_id=self.session_id,\n            start_time=datetime.now(UTC),\n            parent_run_id=parent_run_id,\n            metadata={\"node\": serialized, \"run_depends\": kwargs.get(\"run_depends\", []), \"host\": self.host},\n            tags=self.tags,\n        )\n        return run\n\n    def on_workflow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            input_data (dict[str, Any]): Input data for the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        self.runs[run_id] = Run(\n            id=run_id,\n            name=\"Workflow\",\n            type=RunType.WORKFLOW,\n            trace_id=self.trace_id,\n            source_id=self.source_id,\n            session_id=self.session_id,\n            start_time=datetime.now(UTC),\n            input=format_value(input_data),\n            metadata={\n                \"workflow\": {\"id\": serialized.get(\"id\"), \"version\": serialized.get(\"version\")},\n                \"host\": self.host,\n            },\n            tags=self.tags,\n        )\n\n    def on_workflow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the workflow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            output_data (dict[str, Any]): Output data from the workflow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.output = format_value(output_data)\n        run.status = RunStatus.SUCCEEDED\n\n        self.flush()\n\n    def on_workflow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the workflow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized workflow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.status = RunStatus.FAILED\n        run.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n    def on_flow_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            input_data (dict[str, Any]): Input data for the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        parent_run_id = get_parent_run_id(kwargs)\n\n        self.runs[run_id] = Run(\n            id=run_id,\n            name=\"Flow\",\n            type=RunType.FLOW,\n            trace_id=self.trace_id,\n            source_id=self.source_id,\n            session_id=self.session_id,\n            start_time=datetime.now(UTC),\n            parent_run_id=parent_run_id,\n            input=format_value(input_data),\n            metadata={\"flow\": {\"id\": serialized.get(\"id\")}, \"host\": self.host},\n            tags=self.tags,\n        )\n\n    def on_flow_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the flow ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            output_data (dict[str, Any]): Output data from the flow.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.output = format_value(output_data)\n        run.status = RunStatus.SUCCEEDED\n\n    def on_flow_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the flow errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized flow data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.status = RunStatus.FAILED\n        run.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n    def on_node_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        run = self._get_node_base_run(serialized, **kwargs)\n        run.input = format_value(input_data)\n        self.runs[run_id] = run\n\n    def on_node_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.output = format_value(output_data)\n        run.status = RunStatus.SUCCEEDED\n        run.metadata[\"is_output_from_cache\"] = kwargs.get(\"is_output_from_cache\", False)\n\n    def on_node_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the node errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        run.end_time = datetime.now(UTC)\n        run.status = RunStatus.FAILED\n        run.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n    def on_node_skip(\n        self,\n        serialized: dict[str, Any],\n        skip_data: dict[str, Any],\n        input_data: dict[str, Any],\n        **kwargs: Any,\n    ):\n        \"\"\"Called when the node skips.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            skip_data (dict[str, Any]): Data related to the skip.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run_id = get_run_id(kwargs)\n        if (run := self.runs.get(run_id)) is None:\n            run = self._get_node_base_run(serialized, **kwargs)\n            self.runs[run_id] = run\n\n        run.input = format_value(input_data)\n        run.end_time = run.start_time\n        run.status = RunStatus.SKIPPED\n        run.metadata[\"skip\"] = format_value(skip_data)\n\n    def on_node_execute_start(\n        self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node execute starts.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        execution_run_id = get_execution_run_id(kwargs)\n        execution = ExecutionRun(\n            id=execution_run_id,\n            start_time=datetime.now(UTC),\n            input=format_value(input_data),\n        )\n        run.executions.append(execution)\n\n    def on_node_execute_end(\n        self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n    ):\n        \"\"\"Called when the node execute ends.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n        execution.end_time = datetime.now(UTC)\n        execution.output = format_value(output_data)\n        execution.status = RunStatus.SUCCEEDED\n\n    def on_node_execute_error(\n        self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n    ):\n        \"\"\"Called when the node execute errors.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            error (BaseException): Error encountered.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n        execution.end_time = datetime.now(UTC)\n        execution.status = RunStatus.FAILED\n        execution.error = {\n            \"message\": str(error),\n            \"traceback\": traceback.format_exc(),\n        }\n\n    def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n        \"\"\"Called when the node execute runs.\n\n        Args:\n            serialized (dict[str, Any]): Serialized node data.\n            **kwargs (Any): Additional arguments.\n        \"\"\"\n        run = ensure_run(get_run_id(kwargs), self.runs)\n        if usage := kwargs.get(\"usage_data\"):\n            run.metadata[\"usage\"] = usage\n\n        if prompt_messages := kwargs.get(\"prompt_messages\"):\n            run.metadata[\"node\"][\"prompt\"][\"messages\"] = prompt_messages\n\n    def flush(self):\n        \"\"\"Flush the runs to the tracing client.\"\"\"\n        if self.client:\n            self.client.trace([run for run in self.runs.values()])\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.host","title":"<code>host: dict</code>  <code>cached</code> <code>property</code>","text":"<p>Get host information.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Host information including installed packages.</p>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.flush","title":"<code>flush()</code>","text":"<p>Flush the runs to the tracing client.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def flush(self):\n    \"\"\"Flush the runs to the tracing client.\"\"\"\n    if self.client:\n        self.client.trace([run for run in self.runs.values()])\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_flow_end","title":"<code>on_flow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the flow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_flow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        output_data (dict[str, Any]): Output data from the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.output = format_value(output_data)\n    run.status = RunStatus.SUCCEEDED\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_flow_error","title":"<code>on_flow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the flow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_flow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the flow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.status = RunStatus.FAILED\n    run.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_flow_start","title":"<code>on_flow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the flow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized flow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the flow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_flow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the flow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized flow data.\n        input_data (dict[str, Any]): Input data for the flow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    parent_run_id = get_parent_run_id(kwargs)\n\n    self.runs[run_id] = Run(\n        id=run_id,\n        name=\"Flow\",\n        type=RunType.FLOW,\n        trace_id=self.trace_id,\n        source_id=self.source_id,\n        session_id=self.session_id,\n        start_time=datetime.now(UTC),\n        parent_run_id=parent_run_id,\n        input=format_value(input_data),\n        metadata={\"flow\": {\"id\": serialized.get(\"id\")}, \"host\": self.host},\n        tags=self.tags,\n    )\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_end","title":"<code>on_node_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.output = format_value(output_data)\n    run.status = RunStatus.SUCCEEDED\n    run.metadata[\"is_output_from_cache\"] = kwargs.get(\"is_output_from_cache\", False)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_error","title":"<code>on_node_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the node errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.status = RunStatus.FAILED\n    run.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_end","title":"<code>on_node_execute_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the node execute ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node execute ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n    execution.end_time = datetime.now(UTC)\n    execution.output = format_value(output_data)\n    execution.status = RunStatus.SUCCEEDED\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_error","title":"<code>on_node_execute_error(serialized, error, **kwargs)</code>","text":"<p>Called when the node execute errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the node execute errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    execution = ensure_execution_run(get_execution_run_id(kwargs), run.executions)\n    execution.end_time = datetime.now(UTC)\n    execution.status = RunStatus.FAILED\n    execution.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_run","title":"<code>on_node_execute_run(serialized, **kwargs)</code>","text":"<p>Called when the node execute runs.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_run(self, serialized: dict[str, Any], **kwargs: Any):\n    \"\"\"Called when the node execute runs.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    if usage := kwargs.get(\"usage_data\"):\n        run.metadata[\"usage\"] = usage\n\n    if prompt_messages := kwargs.get(\"prompt_messages\"):\n        run.metadata[\"node\"][\"prompt\"][\"messages\"] = prompt_messages\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_execute_start","title":"<code>on_node_execute_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node execute starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_execute_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node execute starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    execution_run_id = get_execution_run_id(kwargs)\n    execution = ExecutionRun(\n        id=execution_run_id,\n        start_time=datetime.now(UTC),\n        input=format_value(input_data),\n    )\n    run.executions.append(execution)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_skip","title":"<code>on_node_skip(serialized, skip_data, input_data, **kwargs)</code>","text":"<p>Called when the node skips.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>skip_data</code> <code>dict[str, Any]</code> <p>Data related to the skip.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_skip(\n    self,\n    serialized: dict[str, Any],\n    skip_data: dict[str, Any],\n    input_data: dict[str, Any],\n    **kwargs: Any,\n):\n    \"\"\"Called when the node skips.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        skip_data (dict[str, Any]): Data related to the skip.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    if (run := self.runs.get(run_id)) is None:\n        run = self._get_node_base_run(serialized, **kwargs)\n        self.runs[run_id] = run\n\n    run.input = format_value(input_data)\n    run.end_time = run.start_time\n    run.status = RunStatus.SKIPPED\n    run.metadata[\"skip\"] = format_value(skip_data)\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_node_start","title":"<code>on_node_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the node starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized node data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_node_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the node starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized node data.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    run = self._get_node_base_run(serialized, **kwargs)\n    run.input = format_value(input_data)\n    self.runs[run_id] = run\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_workflow_end","title":"<code>on_workflow_end(serialized, output_data, **kwargs)</code>","text":"<p>Called when the workflow ends.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_workflow_end(\n    self, serialized: dict[str, Any], output_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow ends.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        output_data (dict[str, Any]): Output data from the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.output = format_value(output_data)\n    run.status = RunStatus.SUCCEEDED\n\n    self.flush()\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_workflow_error","title":"<code>on_workflow_error(serialized, error, **kwargs)</code>","text":"<p>Called when the workflow errors.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>error</code> <code>BaseException</code> <p>Error encountered.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_workflow_error(\n    self, serialized: dict[str, Any], error: BaseException, **kwargs: Any\n):\n    \"\"\"Called when the workflow errors.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        error (BaseException): Error encountered.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run = ensure_run(get_run_id(kwargs), self.runs)\n    run.end_time = datetime.now(UTC)\n    run.status = RunStatus.FAILED\n    run.error = {\n        \"message\": str(error),\n        \"traceback\": traceback.format_exc(),\n    }\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.TracingCallbackHandler.on_workflow_start","title":"<code>on_workflow_start(serialized, input_data, **kwargs)</code>","text":"<p>Called when the workflow starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized workflow data.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the workflow.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def on_workflow_start(\n    self, serialized: dict[str, Any], input_data: dict[str, Any], **kwargs: Any\n):\n    \"\"\"Called when the workflow starts.\n\n    Args:\n        serialized (dict[str, Any]): Serialized workflow data.\n        input_data (dict[str, Any]): Input data for the workflow.\n        **kwargs (Any): Additional arguments.\n    \"\"\"\n    run_id = get_run_id(kwargs)\n    self.runs[run_id] = Run(\n        id=run_id,\n        name=\"Workflow\",\n        type=RunType.WORKFLOW,\n        trace_id=self.trace_id,\n        source_id=self.source_id,\n        session_id=self.session_id,\n        start_time=datetime.now(UTC),\n        input=format_value(input_data),\n        metadata={\n            \"workflow\": {\"id\": serialized.get(\"id\"), \"version\": serialized.get(\"version\")},\n            \"host\": self.host,\n        },\n        tags=self.tags,\n    )\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ensure_execution_run","title":"<code>ensure_execution_run(execution_run_id, executions)</code>","text":"<p>Ensure the execution run exists in the executions list.</p> <p>Parameters:</p> Name Type Description Default <code>execution_run_id</code> <code>UUID</code> <p>Execution run ID.</p> required <code>executions</code> <code>list[ExecutionRun]</code> <p>List of execution runs.</p> required <p>Returns:</p> Name Type Description <code>ExecutionRun</code> <code>ExecutionRun</code> <p>The execution run corresponding to the execution run ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the execution run is not found.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def ensure_execution_run(execution_run_id: UUID, executions: list[ExecutionRun]) -&gt; ExecutionRun:\n    \"\"\"Ensure the execution run exists in the executions list.\n\n    Args:\n        execution_run_id (UUID): Execution run ID.\n        executions (list[ExecutionRun]): List of execution runs.\n\n    Returns:\n        ExecutionRun: The execution run corresponding to the execution run ID.\n\n    Raises:\n        ValueError: If the execution run is not found.\n    \"\"\"\n    for execution in executions:\n        if execution.id == execution_run_id:\n            return execution\n\n    raise ValueError(f\"execution run {execution_run_id} not found\")\n</code></pre>"},{"location":"dynamiq/callbacks/tracing/#dynamiq.callbacks.tracing.ensure_run","title":"<code>ensure_run(run_id, runs)</code>","text":"<p>Ensure the run exists in the runs dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>UUID</code> <p>Run ID.</p> required <code>runs</code> <code>dict[UUID, Run]</code> <p>Dictionary of runs.</p> required <p>Returns:</p> Name Type Description <code>Run</code> <code>Run</code> <p>The run corresponding to the run ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the run is not found.</p> Source code in <code>dynamiq/callbacks/tracing.py</code> <pre><code>def ensure_run(run_id: UUID, runs: dict[UUID, Run]) -&gt; Run:\n    \"\"\"Ensure the run exists in the runs dictionary.\n\n    Args:\n        run_id (UUID): Run ID.\n        runs (dict[UUID, Run]): Dictionary of runs.\n\n    Returns:\n        Run: The run corresponding to the run ID.\n\n    Raises:\n        ValueError: If the run is not found.\n    \"\"\"\n    run = runs.get(run_id)\n    if not run:\n        raise ValueError(f\"run {run_id} not found\")\n\n    return runs[run_id]\n</code></pre>"},{"location":"dynamiq/clients/base/","title":"Base","text":""},{"location":"dynamiq/clients/base/#dynamiq.clients.base.BaseTracingClient","title":"<code>BaseTracingClient</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tracing clients.</p> Source code in <code>dynamiq/clients/base.py</code> <pre><code>class BaseTracingClient(abc.ABC):\n    \"\"\"Abstract base class for tracing clients.\"\"\"\n\n    @abc.abstractmethod\n    def trace(self, runs: list[\"Run\"]) -&gt; None:\n        \"\"\"Trace the given runs.\n\n        Args:\n            runs (list[\"Run\"]): List of runs to trace.\n\n        Raises:\n            NotImplementedError: If not implemented.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/clients/base/#dynamiq.clients.base.BaseTracingClient.trace","title":"<code>trace(runs)</code>  <code>abstractmethod</code>","text":"<p>Trace the given runs.</p> <p>Parameters:</p> Name Type Description Default <code>runs</code> <code>list[Run]</code> <p>List of runs to trace.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented.</p> Source code in <code>dynamiq/clients/base.py</code> <pre><code>@abc.abstractmethod\ndef trace(self, runs: list[\"Run\"]) -&gt; None:\n    \"\"\"Trace the given runs.\n\n    Args:\n        runs (list[\"Run\"]): List of runs to trace.\n\n    Raises:\n        NotImplementedError: If not implemented.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/","title":"Serializers","text":""},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.BaseSerializer","title":"<code>BaseSerializer</code>","text":"<p>Base class for serializers providing interface for dumps and loads methods.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class BaseSerializer:\n    \"\"\"\n    Base class for serializers providing interface for dumps and loads methods.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Serialize the given value to a string.\n\n        Args:\n            value (Any): The value to be serialized.\n\n        Returns:\n            str: The serialized string representation of the value.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n\n    def loads(self, value: Any) -&gt; Any:\n        \"\"\"\n        Deserialize the given value from a string.\n\n        Args:\n            value (Any): The serialized string to be deserialized.\n\n        Returns:\n            Any: The deserialized value.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.BaseSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Serialize the given value to a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be serialized.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized string representation of the value.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be implemented by subclasses.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Serialize the given value to a string.\n\n    Args:\n        value (Any): The value to be serialized.\n\n    Returns:\n        str: The serialized string representation of the value.\n\n    Raises:\n        NotImplementedError: This method should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.BaseSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Deserialize the given value from a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The serialized string to be deserialized.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized value.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be implemented by subclasses.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: Any) -&gt; Any:\n    \"\"\"\n    Deserialize the given value from a string.\n\n    Args:\n        value (Any): The serialized string to be deserialized.\n\n    Returns:\n        Any: The deserialized value.\n\n    Raises:\n        NotImplementedError: This method should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonPickleSerializer","title":"<code>JsonPickleSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializer that uses jsonpickle to convert complex Python objects to and from JSON format.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class JsonPickleSerializer(BaseSerializer):\n    \"\"\"\n    Serializer that uses jsonpickle to convert complex Python objects to and from JSON format.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Serialize the given value to a JSON string using jsonpickle.\n\n        Args:\n            value (Any): The value to be serialized.\n\n        Returns:\n            str: The JSON string representation of the value.\n        \"\"\"\n        import jsonpickle\n\n        return jsonpickle.encode(value)\n\n    def loads(self, value: str) -&gt; Any:\n        \"\"\"\n        Deserialize the given JSON string to a Python object using jsonpickle.\n\n        Args:\n            value (str): The JSON string to be deserialized.\n\n        Returns:\n            Any: The deserialized Python object.\n        \"\"\"\n        import jsonpickle\n\n        return jsonpickle.decode(value)  # nosec\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonPickleSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Serialize the given value to a JSON string using jsonpickle.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be serialized.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string representation of the value.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Serialize the given value to a JSON string using jsonpickle.\n\n    Args:\n        value (Any): The value to be serialized.\n\n    Returns:\n        str: The JSON string representation of the value.\n    \"\"\"\n    import jsonpickle\n\n    return jsonpickle.encode(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonPickleSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Deserialize the given JSON string to a Python object using jsonpickle.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The JSON string to be deserialized.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Python object.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: str) -&gt; Any:\n    \"\"\"\n    Deserialize the given JSON string to a Python object using jsonpickle.\n\n    Args:\n        value (str): The JSON string to be deserialized.\n\n    Returns:\n        Any: The deserialized Python object.\n    \"\"\"\n    import jsonpickle\n\n    return jsonpickle.decode(value)  # nosec\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonSerializer","title":"<code>JsonSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializer that converts values to and from JSON format.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class JsonSerializer(BaseSerializer):\n    \"\"\"\n    Serializer that converts values to and from JSON format.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Serialize the given value to a JSON string.\n\n        Args:\n            value (Any): The value to be serialized to JSON.\n\n        Returns:\n            str: The JSON string representation of the value.\n        \"\"\"\n        return json.dumps(value, cls=JsonWorkflowEncoder)\n\n    def loads(self, value: str | None) -&gt; Any:\n        \"\"\"\n        Deserialize the given JSON string to a Python object.\n\n        Args:\n            value (str | None): The JSON string to be deserialized, or None.\n\n        Returns:\n            Any: The deserialized Python object, or None if the input is None.\n        \"\"\"\n        if value is None:\n            return None\n        return json.loads(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Serialize the given value to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be serialized to JSON.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string representation of the value.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Serialize the given value to a JSON string.\n\n    Args:\n        value (Any): The value to be serialized to JSON.\n\n    Returns:\n        str: The JSON string representation of the value.\n    \"\"\"\n    return json.dumps(value, cls=JsonWorkflowEncoder)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.JsonSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Deserialize the given JSON string to a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>The JSON string to be deserialized, or None.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The deserialized Python object, or None if the input is None.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: str | None) -&gt; Any:\n    \"\"\"\n    Deserialize the given JSON string to a Python object.\n\n    Args:\n        value (str | None): The JSON string to be deserialized, or None.\n\n    Returns:\n        Any: The deserialized Python object, or None if the input is None.\n    \"\"\"\n    if value is None:\n        return None\n    return json.loads(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.StringSerializer","title":"<code>StringSerializer</code>","text":"<p>               Bases: <code>BaseSerializer</code></p> <p>Serializer that converts values to and from strings.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>class StringSerializer(BaseSerializer):\n    \"\"\"\n    Serializer that converts values to and from strings.\n    \"\"\"\n\n    def dumps(self, value: Any) -&gt; str:\n        \"\"\"\n        Convert the given value to a string.\n\n        Args:\n            value (Any): The value to be converted to a string.\n\n        Returns:\n            str: The string representation of the value.\n        \"\"\"\n        return str(value)\n\n    def loads(self, value: Any) -&gt; Any:\n        \"\"\"\n        Return the input value as is, since it's already a string.\n\n        Args:\n            value (Any): The value to be deserialized (expected to be a string).\n\n        Returns:\n            Any: The input value, unchanged.\n        \"\"\"\n        return value\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.StringSerializer.dumps","title":"<code>dumps(value)</code>","text":"<p>Convert the given value to a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be converted to a string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string representation of the value.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def dumps(self, value: Any) -&gt; str:\n    \"\"\"\n    Convert the given value to a string.\n\n    Args:\n        value (Any): The value to be converted to a string.\n\n    Returns:\n        str: The string representation of the value.\n    \"\"\"\n    return str(value)\n</code></pre>"},{"location":"dynamiq/components/serializers/#dynamiq.components.serializers.StringSerializer.loads","title":"<code>loads(value)</code>","text":"<p>Return the input value as is, since it's already a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to be deserialized (expected to be a string).</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The input value, unchanged.</p> Source code in <code>dynamiq/components/serializers.py</code> <pre><code>def loads(self, value: Any) -&gt; Any:\n    \"\"\"\n    Return the input value as is, since it's already a string.\n\n    Args:\n        value (Any): The value to be deserialized (expected to be a string).\n\n    Returns:\n        Any: The input value, unchanged.\n    \"\"\"\n    return value\n</code></pre>"},{"location":"dynamiq/components/converters/base/","title":"Base","text":""},{"location":"dynamiq/components/converters/base/#dynamiq.components.converters.base.BaseConverter","title":"<code>BaseConverter</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/components/converters/base.py</code> <pre><code>class BaseConverter(BaseModel):\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(**kwargs)\n\n    def run(\n        self,\n        file_paths: list[str] | list[os.PathLike] | None = None,\n        files: list[BytesIO] | None = None,\n        metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Converts files to Documents using the PyPDF.\n\n        Processes file paths or BytesIO objects into Documents. Handles directories and files.\n\n        Args:\n            paths: List of file or directory paths to convert.\n            files: List of BytesIO objects to convert.\n            metadata: Metadata for documents. Can be a dict for all or a list of dicts for each.\n\n        Returns:\n            Dict with 'documents' key containing a list of created Documents.\n\n        Raises:\n            ValueError: If neither paths nor files provided, or if metadata is a list with\n                directory paths.\n        \"\"\"\n        if file_paths is None and files is None:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n\n        documents = []\n\n        if file_paths is not None:\n            paths_obj = [Path(path) for path in file_paths]\n            filepaths = [path for path in paths_obj if path.is_file()]\n            filepaths_in_directories = [\n                filepath for path in paths_obj if path.is_dir() for filepath in path.glob(\"*.*\") if filepath.is_file()\n            ]\n            if filepaths_in_directories and isinstance(metadata, list):\n                raise ValueError(\n                    \"If providing directories in the `paths` parameter, \"\n                    \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                    \"and not a list. To specify different metadata for each file, \"\n                    \"provide an explicit list of direct paths instead.\"\n                )\n\n            all_filepaths = set(filepaths + filepaths_in_directories)\n            meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n            for filepath, meta in zip(all_filepaths, meta_list):\n                documents.extend(self._process_file(filepath, meta))\n\n        if files is not None:\n            meta_list = self._normalize_metadata(metadata, len(files))\n            for file, meta in zip(files, meta_list):\n                documents.extend(self._process_file(file, meta))\n\n        return {\"documents\": documents}\n\n    @staticmethod\n    def _normalize_metadata(\n        metadata: dict[str, Any] | list[dict[str, Any]] | None, sources_count: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Normalizes metadata input for a converter.\n\n        Given all possible values of the metadata input for a converter (None, dictionary, or list of\n        dicts), ensures to return a list of dictionaries of the correct length for the converter to use.\n\n        Args:\n            metadata: The meta input of the converter, as-is. Can be None, a dictionary, or a list of\n                dictionaries.\n            sources_count: The number of sources the converter received.\n\n        Returns:\n            A list of dictionaries of the same length as the sources list.\n\n        Raises:\n            ValueError: If metadata is not None, a dictionary, or a list of dictionaries, or if the length\n                of the metadata list doesn't match the number of sources.\n        \"\"\"\n        if metadata is None:\n            return [{} for _ in range(sources_count)]\n        if isinstance(metadata, dict):\n            return [copy.deepcopy(metadata) for _ in range(sources_count)]\n        if isinstance(metadata, list):\n            metadata_count = len(metadata)\n            if sources_count != metadata_count:\n                raise ValueError(\n                    f\"The length of the metadata list [{metadata_count}] \"\n                    f\"must match the number of sources [{sources_count}].\"\n                )\n            return metadata\n        raise ValueError(\"metadata must be either None, a dictionary or a list of dictionaries.\")\n\n    @abstractmethod\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: Any,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        pass\n\n    @abstractmethod\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        pass\n</code></pre>"},{"location":"dynamiq/components/converters/base/#dynamiq.components.converters.base.BaseConverter.run","title":"<code>run(file_paths=None, files=None, metadata=None)</code>","text":"<p>Converts files to Documents using the PyPDF.</p> <p>Processes file paths or BytesIO objects into Documents. Handles directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <p>List of file or directory paths to convert.</p> required <code>files</code> <code>list[BytesIO] | None</code> <p>List of BytesIO objects to convert.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | list[dict[str, Any]] | None</code> <p>Metadata for documents. Can be a dict for all or a list of dicts for each.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of created Documents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither paths nor files provided, or if metadata is a list with directory paths.</p> Source code in <code>dynamiq/components/converters/base.py</code> <pre><code>def run(\n    self,\n    file_paths: list[str] | list[os.PathLike] | None = None,\n    files: list[BytesIO] | None = None,\n    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Converts files to Documents using the PyPDF.\n\n    Processes file paths or BytesIO objects into Documents. Handles directories and files.\n\n    Args:\n        paths: List of file or directory paths to convert.\n        files: List of BytesIO objects to convert.\n        metadata: Metadata for documents. Can be a dict for all or a list of dicts for each.\n\n    Returns:\n        Dict with 'documents' key containing a list of created Documents.\n\n    Raises:\n        ValueError: If neither paths nor files provided, or if metadata is a list with\n            directory paths.\n    \"\"\"\n    if file_paths is None and files is None:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n\n    documents = []\n\n    if file_paths is not None:\n        paths_obj = [Path(path) for path in file_paths]\n        filepaths = [path for path in paths_obj if path.is_file()]\n        filepaths_in_directories = [\n            filepath for path in paths_obj if path.is_dir() for filepath in path.glob(\"*.*\") if filepath.is_file()\n        ]\n        if filepaths_in_directories and isinstance(metadata, list):\n            raise ValueError(\n                \"If providing directories in the `paths` parameter, \"\n                \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                \"and not a list. To specify different metadata for each file, \"\n                \"provide an explicit list of direct paths instead.\"\n            )\n\n        all_filepaths = set(filepaths + filepaths_in_directories)\n        meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n        for filepath, meta in zip(all_filepaths, meta_list):\n            documents.extend(self._process_file(filepath, meta))\n\n    if files is not None:\n        meta_list = self._normalize_metadata(metadata, len(files))\n        for file, meta in zip(files, meta_list):\n            documents.extend(self._process_file(file, meta))\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/components/converters/pptx/","title":"Pptx","text":""},{"location":"dynamiq/components/converters/pptx/#dynamiq.components.converters.pptx.PPTXConverter","title":"<code>PPTXConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting files to Documents using the pptx library.</p> <pre><code>  Initializes the object with the configuration for converting documents using\n  the python-pptx.\n\n  Args:\n  document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"], optional):\n      Determines how to create Documents from the elements of presentation. Options are:\n      - `\"one-doc-per-file\"`: Creates one Document per file.\n          All elements are concatenated into one text field.\n      - `\"one-doc-per-page\"`: Creates one Document per page.\n          All elements on a page are concatenated into one text field.\n      Defaults to `\"one-doc-per-file\"`.\n  extraction_mode(ExtractionMode): Type of text extraction format.\n\nUsage example:\n    ```python\n    from dynamiq.components.converters.pptx import PPTXConverter\n\n    converter = PPTXConverter()\n    documents = converter.run(paths=[\"a/file/path.pptx\", \"a/directory/path\"])[\"documents\"]\n    ```\n</code></pre> Source code in <code>dynamiq/components/converters/pptx.py</code> <pre><code>class PPTXConverter(BaseConverter):\n    \"\"\"\n    A component for converting files to Documents using the pptx library.\n\n          Initializes the object with the configuration for converting documents using\n          the python-pptx.\n\n          Args:\n          document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"], optional):\n              Determines how to create Documents from the elements of presentation. Options are:\n              - `\"one-doc-per-file\"`: Creates one Document per file.\n                  All elements are concatenated into one text field.\n              - `\"one-doc-per-page\"`: Creates one Document per page.\n                  All elements on a page are concatenated into one text field.\n              Defaults to `\"one-doc-per-file\"`.\n          extraction_mode(ExtractionMode): Type of text extraction format.\n\n        Usage example:\n            ```python\n            from dynamiq.components.converters.pptx import PPTXConverter\n\n            converter = PPTXConverter()\n            documents = converter.run(paths=[\"a/file/path.pptx\", \"a/directory/path\"])[\"documents\"]\n            ```\n    \"\"\"\n\n    document_creation_mode: Literal[DocumentCreationMode.ONE_DOC_PER_FILE, DocumentCreationMode.ONE_DOC_PER_PAGE] = (\n        DocumentCreationMode.ONE_DOC_PER_FILE\n    )\n\n    def _process_file(self, file: Path | BytesIO, metadata: dict[str, Any]) -&gt; list[Any]:\n        \"\"\"\n        Process a single presentation and create documents.\n\n        Args:\n            file (Union[Path, BytesIO]): The file to process.\n            metadata (Dict[str, Any]): Metadata to attach to the documents.\n\n        Returns:\n            List[Any]: A list of created documents.\n\n        Raises:\n            ValueError: If the file object doesn't have a name and its extension can't be guessed.\n            TypeError: If the file argument is neither a Path nor a BytesIO object.\n        \"\"\"\n        if isinstance(file, Path):\n            with open(file, \"rb\") as upload_file:\n                file_content = BytesIO(upload_file.read())\n                file_path = upload_file.name\n        elif isinstance(file, BytesIO):\n            file_path = get_filename_for_bytesio(file)\n            file_content = file\n        else:\n            raise TypeError(\"Expected a Path object or a BytesIO object.\")\n        elements = Presentation(file_content)\n        return self._create_documents(\n            filepath=file_path,\n            elements=elements,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: Any,\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the elements of the presentation.\n        \"\"\"\n        docs = []\n        if document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            text_all_slides = \"\\n\".join(\n                \"\\n\".join(shape.text for shape in slide.shapes if hasattr(shape, \"text\") and shape.text)\n                for slide in elements.slides\n            )\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = filepath\n            docs = [Document(content=text_all_slides, metadata=metadata)]\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_PAGE:\n            texts_per_page = [\n                \"\\n\".join(shape.text for shape in slide.shapes if hasattr(shape, \"text\") and shape.text)\n                for slide in elements.slides\n            ]\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = filepath\n\n            docs = [Document(content=text, metadata=metadata) for text in texts_per_page]\n\n        return docs\n</code></pre>"},{"location":"dynamiq/components/converters/pypdf/","title":"Pypdf","text":"<p>pdf</p>"},{"location":"dynamiq/components/converters/unstructured/","title":"Unstructured","text":""},{"location":"dynamiq/components/converters/unstructured/#dynamiq.components.converters.unstructured.UnstructuredFileConverter","title":"<code>UnstructuredFileConverter</code>","text":"<p>               Bases: <code>BaseConverter</code></p> <p>A component for converting files to Documents using the Unstructured API (hosted or running locally).</p> <p>For the supported file types and the specific API parameters, see Unstructured docs.</p> <p>Usage example: <pre><code>from dynamiq.components.converters.unstructured.file_converter import UnstructuredFileConverter\n\n# make sure to either set the environment variable UNSTRUCTURED_API_KEY\n# or run the Unstructured API locally:\n# docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest\n# --port 8000 --host 0.0.0.0\n\nconverter = UnstructuredFileConverter()\ndocuments = converter.run(paths=[\"a/file/path.pdf\", \"a/directory/path\"])[\"documents\"]\n</code></pre></p> Source code in <code>dynamiq/components/converters/unstructured.py</code> <pre><code>class UnstructuredFileConverter(BaseConverter):\n    \"\"\"\n    A component for converting files to Documents using the Unstructured API (hosted or running locally).\n\n    For the supported file types and the specific API parameters, see\n    [Unstructured docs](https://unstructured-io.github.io/unstructured/api.html).\n\n    Usage example:\n    ```python\n    from dynamiq.components.converters.unstructured.file_converter import UnstructuredFileConverter\n\n    # make sure to either set the environment variable UNSTRUCTURED_API_KEY\n    # or run the Unstructured API locally:\n    # docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest\n    # --port 8000 --host 0.0.0.0\n\n    converter = UnstructuredFileConverter()\n    documents = converter.run(paths=[\"a/file/path.pdf\", \"a/directory/path\"])[\"documents\"]\n    ```\n    \"\"\"\n\n    connection: UnstructuredConnection = None\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    separator: str = \"\\n\\n\"\n    strategy: ConvertStrategy = ConvertStrategy.AUTO\n    unstructured_kwargs: dict[str, Any] | None = None\n\n    def __init__(self, *args, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = UnstructuredConnection()\n        super().__init__(**kwargs)\n        \"\"\"\n        Initializes the object with the configuration for converting documents using\n        the Unstructured API.\n\n        Args:\n        connection (UnstructuredConnection, optional): The connection to use for the Unstructured API.\n            Defaults to None, which will initialize a new UnstructuredConnection.\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"], optional):\n            Determines how to create Documents from the elements returned by Unstructured. Options are:\n            - `\"one-doc-per-file\"`: Creates one Document per file.\n                All elements are concatenated into one text field.\n            - `\"one-doc-per-page\"`: Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            - `\"one-doc-per-element\"`: Creates one Document per element.\n                Each element is converted to a separate Document.\n            Defaults to `\"one-doc-per-file\"`.\n        separator (str, optional): The separator to use between elements when concatenating them into one text field.\n            Defaults to \"\\n\\n\".\n        strategy (Literal[\"auto\", \"fast\", \"hi_res\", \"ocr_only\"], optional): The strategy to use for document processing.\n            Defaults to \"auto\".\n        unstructured_kwargs (Optional[dict[str, Any]], optional): Additional parameters to pass to the Unstructured API.\n            See [Unstructured API docs](https://unstructured-io.github.io/unstructured/apis/api_parameters.html)\n                for available parameters.\n            Defaults to None.\n        progress_bar (bool, optional): Whether to show a progress bar during the conversion process.\n            Defaults to True.\n\n        Returns:\n        None\n        \"\"\"\n\n    def _process_file(\n        self, file: Path | BytesIO, metadata: dict[str, Any]\n    ) -&gt; list[Any]:\n        \"\"\"\n        Process a single file and create documents.\n\n        Args:\n            file (Union[Path, BytesIO]): The file to process.\n            metadata (Dict[str, Any]): Metadata to attach to the documents.\n\n        Returns:\n            List[Any]: A list of created documents.\n\n        Raises:\n            ValueError: If the file object doesn't have a name and its extension can't be guessed.\n            TypeError: If the file argument is neither a Path nor a BytesIO object.\n        \"\"\"\n        if isinstance(file, Path):\n            file_name = str(file)\n            elements = self._partition_file_into_elements_by_filepath(file_name)\n        elif isinstance(file, BytesIO):\n            file_name = get_filename_for_bytesio(file)\n            elements = self._partition_file_into_elements_by_file(file, file_name)\n        else:\n            raise TypeError(\"Expected a Path object or a BytesIO object.\")\n        return self._create_documents(\n            filepath=file_name,\n            elements=elements,\n            document_creation_mode=self.document_creation_mode,\n            metadata=metadata,\n        )\n\n    def _partition_file_into_elements_by_filepath(\n        self, filepath: Path\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Partition a file into elements using the Unstructured API.\n\n        Args:\n            filepath (Path): The path to the file to partition.\n\n        Returns:\n            List[Dict[str, Any]]: A list of elements extracted from the file.\n        \"\"\"\n        try:\n            return partition_via_api(\n                filename=str(filepath),\n                api_url=self.connection.url,\n                api_key=self.connection.api_key,\n                strategy=self.strategy,\n                **self.unstructured_kwargs or {},\n            )\n        except Exception as e:\n            logger.warning(\n                f\"Unstructured could not process file {filepath}. Error: {e}\"\n            )\n            return []\n\n    def _partition_file_into_elements_by_file(\n        self,\n        file: BytesIO,\n        metadata_filename: str,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Partition a file into elements using the Unstructured API.\n\n        Args:\n            file (BytesIO): The file object to partition.\n            metadata_filename (str): The filename to store in element metadata.\n\n        Returns:\n            List[Dict[str, Any]]: A list of elements extracted from the file.\n        \"\"\"\n        try:\n            return partition_via_api(\n                filename=None,\n                file=file,\n                metadata_filename=metadata_filename,\n                api_url=self.connection.url,\n                api_key=self.connection.api_key,\n                strategy=self.strategy,\n                **self.unstructured_kwargs or {},\n            )\n        except Exception as e:\n            logger.warning(\n                f\"Unstructured could not process file {metadata_filename}. Error: {e}\"\n            )\n            return []\n\n    def _create_documents(\n        self,\n        filepath: str,\n        elements: list[dict],\n        document_creation_mode: DocumentCreationMode,\n        metadata: dict[str, Any],\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Create Documents from the elements returned by Unstructured.\n        \"\"\"\n        separator = self.separator\n        docs = []\n        if document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            element_texts = []\n            for el in elements:\n                text = str(el.get(\"text\", \"\"))\n                if el.get(\"category\") == \"Title\":\n                    element_texts.append(\"# \" + text)\n                else:\n                    element_texts.append(text)\n\n            text = separator.join(element_texts)\n            metadata = copy.deepcopy(metadata)\n            metadata[\"file_path\"] = str(filepath)\n            docs = [Document(content=text, metadata=metadata)]\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_PAGE:\n            texts_per_page: defaultdict[int, str] = defaultdict(str)\n            meta_per_page: defaultdict[int, dict] = defaultdict(dict)\n            for el in elements:\n                text = str(el.get(\"text\", \"\"))\n                metadata = copy.deepcopy(metadata)\n                metadata[\"file_path\"] = str(filepath)\n                element_medata = el.get(\"metadata\")\n                if element_medata:\n                    metadata.update(element_medata)\n                page_number = int(metadata.get(\"page_number\", 1))\n\n                texts_per_page[page_number] += text + separator\n                meta_per_page[page_number].update(metadata)\n\n            docs = [\n                Document(content=texts_per_page[page], metadata=meta_per_page[page])\n                for page in texts_per_page.keys()\n            ]\n\n        elif document_creation_mode == DocumentCreationMode.ONE_DOC_PER_ELEMENT:\n            for index, el in enumerate(elements):\n                text = str(el.get(\"text\", \"\"))\n                metadata = copy.deepcopy(metadata)\n                metadata[\"file_path\"] = str(filepath)\n                metadata[\"element_index\"] = index\n                element_medata = el.get(\"metadata\")\n                if element_medata:\n                    metadata.update(element_medata)\n                element_category = el.get(\"category\")\n                if element_category:\n                    metadata[\"category\"] = element_category\n                doc = Document(content=str(el), metadata=metadata)\n                docs.append(doc)\n        return docs\n</code></pre>"},{"location":"dynamiq/components/converters/unstructured/#dynamiq.components.converters.unstructured.partition_via_api","title":"<code>partition_via_api(filename=None, content_type=None, file=None, file_filename=None, api_url='https://api.unstructured.io/', api_key='', metadata_filename=None, **request_kwargs)</code>","text":"<p>Partitions a document using the Unstructured REST API. This is equivalent to running the document through partition.</p> <p>See https://api.unstructured.io/general/docs for the hosted API documentation or https://github.com/Unstructured-IO/unstructured-api for instructions on how to run the API locally as a container.</p>"},{"location":"dynamiq/components/converters/unstructured/#dynamiq.components.converters.unstructured.partition_via_api--parameters","title":"Parameters","text":"<p>filename     A string defining the target filename path. content_type     A string defining the file content in MIME type file     A file-like object using \"rb\" mode --&gt; open(filename, \"rb\"). metadata_filename     When file is not None, the filename (string) to store in element metadata. E.g. \"foo.txt\" api_url     The URL for the Unstructured API. Defaults to the hosted Unstructured API. api_key     The API key to pass to the Unstructured API. request_kwargs     Additional parameters to pass to the data field of the request to the Unstructured API.     For example the <code>strategy</code> parameter.</p> Source code in <code>dynamiq/components/converters/unstructured.py</code> <pre><code>def partition_via_api(\n    filename: str | None = None,\n    content_type: str | None = None,\n    file: IO[bytes] | None = None,\n    file_filename: str | None = None,\n    api_url: str = \"https://api.unstructured.io/\",\n    api_key: str = \"\",\n    metadata_filename: str | None = None,\n    **request_kwargs,\n) -&gt; list[dict]:\n    \"\"\"Partitions a document using the Unstructured REST API. This is equivalent to\n    running the document through partition.\n\n    See https://api.unstructured.io/general/docs for the hosted API documentation or\n    https://github.com/Unstructured-IO/unstructured-api for instructions on how to run\n    the API locally as a container.\n\n    Parameters\n    ----------\n    filename\n        A string defining the target filename path.\n    content_type\n        A string defining the file content in MIME type\n    file\n        A file-like object using \"rb\" mode --&gt; open(filename, \"rb\").\n    metadata_filename\n        When file is not None, the filename (string) to store in element metadata. E.g. \"foo.txt\"\n    api_url\n        The URL for the Unstructured API. Defaults to the hosted Unstructured API.\n    api_key\n        The API key to pass to the Unstructured API.\n    request_kwargs\n        Additional parameters to pass to the data field of the request to the Unstructured API.\n        For example the `strategy` parameter.\n    \"\"\"\n\n    if metadata_filename and file_filename:\n        raise ValueError(\n            \"Only one of metadata_filename and file_filename is specified. \"\n            \"metadata_filename is preferred. file_filename is marked for deprecation.\",\n        )\n\n    if file_filename is not None:\n        metadata_filename = file_filename\n        logger.warn(\n            \"The file_filename kwarg will be deprecated in a future version of unstructured. \"\n            \"Please use metadata_filename instead.\",\n        )\n\n    # Note(austin) - the sdk takes the base url, but we have the full api_url\n    # For consistency, just strip off the path when it's given\n    base_url = api_url[:-19] if \"/general/v0/general\" in api_url else api_url\n    sdk = UnstructuredClient(api_key_auth=api_key, server_url=base_url)\n\n    files = None\n    if filename is not None:\n        with open(filename, \"rb\") as f:\n            files = shared.Files(\n                content=f.read(),\n                file_name=filename,\n            )\n\n    elif file is not None:\n        if metadata_filename is None:\n            raise ValueError(\n                \"If file is specified in partition_via_api, \"\n                \"metadata_filename must be specified as well.\",\n            )\n        files = shared.Files(\n            content=file,\n            file_name=metadata_filename,\n        )\n\n    req = shared.PartitionParameters(\n        files=files,\n        **request_kwargs,\n    )\n    response = sdk.general.partition(req)\n\n    if response.status_code == 200:\n        element_dict = json.loads(response.raw_response.text)\n        return element_dict\n    else:\n        raise ValueError(\n            f\"Receive unexpected status code {response.status_code} from the API.\",\n        )\n</code></pre>"},{"location":"dynamiq/components/converters/utils/","title":"Utils","text":""},{"location":"dynamiq/components/converters/utils/#dynamiq.components.converters.utils.get_filename_for_bytesio","title":"<code>get_filename_for_bytesio(file)</code>","text":"<p>Get a filepath for a BytesIO object.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>BytesIO</code> <p>The BytesIO object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A filename for the BytesIO object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension couldn't be guessed.</p> Source code in <code>dynamiq/components/converters/utils.py</code> <pre><code>def get_filename_for_bytesio(file: BytesIO) -&gt; str:\n    \"\"\"\n    Get a filepath for a BytesIO object.\n\n    Args:\n        file (BytesIO): The BytesIO object.\n\n    Returns:\n        str: A filename for the BytesIO object.\n\n    Raises:\n        ValueError: If the file extension couldn't be guessed.\n    \"\"\"\n    filename = getattr(file, \"name\", None)\n    if filename is None:\n        file_extension = filetype.guess_extension(file)\n        if file_extension:\n            filename = f\"{generate_uuid()}.{file_extension}\"\n        else:\n            raise ValueError(\n                \"Unable to determine file extension. BytesIO object lacks name and \"\n                \"extension couldn't be guessed.\"\n            )\n    return filename\n</code></pre>"},{"location":"dynamiq/components/embedders/base/","title":"Base","text":""},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder","title":"<code>BaseEmbedder</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>class BaseEmbedder(BaseModel):\n    model: str\n    connection: BaseConnection\n    prefix: str = \"\"\n    suffix: str = \"\"\n    batch_size: int = 32\n    meta_fields_to_embed: list[str] | None = []\n    embedding_separator: str = \"\\n\"\n    truncate: str | None = None\n    input_type: str | None = None\n    dimensions: int | None = None\n    client: Any | None = None\n\n    _embedding: Callable = PrivateAttr()\n    \"\"\"\n        Initializes the Embedder component with given configuration.\n\n        Attributes:\n            connection (Optional[BaseConnection]): The connection to the  API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding.\n            prefix (str): A prefix string to prepend to each document text before embedding.\n            suffix (str): A suffix string to append to each document text after embedding.\n            batch_size (int): The number of documents to encode in a single batch.\n            meta_fields_to_embed (Optional[list[str]]): A list of document meta fields to embed alongside\n                the document text.\n            embedding_separator (str): The separator string used to join document text with meta fields\n                for embedding.\n            truncate(str): truncate embeddings that are too long from start or end, (\"NONE\"|\"START\"|\"END\").\n                Passing \"START\" will discard the start of the input. \"END\" will discard the end of the input. In both\n                cases, input is discarded until the remaining input is exactly the maximum input token length\n                for the model. If \"NONE\" is selected, when the input exceeds the maximum input token length\n                an error will be returned.\n            input_type(str):specifies the type of input you're giving to the model. Supported values are\n                \"search_document\", \"search_query\", \"classification\" and \"clustering\".\n            dimensions(int):he number of dimensions the resulting output embeddings should have.\n                Only supported in OpenAI/Azure text-embedding-3 and later models.\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Import in runtime to save memory\n        super().__init__(**kwargs)\n        from litellm import embedding\n\n        self._embedding = embedding\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = self.connection.conn_params\n        if self.client:\n            params = {\"client\": self.client}\n        return params\n\n    def embed_text(self, text: str) -&gt; dict:\n        \"\"\"\n        Embeds a single string using the Embedder model specified during the initialization of the component.\n\n        Args:\n            text (str): The text string to be embedded.\n\n        Returns:\n            dict: A dictionary containing:\n                - 'embedding': A list representing the embedding vector of the input text.\n                - 'meta': A dictionary with metadata information about the model usage.\n        \"\"\"\n        if not isinstance(text, str):\n            msg = (\n                \"TextEmbedder expects a string as input.\"\n                \"In case you want to embed a list of Documents, please use the DocumentEmbedder.\"\n            )\n            raise TypeError(msg)\n\n        text_to_embed = self.prefix + text + self.suffix\n        text_to_embed = text_to_embed.replace(\"\\n\", \" \")\n\n        response = self._embedding(\n            model=self.model, input=[text_to_embed], **self.embed_params\n        )\n\n        meta = {\"model\": response.model, \"usage\": dict(response.usage)}\n\n        return {\"embedding\": response.data[0][\"embedding\"], \"meta\": meta}\n\n    def _prepare_documents_to_embed(self, documents: list[Document]) -&gt; list[str]:\n        \"\"\"\n        Prepare the texts to embed by concatenating the Document text with the metadata fields to embed.\n\n        Args:\n            documents (list[Document]): A list of Document objects to prepare for embedding.\n\n        Returns:\n            list[str]: A list of concatenated strings ready for embedding.\n        \"\"\"\n        texts_to_embed: list[str] = []\n        for doc in documents:\n            meta_values_to_embed = [\n                str(doc.meta[key])\n                for key in self.meta_fields_to_embed\n                if doc.meta.get(key) is not None\n            ]\n\n            text_to_embed = self.embedding_separator.join(\n                meta_values_to_embed + [doc.content or \"\"]\n            )\n            texts_to_embed.append(text_to_embed)\n        return texts_to_embed\n\n    def _embed_texts_batch(\n        self, texts_to_embed: list[str], batch_size: int\n    ) -&gt; tuple[list[list[float]], dict[str, Any]]:\n        \"\"\"\n        Embed a list of texts in batches.\n        \"\"\"\n        all_embeddings = []\n        meta: dict[str, Any] = {}\n        embed_params = self.embed_params\n        for i in range(0, len(texts_to_embed), batch_size):\n            batch = texts_to_embed[i : i + batch_size]\n            response = self._embedding(model=self.model, input=batch, **embed_params)\n            embeddings = [el[\"embedding\"] for el in response.data]\n            all_embeddings.extend(embeddings)\n\n            if \"model\" not in meta:\n                meta[\"model\"] = response.model\n            if \"usage\" not in meta:\n                meta[\"usage\"] = dict(response.usage)\n            else:\n                meta[\"usage\"][\"prompt_tokens\"] += response.usage.prompt_tokens\n                meta[\"usage\"][\"total_tokens\"] += response.usage.total_tokens\n\n        return all_embeddings, meta\n\n    def embed_documents(self, documents: list[Document]) -&gt; dict:\n        \"\"\"\n        Embeds a list of documents and returns the embedded documents along with meta information.\n\n        Args:\n            documents (list[Document]): The documents to be embedded.\n\n        Returns:\n            dict: A dictionary containing:\n                - 'documents' (list[Document]): The input documents with their embeddings populated.\n                - 'meta' (dict): Metadata information about the embedding process.\n        \"\"\"\n        if (\n            not isinstance(documents, list)\n            or documents\n            and not isinstance(documents[0], Document)\n        ):\n            msg = (\n                \"DocumentEmbedder expects a list of Documents as input.\"\n                \"In case you want to embed a string, please use the embed_text.\"\n            )\n            raise TypeError(msg)\n\n        if not documents:\n            # return early if we were passed an empty list\n            return {\"documents\": [], \"meta\": {}}\n\n        texts_to_embed = self._prepare_documents_to_embed(documents=documents)\n\n        embeddings, meta = self._embed_texts_batch(\n            texts_to_embed=texts_to_embed, batch_size=self.batch_size\n        )\n\n        for doc, emb in zip(documents, embeddings):\n            doc.embedding = emb\n\n        return {\"documents\": documents, \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder.embed_documents","title":"<code>embed_documents(documents)</code>","text":"<p>Embeds a list of documents and returns the embedded documents along with meta information.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>The documents to be embedded.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - 'documents' (list[Document]): The input documents with their embeddings populated. - 'meta' (dict): Metadata information about the embedding process.</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>def embed_documents(self, documents: list[Document]) -&gt; dict:\n    \"\"\"\n    Embeds a list of documents and returns the embedded documents along with meta information.\n\n    Args:\n        documents (list[Document]): The documents to be embedded.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'documents' (list[Document]): The input documents with their embeddings populated.\n            - 'meta' (dict): Metadata information about the embedding process.\n    \"\"\"\n    if (\n        not isinstance(documents, list)\n        or documents\n        and not isinstance(documents[0], Document)\n    ):\n        msg = (\n            \"DocumentEmbedder expects a list of Documents as input.\"\n            \"In case you want to embed a string, please use the embed_text.\"\n        )\n        raise TypeError(msg)\n\n    if not documents:\n        # return early if we were passed an empty list\n        return {\"documents\": [], \"meta\": {}}\n\n    texts_to_embed = self._prepare_documents_to_embed(documents=documents)\n\n    embeddings, meta = self._embed_texts_batch(\n        texts_to_embed=texts_to_embed, batch_size=self.batch_size\n    )\n\n    for doc, emb in zip(documents, embeddings):\n        doc.embedding = emb\n\n    return {\"documents\": documents, \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/base/#dynamiq.components.embedders.base.BaseEmbedder.embed_text","title":"<code>embed_text(text)</code>","text":"<p>Embeds a single string using the Embedder model specified during the initialization of the component.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text string to be embedded.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - 'embedding': A list representing the embedding vector of the input text. - 'meta': A dictionary with metadata information about the model usage.</p> Source code in <code>dynamiq/components/embedders/base.py</code> <pre><code>def embed_text(self, text: str) -&gt; dict:\n    \"\"\"\n    Embeds a single string using the Embedder model specified during the initialization of the component.\n\n    Args:\n        text (str): The text string to be embedded.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'embedding': A list representing the embedding vector of the input text.\n            - 'meta': A dictionary with metadata information about the model usage.\n    \"\"\"\n    if not isinstance(text, str):\n        msg = (\n            \"TextEmbedder expects a string as input.\"\n            \"In case you want to embed a list of Documents, please use the DocumentEmbedder.\"\n        )\n        raise TypeError(msg)\n\n    text_to_embed = self.prefix + text + self.suffix\n    text_to_embed = text_to_embed.replace(\"\\n\", \" \")\n\n    response = self._embedding(\n        model=self.model, input=[text_to_embed], **self.embed_params\n    )\n\n    meta = {\"model\": response.model, \"usage\": dict(response.usage)}\n\n    return {\"embedding\": response.data[0][\"embedding\"], \"meta\": meta}\n</code></pre>"},{"location":"dynamiq/components/embedders/bedrock/","title":"Bedrock","text":""},{"location":"dynamiq/components/embedders/bedrock/#dynamiq.components.embedders.bedrock.BedrockEmbedder","title":"<code>BedrockEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> Source code in <code>dynamiq/components/embedders/bedrock.py</code> <pre><code>class BedrockEmbedder(BaseEmbedder):\n    connection: BedrockConnection\n    model: str = \"amazon.titan-embed-text-v1\"\n    \"\"\"\n        Initializes the BedrockEmbedder component with given configuration.\n\n        Attributes:\n            connection (BedrockConnection): The connection to the  Bedrock API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding. Defaults to \"amazon.titan-embed-text-v1\".\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = BedrockConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n        if \"cohere\" in self.model:\n            params[\"input_type\"] = self.input_type\n            if self.truncate:\n                params[\"truncate\"] = self.truncate\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/bedrock/#dynamiq.components.embedders.bedrock.BedrockEmbedder.model","title":"<code>model: str = 'amazon.titan-embed-text-v1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Initializes the BedrockEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AWS</code> <p>The connection to the  Bedrock API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"amazon.titan-embed-text-v1\".</p>"},{"location":"dynamiq/components/embedders/cohere/","title":"Cohere","text":""},{"location":"dynamiq/components/embedders/cohere/#dynamiq.components.embedders.cohere.CohereEmbedder","title":"<code>CohereEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> Source code in <code>dynamiq/components/embedders/cohere.py</code> <pre><code>class CohereEmbedder(BaseEmbedder):\n    connection: CohereConnection\n    model: str = \"cohere/embed-english-v2.0\"\n    input_type: str = \"search_query\"\n    \"\"\"\n        Initializes the CohereEmbedder component with given configuration.\n\n        Attributes:\n            connection (CohereConnection): The connection to the  Cohere API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding. Defaults to \"cohere/embed-english-v2.0\"\n            input_type (str): Specifies the type of input you're giving to the model. Defaults to \"search_query\"\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n        params[\"input_type\"] = self.input_type\n        if self.truncate:\n            params[\"truncate\"] = self.truncate\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/cohere/#dynamiq.components.embedders.cohere.CohereEmbedder.input_type","title":"<code>input_type: str = 'search_query'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Initializes the CohereEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Cohere</code> <p>The connection to the  Cohere API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"cohere/embed-english-v2.0\"</p> <code>input_type</code> <code>str</code> <p>Specifies the type of input you're giving to the model. Defaults to \"search_query\"</p>"},{"location":"dynamiq/components/embedders/huggingface/","title":"Huggingface","text":""},{"location":"dynamiq/components/embedders/huggingface/#dynamiq.components.embedders.huggingface.HuggingFaceEmbedder","title":"<code>HuggingFaceEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> Source code in <code>dynamiq/components/embedders/huggingface.py</code> <pre><code>class HuggingFaceEmbedder(BaseEmbedder):\n    connection: HuggingFaceConnection\n    model: str = \"huggingface/BAAI/bge-large-zh\"\n    \"\"\"\n        Initializes the HuggingFaceEmbedder component with given configuration.\n\n        Attributes:\n            connection (HuggingFaceConnection): The connection to the  HuggingFace API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding. Defaults to \"huggingface/BAAI/bge-large-zh\"\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/components/embedders/huggingface/#dynamiq.components.embedders.huggingface.HuggingFaceEmbedder.model","title":"<code>model: str = 'huggingface/BAAI/bge-large-zh'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Initializes the HuggingFaceEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>HuggingFace</code> <p>The connection to the  HuggingFace API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"huggingface/BAAI/bge-large-zh\"</p>"},{"location":"dynamiq/components/embedders/mistral/","title":"Mistral","text":""},{"location":"dynamiq/components/embedders/mistral/#dynamiq.components.embedders.mistral.MistralEmbedder","title":"<code>MistralEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> Source code in <code>dynamiq/components/embedders/mistral.py</code> <pre><code>class MistralEmbedder(BaseEmbedder):\n    connection: MistralConnection\n    model: str = \"mistral/mistral-embed\"\n    \"\"\"\n        Initializes the MistralEmbedder component with given configuration.\n\n        Attributes:\n            connection (MistralConnection): The connection to the  Mistral API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding. Defaults to \"mistral/mistral-embed\"\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/components/embedders/mistral/#dynamiq.components.embedders.mistral.MistralEmbedder.model","title":"<code>model: str = 'mistral/mistral-embed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Initializes the MistralEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Mistral</code> <p>The connection to the  Mistral API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"mistral/mistral-embed\"</p>"},{"location":"dynamiq/components/embedders/openai/","title":"Openai","text":""},{"location":"dynamiq/components/embedders/openai/#dynamiq.components.embedders.openai.OpenAIEmbedder","title":"<code>OpenAIEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> <p>Provides functionality to compute embeddings for documents using OpenAI's models.</p> <p>This class leverages the OpenAI API to generate embeddings for given text documents. It's designed to work with instances of the Document class from the dynamiq package. The embeddings generated can be used for tasks such as similarity search, clustering, and more.</p> Example <p>from dynamiq.types import Document from dynamiq.components.embedders.openai import OpenAIEmbedder</p> <p>doc = Document(content=\"I love pizza!\")</p> <p>document_embedder = OpenAIEmbedder()</p> <p>result = document_embedder.run([doc]) print(result\"documents\".embedding) [0.017020374536514282, -0.023255806416273117, ...]</p> Note <p>An OpenAI API key must be provided either via environment variables or when creating an instance of OpenAIDocumentEmbedder through the OpenAIConnection.</p> Source code in <code>dynamiq/components/embedders/openai.py</code> <pre><code>class OpenAIEmbedder(BaseEmbedder):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using OpenAI's models.\n\n    This class leverages the OpenAI API to generate embeddings for given text documents. It's designed to work\n    with instances of the Document class from the dynamiq package. The embeddings generated can be used for tasks\n    such as similarity search, clustering, and more.\n\n\n    Example:\n        &gt;&gt;&gt; from dynamiq.types import Document\n        &gt;&gt;&gt; from dynamiq.components.embedders.openai import OpenAIEmbedder\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; doc = Document(content=\"I love pizza!\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; document_embedder = OpenAIEmbedder()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; result = document_embedder.run([doc])\n        &gt;&gt;&gt; print(result[\"documents\"][0].embedding)\n        [0.017020374536514282, -0.023255806416273117, ...]\n\n    Note:\n        An OpenAI API key must be provided either via environment variables or when creating an instance of\n        OpenAIDocumentEmbedder through the OpenAIConnection.\n    \"\"\"\n\n    connection: OpenAIConnection | None = None\n    model: str = \"text-embedding-3-small\"\n    \"\"\"\n        Initializes the OpenAIEmbedder component with given configuration.\n\n        Attributes:\n            connection (OpenAIConnection): The connection to the  OpenAI API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding. Defaults to \"text-embedding-3-small\"\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def embed_params(self) -&gt; dict:\n        params = super().embed_params\n        if self.dimensions:\n            params[\"dimensions\"] = self.dimensions\n\n        return params\n</code></pre>"},{"location":"dynamiq/components/embedders/openai/#dynamiq.components.embedders.openai.OpenAIEmbedder.model","title":"<code>model: str = 'text-embedding-3-small'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Initializes the OpenAIEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>OpenAI</code> <p>The connection to the  OpenAI API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"text-embedding-3-small\"</p>"},{"location":"dynamiq/components/embedders/watsonx/","title":"Watsonx","text":""},{"location":"dynamiq/components/embedders/watsonx/#dynamiq.components.embedders.watsonx.WatsonXEmbedder","title":"<code>WatsonXEmbedder</code>","text":"<p>               Bases: <code>BaseEmbedder</code></p> Source code in <code>dynamiq/components/embedders/watsonx.py</code> <pre><code>class WatsonXEmbedder(BaseEmbedder):\n    connection: WatsonXConnection\n    model: str = \"watsonx/ibm/slate-30m-english-rtrvr\"\n    \"\"\"\n        Initializes the WatsonXEmbedder component with given configuration.\n\n        Attributes:\n            connection (WatsonXConnection): The connection to the  WatsonX API. A new connection\n                is created if none is provided.\n            model (str): The model name to use for embedding. Defaults to \"watsonx/ibm/slate-30m-english-rtrvr\"\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/components/embedders/watsonx/#dynamiq.components.embedders.watsonx.WatsonXEmbedder.model","title":"<code>model: str = 'watsonx/ibm/slate-30m-english-rtrvr'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Initializes the WatsonXEmbedder component with given configuration.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>WatsonX</code> <p>The connection to the  WatsonX API. A new connection is created if none is provided.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to \"watsonx/ibm/slate-30m-english-rtrvr\"</p>"},{"location":"dynamiq/components/evaluators/llm_evaluator/","title":"Llm evaluator","text":""},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator","title":"<code>LLMEvaluator</code>","text":"<p>Uses an LLM to evaluate inputs based on a prompt containing instructions and examples.</p> <p>The inputs are lists that are user-defined depending on the desired metric. The output is a dictionary with a key <code>results</code> containing a list of evaluation results. Each result is a dictionary with user-defined keys and values of either 0 for FALSE or 1 for TRUE.</p> <p>Usage example: <pre><code>from typing import List\nfrom dynamiq.components.evaluators import LLMEvaluator\nevaluator = LLMEvaluator(\n    instructions=\"Is this answer problematic for children?\",\n    inputs=[(\"predicted_answers\", List[str])],\n    outputs=[\"score\"],\n    examples=[\n        {\"inputs\": {\"predicted_answers\": \"Damn, this is straight outta hell!!!\"},\n         \"outputs\": {\"score\": 1}},\n        {\"inputs\": {\"predicted_answers\": \"Football is the most popular sport.\"},\n         \"outputs\": {\"score\": 0}},\n    ],\n)\npredicted_answers = [\n    \"Football is the most popular sport with around 4 billion followers worldwide\",\n    \"Python language was created by Guido van Rossum.\",\n]\nresults = evaluator.run(predicted_answers=predicted_answers)\nprint(results)\n# {'results': [{'score': 0}, {'score': 0}]}\n</code></pre></p> Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>class LLMEvaluator:\n    \"\"\"\n    Uses an LLM to evaluate inputs based on a prompt containing instructions and examples.\n\n    The inputs are lists that are user-defined depending on the desired metric. The output\n    is a dictionary with a key `results` containing a list of evaluation results. Each result\n    is a dictionary with user-defined keys and values of either 0 for FALSE or 1 for TRUE.\n\n    Usage example:\n    ```python\n    from typing import List\n    from dynamiq.components.evaluators import LLMEvaluator\n    evaluator = LLMEvaluator(\n        instructions=\"Is this answer problematic for children?\",\n        inputs=[(\"predicted_answers\", List[str])],\n        outputs=[\"score\"],\n        examples=[\n            {\"inputs\": {\"predicted_answers\": \"Damn, this is straight outta hell!!!\"},\n             \"outputs\": {\"score\": 1}},\n            {\"inputs\": {\"predicted_answers\": \"Football is the most popular sport.\"},\n             \"outputs\": {\"score\": 0}},\n        ],\n    )\n    predicted_answers = [\n        \"Football is the most popular sport with around 4 billion followers worldwide\",\n        \"Python language was created by Guido van Rossum.\",\n    ]\n    results = evaluator.run(predicted_answers=predicted_answers)\n    print(results)\n    # {'results': [{'score': 0}, {'score': 0}]}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        instructions: str,\n        inputs: list[tuple[str, type[list]]],\n        outputs: list[str],\n        examples: list[dict[str, Any]],\n        *,\n        raise_on_failure: bool = True,\n        llm: Node,\n    ):\n        \"\"\"\n        Creates an instance of LLMEvaluator.\n\n        Args:\n            instructions (str): The prompt instructions to use for evaluation.\n            inputs (list[tuple[str, type[list]]]): The inputs that the component expects as incoming\n                connections and that it evaluates. Each input is a tuple of an input name and input\n                type. Input types must be lists.\n            outputs (list[str]): Output names of the evaluation results. They correspond to keys in\n                the output dictionary.\n            examples (list[dict[str, Any]]): Few-shot examples conforming to the expected input and\n                output format as defined in the `inputs` and `outputs` parameters. Each example is a\n                dictionary with keys \"inputs\" and \"outputs\". They contain the input and output as\n                dictionaries respectively.\n            raise_on_failure (bool): If True, the component will raise an exception on an\n                unsuccessful API call.\n            llm (Node): The LLM node to use for evaluation.\n        \"\"\"\n        self.validate_init_parameters(inputs, outputs, examples)\n        self.raise_on_failure = raise_on_failure\n        self.instructions = instructions\n        self.inputs = inputs\n        self.outputs = outputs\n        self.examples = examples\n        self.api_params = {}\n\n        default_generation_kwargs = {\n            \"response_format\": {\"type\": \"json_object\"},\n            \"seed\": 42,\n        }\n        user_generation_kwargs = self.api_params.get(\"generation_kwargs\", {})\n        merged_generation_kwargs = {\n            **default_generation_kwargs,\n            **user_generation_kwargs,\n        }\n        self.api_params[\"generation_kwargs\"] = merged_generation_kwargs\n\n        template = self.prepare_template()\n        message = Message(role=\"user\", content=template)\n        self.prompt = Prompt(messages=[message])\n        self.builder = Prompt(messages=[message])\n\n        self.llm = llm\n\n    @staticmethod\n    def validate_init_parameters(\n        inputs: list[tuple[str, type[list]]],\n        outputs: list[str],\n        examples: list[dict[str, Any]],\n    ):\n        \"\"\"\n        Validate the init parameters.\n\n        Args:\n            inputs (list[tuple[str, type[list]]]): The inputs to validate.\n            outputs (list[str]): The outputs to validate.\n            examples (list[dict[str, Any]]): The examples to validate.\n\n        Raises:\n            ValueError: If the inputs are not a list of tuples with a string and a type of list.\n                        If the outputs are not a list of strings.\n                        If the examples are not a list of dictionaries.\n                        If any example does not have keys \"inputs\" and \"outputs\" with values that\n                        are dictionaries with string keys.\n        \"\"\"\n        if (\n            not isinstance(inputs, list)\n            or not all(isinstance(_input, tuple) for _input in inputs)\n            or not all(\n                isinstance(_input[0], str)\n                and _input[1] is not list\n                and len(_input) == 2\n                for _input in inputs\n            )\n        ):\n            msg = (\n                f\"LLM evaluator expects inputs to be a list of tuples. Each tuple must contain \"\n                f\"an input name and type of list but received {inputs}.\"\n            )\n            raise ValueError(msg)\n\n        if not isinstance(outputs, list) or not all(\n            isinstance(output, str) for output in outputs\n        ):\n            msg = f\"LLM evaluator expects outputs to be a list of str but received {outputs}.\"\n            raise ValueError(msg)\n\n        if not isinstance(examples, list) or not all(\n            isinstance(example, dict) for example in examples\n        ):\n            msg = f\"LLM evaluator expects examples to be a list of dictionaries but received {examples}.\"\n            raise ValueError(msg)\n\n        for example in examples:\n            if (\n                {\"inputs\", \"outputs\"} != example.keys()\n                or not all(\n                    isinstance(example[param], dict) for param in [\"inputs\", \"outputs\"]\n                )\n                or not all(\n                    isinstance(key, str)\n                    for param in [\"inputs\", \"outputs\"]\n                    for key in example[param]\n                )\n            ):\n                msg = (\n                    f\"LLM evaluator expects each example to have keys `inputs` and `outputs` with \"\n                    f\"values that are dictionaries with str keys but received {example}.\"\n                )\n                raise ValueError(msg)\n\n    def run(self, **inputs) -&gt; dict[str, Any]:\n        \"\"\"\n        Run the LLM evaluator.\n\n        Args:\n            inputs: The input values to evaluate. The keys are the input names and the values are\n                lists of input values.\n\n        Returns:\n            dict[str, Any]: A dictionary with a `results` entry that contains a list of results.\n                Each result is a dictionary containing the keys as defined in the `outputs` parameter\n                of the LLMEvaluator and the evaluation results as the values. If an exception occurs\n                for a particular input value, the result will be `None` for that entry. If the API is\n                \"openai\" and the response contains a \"meta\" key, the metadata from OpenAI will be\n                included in the output dictionary, under the key \"meta\".\n\n        Raises:\n            ValueError: Only in the case that `raise_on_failure` is set to True and the received\n                inputs are not lists or have different lengths, or if the output is not a valid JSON\n                or doesn't contain the expected keys.\n        \"\"\"\n        self.validate_input_parameters(dict(self.inputs), inputs)\n\n        input_names, values = inputs.keys(), list(zip(*inputs.values()))\n        list_of_input_names_to_values = [dict(zip(input_names, v)) for v in values]\n\n        results: list[dict[str, Any] | None] = []\n        errors = 0\n        for input_names_to_values in list_of_input_names_to_values:\n            prompt = self.prompt\n            try:\n                result = self.llm.execute(\n                    input_data=input_names_to_values, prompt=prompt\n                )\n            except Exception as e:\n                msg = (\n                    f\"Error while generating response for prompt: {prompt}. Error: {e}\"\n                )\n                if self.raise_on_failure:\n                    raise ValueError(msg)\n                warn(msg)\n                results.append(None)\n                errors += 1\n                continue\n\n            if self.is_valid_json_and_has_expected_keys(\n                expected=self.outputs, received=result[\"content\"]\n            ):\n                parsed_result = json.loads(result[\"content\"])\n                results.append(parsed_result)\n            else:\n                results.append(None)\n                errors += 1\n\n        if errors &gt; 0:\n            msg = f\"LLM evaluator failed for {errors} out of {len(list_of_input_names_to_values)} inputs.\"\n            warn(msg)\n\n        return {\"results\": results}\n\n    def prepare_template(self) -&gt; str:\n        \"\"\"\n        Prepare the prompt template.\n\n        Combine instructions, inputs, outputs, and examples into one prompt template with the\n        following format:\n        Instructions:\n        &lt;instructions&gt;\n\n        Generate the response in JSON format with the following keys:\n        &lt;list of output keys&gt;\n        Consider the instructions and the examples below to determine those values.\n\n        Examples:\n        &lt;examples&gt;\n\n        Inputs:\n        &lt;inputs&gt;\n        Outputs:\n\n        Returns:\n            str: The prompt template.\n        \"\"\"\n        inputs_section = (\n            \"{\"\n            + \", \".join(\n                [\n                    f'\"{input_socket[0]}\": {{{{ {input_socket[0]} }}}}'\n                    for input_socket in self.inputs\n                ]\n            )\n            + \"}\"\n        )\n\n        examples_section = \"\\n\".join(\n            [\n                \"Inputs:\\n\"\n                + json.dumps(example[\"inputs\"])\n                + \"\\nOutputs:\\n\"\n                + json.dumps(example[\"outputs\"])\n                for example in self.examples\n            ]\n        )\n        return (\n            f\"Instructions:\\n\"\n            f\"{self.instructions}\\n\\n\"\n            f\"Generate the response in JSON format omit extra keys and markdown syntax elements, \"\n            f\"and include the following keys:\\n\"\n            f\"{json.dumps(self.outputs)}\\n\"\n            f\"Consider the instructions and the examples below to determine those values.\\n\\n\"\n            f\"Examples:\\n\"\n            f\"{examples_section}\\n\\n\"\n            f\"Inputs:\\n\"\n            f\"{inputs_section}\\n\"\n            f\"Outputs:\\n\"\n        )\n\n    @staticmethod\n    def validate_input_parameters(\n        expected: dict[str, Any], received: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Validate the input parameters.\n\n        Args:\n            expected (dict[str, Any]): The expected input parameters.\n            received (dict[str, Any]): The received input parameters.\n\n        Raises:\n            ValueError: If not all expected inputs are present in the received inputs.\n                        If the received inputs are not lists or have different lengths.\n        \"\"\"\n        for param in expected.keys():\n            if param not in received:\n                msg = f\"LLM evaluator expected input parameter '{param}' but received only {received.keys()}.\"\n                raise ValueError(msg)\n\n        if not all(isinstance(_input, list) for _input in received.values()):\n            msg = (\n                \"LLM evaluator expects all input values to be lists but received \"\n                f\"{[type(_input) for _input in received.values()]}.\"\n            )\n            raise ValueError(msg)\n\n        inputs = received.values()\n        length = len(next(iter(inputs)))\n        if not all(len(_input) == length for _input in inputs):\n            msg = (\n                f\"LLM evaluator expects all input lists to have the same length but received {inputs} with lengths \"\n                f\"{[len(_input) for _input in inputs]}.\"\n            )\n            raise ValueError(msg)\n\n    def is_valid_json_and_has_expected_keys(\n        self, expected: list[str], received: str\n    ) -&gt; bool:\n        \"\"\"\n        Output must be a valid JSON with the expected keys.\n\n        Args:\n            expected (list[str]): Names of expected outputs.\n            received (str): Names of received outputs.\n\n        Raises:\n            ValueError: If the output is not a valid JSON with the expected keys:\n                        - with `raise_on_failure` set to True a ValueError is raised.\n                        - with `raise_on_failure` set to False a warning is issued and False is returned.\n\n        Returns:\n            bool: True if the received output is a valid JSON with the expected keys, False otherwise.\n        \"\"\"\n        try:\n            parsed_output = json.loads(received)\n        except json.JSONDecodeError:\n            msg = f\"Response from LLM evaluator is not a valid JSON: {received}.\"\n            if self.raise_on_failure:\n                raise ValueError(msg)\n            warn(msg)\n            return False\n\n        if not all(output in parsed_output for output in expected):\n            msg = f\"Expected response from LLM evaluator to be JSON with keys {expected}, got {received}.\"\n            if self.raise_on_failure:\n                raise ValueError(msg)\n            warn(msg)\n            return False\n\n        return True\n</code></pre>"},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator.__init__","title":"<code>__init__(instructions, inputs, outputs, examples, *, raise_on_failure=True, llm)</code>","text":"<p>Creates an instance of LLMEvaluator.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The prompt instructions to use for evaluation.</p> required <code>inputs</code> <code>list[tuple[str, type[list]]]</code> <p>The inputs that the component expects as incoming connections and that it evaluates. Each input is a tuple of an input name and input type. Input types must be lists.</p> required <code>outputs</code> <code>list[str]</code> <p>Output names of the evaluation results. They correspond to keys in the output dictionary.</p> required <code>examples</code> <code>list[dict[str, Any]]</code> <p>Few-shot examples conforming to the expected input and output format as defined in the <code>inputs</code> and <code>outputs</code> parameters. Each example is a dictionary with keys \"inputs\" and \"outputs\". They contain the input and output as dictionaries respectively.</p> required <code>raise_on_failure</code> <code>bool</code> <p>If True, the component will raise an exception on an unsuccessful API call.</p> <code>True</code> <code>llm</code> <code>Node</code> <p>The LLM node to use for evaluation.</p> required Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>def __init__(\n    self,\n    instructions: str,\n    inputs: list[tuple[str, type[list]]],\n    outputs: list[str],\n    examples: list[dict[str, Any]],\n    *,\n    raise_on_failure: bool = True,\n    llm: Node,\n):\n    \"\"\"\n    Creates an instance of LLMEvaluator.\n\n    Args:\n        instructions (str): The prompt instructions to use for evaluation.\n        inputs (list[tuple[str, type[list]]]): The inputs that the component expects as incoming\n            connections and that it evaluates. Each input is a tuple of an input name and input\n            type. Input types must be lists.\n        outputs (list[str]): Output names of the evaluation results. They correspond to keys in\n            the output dictionary.\n        examples (list[dict[str, Any]]): Few-shot examples conforming to the expected input and\n            output format as defined in the `inputs` and `outputs` parameters. Each example is a\n            dictionary with keys \"inputs\" and \"outputs\". They contain the input and output as\n            dictionaries respectively.\n        raise_on_failure (bool): If True, the component will raise an exception on an\n            unsuccessful API call.\n        llm (Node): The LLM node to use for evaluation.\n    \"\"\"\n    self.validate_init_parameters(inputs, outputs, examples)\n    self.raise_on_failure = raise_on_failure\n    self.instructions = instructions\n    self.inputs = inputs\n    self.outputs = outputs\n    self.examples = examples\n    self.api_params = {}\n\n    default_generation_kwargs = {\n        \"response_format\": {\"type\": \"json_object\"},\n        \"seed\": 42,\n    }\n    user_generation_kwargs = self.api_params.get(\"generation_kwargs\", {})\n    merged_generation_kwargs = {\n        **default_generation_kwargs,\n        **user_generation_kwargs,\n    }\n    self.api_params[\"generation_kwargs\"] = merged_generation_kwargs\n\n    template = self.prepare_template()\n    message = Message(role=\"user\", content=template)\n    self.prompt = Prompt(messages=[message])\n    self.builder = Prompt(messages=[message])\n\n    self.llm = llm\n</code></pre>"},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator.is_valid_json_and_has_expected_keys","title":"<code>is_valid_json_and_has_expected_keys(expected, received)</code>","text":"<p>Output must be a valid JSON with the expected keys.</p> <p>Parameters:</p> Name Type Description Default <code>expected</code> <code>list[str]</code> <p>Names of expected outputs.</p> required <code>received</code> <code>str</code> <p>Names of received outputs.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the output is not a valid JSON with the expected keys:         - with <code>raise_on_failure</code> set to True a ValueError is raised.         - with <code>raise_on_failure</code> set to False a warning is issued and False is returned.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the received output is a valid JSON with the expected keys, False otherwise.</p> Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>def is_valid_json_and_has_expected_keys(\n    self, expected: list[str], received: str\n) -&gt; bool:\n    \"\"\"\n    Output must be a valid JSON with the expected keys.\n\n    Args:\n        expected (list[str]): Names of expected outputs.\n        received (str): Names of received outputs.\n\n    Raises:\n        ValueError: If the output is not a valid JSON with the expected keys:\n                    - with `raise_on_failure` set to True a ValueError is raised.\n                    - with `raise_on_failure` set to False a warning is issued and False is returned.\n\n    Returns:\n        bool: True if the received output is a valid JSON with the expected keys, False otherwise.\n    \"\"\"\n    try:\n        parsed_output = json.loads(received)\n    except json.JSONDecodeError:\n        msg = f\"Response from LLM evaluator is not a valid JSON: {received}.\"\n        if self.raise_on_failure:\n            raise ValueError(msg)\n        warn(msg)\n        return False\n\n    if not all(output in parsed_output for output in expected):\n        msg = f\"Expected response from LLM evaluator to be JSON with keys {expected}, got {received}.\"\n        if self.raise_on_failure:\n            raise ValueError(msg)\n        warn(msg)\n        return False\n\n    return True\n</code></pre>"},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator.prepare_template","title":"<code>prepare_template()</code>","text":"<p>Prepare the prompt template.</p> <p>Combine instructions, inputs, outputs, and examples into one prompt template with the following format: Instructions:  <p>Generate the response in JSON format with the following keys:  Consider the instructions and the examples below to determine those values. <p>Examples:  <p>Inputs:  Outputs: <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The prompt template.</p> Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>def prepare_template(self) -&gt; str:\n    \"\"\"\n    Prepare the prompt template.\n\n    Combine instructions, inputs, outputs, and examples into one prompt template with the\n    following format:\n    Instructions:\n    &lt;instructions&gt;\n\n    Generate the response in JSON format with the following keys:\n    &lt;list of output keys&gt;\n    Consider the instructions and the examples below to determine those values.\n\n    Examples:\n    &lt;examples&gt;\n\n    Inputs:\n    &lt;inputs&gt;\n    Outputs:\n\n    Returns:\n        str: The prompt template.\n    \"\"\"\n    inputs_section = (\n        \"{\"\n        + \", \".join(\n            [\n                f'\"{input_socket[0]}\": {{{{ {input_socket[0]} }}}}'\n                for input_socket in self.inputs\n            ]\n        )\n        + \"}\"\n    )\n\n    examples_section = \"\\n\".join(\n        [\n            \"Inputs:\\n\"\n            + json.dumps(example[\"inputs\"])\n            + \"\\nOutputs:\\n\"\n            + json.dumps(example[\"outputs\"])\n            for example in self.examples\n        ]\n    )\n    return (\n        f\"Instructions:\\n\"\n        f\"{self.instructions}\\n\\n\"\n        f\"Generate the response in JSON format omit extra keys and markdown syntax elements, \"\n        f\"and include the following keys:\\n\"\n        f\"{json.dumps(self.outputs)}\\n\"\n        f\"Consider the instructions and the examples below to determine those values.\\n\\n\"\n        f\"Examples:\\n\"\n        f\"{examples_section}\\n\\n\"\n        f\"Inputs:\\n\"\n        f\"{inputs_section}\\n\"\n        f\"Outputs:\\n\"\n    )\n</code></pre>"},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator.run","title":"<code>run(**inputs)</code>","text":"<p>Run the LLM evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>The input values to evaluate. The keys are the input names and the values are lists of input values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary with a <code>results</code> entry that contains a list of results. Each result is a dictionary containing the keys as defined in the <code>outputs</code> parameter of the LLMEvaluator and the evaluation results as the values. If an exception occurs for a particular input value, the result will be <code>None</code> for that entry. If the API is \"openai\" and the response contains a \"meta\" key, the metadata from OpenAI will be included in the output dictionary, under the key \"meta\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Only in the case that <code>raise_on_failure</code> is set to True and the received inputs are not lists or have different lengths, or if the output is not a valid JSON or doesn't contain the expected keys.</p> Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>def run(self, **inputs) -&gt; dict[str, Any]:\n    \"\"\"\n    Run the LLM evaluator.\n\n    Args:\n        inputs: The input values to evaluate. The keys are the input names and the values are\n            lists of input values.\n\n    Returns:\n        dict[str, Any]: A dictionary with a `results` entry that contains a list of results.\n            Each result is a dictionary containing the keys as defined in the `outputs` parameter\n            of the LLMEvaluator and the evaluation results as the values. If an exception occurs\n            for a particular input value, the result will be `None` for that entry. If the API is\n            \"openai\" and the response contains a \"meta\" key, the metadata from OpenAI will be\n            included in the output dictionary, under the key \"meta\".\n\n    Raises:\n        ValueError: Only in the case that `raise_on_failure` is set to True and the received\n            inputs are not lists or have different lengths, or if the output is not a valid JSON\n            or doesn't contain the expected keys.\n    \"\"\"\n    self.validate_input_parameters(dict(self.inputs), inputs)\n\n    input_names, values = inputs.keys(), list(zip(*inputs.values()))\n    list_of_input_names_to_values = [dict(zip(input_names, v)) for v in values]\n\n    results: list[dict[str, Any] | None] = []\n    errors = 0\n    for input_names_to_values in list_of_input_names_to_values:\n        prompt = self.prompt\n        try:\n            result = self.llm.execute(\n                input_data=input_names_to_values, prompt=prompt\n            )\n        except Exception as e:\n            msg = (\n                f\"Error while generating response for prompt: {prompt}. Error: {e}\"\n            )\n            if self.raise_on_failure:\n                raise ValueError(msg)\n            warn(msg)\n            results.append(None)\n            errors += 1\n            continue\n\n        if self.is_valid_json_and_has_expected_keys(\n            expected=self.outputs, received=result[\"content\"]\n        ):\n            parsed_result = json.loads(result[\"content\"])\n            results.append(parsed_result)\n        else:\n            results.append(None)\n            errors += 1\n\n    if errors &gt; 0:\n        msg = f\"LLM evaluator failed for {errors} out of {len(list_of_input_names_to_values)} inputs.\"\n        warn(msg)\n\n    return {\"results\": results}\n</code></pre>"},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator.validate_init_parameters","title":"<code>validate_init_parameters(inputs, outputs, examples)</code>  <code>staticmethod</code>","text":"<p>Validate the init parameters.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list[tuple[str, type[list]]]</code> <p>The inputs to validate.</p> required <code>outputs</code> <code>list[str]</code> <p>The outputs to validate.</p> required <code>examples</code> <code>list[dict[str, Any]]</code> <p>The examples to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the inputs are not a list of tuples with a string and a type of list.         If the outputs are not a list of strings.         If the examples are not a list of dictionaries.         If any example does not have keys \"inputs\" and \"outputs\" with values that         are dictionaries with string keys.</p> Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>@staticmethod\ndef validate_init_parameters(\n    inputs: list[tuple[str, type[list]]],\n    outputs: list[str],\n    examples: list[dict[str, Any]],\n):\n    \"\"\"\n    Validate the init parameters.\n\n    Args:\n        inputs (list[tuple[str, type[list]]]): The inputs to validate.\n        outputs (list[str]): The outputs to validate.\n        examples (list[dict[str, Any]]): The examples to validate.\n\n    Raises:\n        ValueError: If the inputs are not a list of tuples with a string and a type of list.\n                    If the outputs are not a list of strings.\n                    If the examples are not a list of dictionaries.\n                    If any example does not have keys \"inputs\" and \"outputs\" with values that\n                    are dictionaries with string keys.\n    \"\"\"\n    if (\n        not isinstance(inputs, list)\n        or not all(isinstance(_input, tuple) for _input in inputs)\n        or not all(\n            isinstance(_input[0], str)\n            and _input[1] is not list\n            and len(_input) == 2\n            for _input in inputs\n        )\n    ):\n        msg = (\n            f\"LLM evaluator expects inputs to be a list of tuples. Each tuple must contain \"\n            f\"an input name and type of list but received {inputs}.\"\n        )\n        raise ValueError(msg)\n\n    if not isinstance(outputs, list) or not all(\n        isinstance(output, str) for output in outputs\n    ):\n        msg = f\"LLM evaluator expects outputs to be a list of str but received {outputs}.\"\n        raise ValueError(msg)\n\n    if not isinstance(examples, list) or not all(\n        isinstance(example, dict) for example in examples\n    ):\n        msg = f\"LLM evaluator expects examples to be a list of dictionaries but received {examples}.\"\n        raise ValueError(msg)\n\n    for example in examples:\n        if (\n            {\"inputs\", \"outputs\"} != example.keys()\n            or not all(\n                isinstance(example[param], dict) for param in [\"inputs\", \"outputs\"]\n            )\n            or not all(\n                isinstance(key, str)\n                for param in [\"inputs\", \"outputs\"]\n                for key in example[param]\n            )\n        ):\n            msg = (\n                f\"LLM evaluator expects each example to have keys `inputs` and `outputs` with \"\n                f\"values that are dictionaries with str keys but received {example}.\"\n            )\n            raise ValueError(msg)\n</code></pre>"},{"location":"dynamiq/components/evaluators/llm_evaluator/#dynamiq.components.evaluators.llm_evaluator.LLMEvaluator.validate_input_parameters","title":"<code>validate_input_parameters(expected, received)</code>  <code>staticmethod</code>","text":"<p>Validate the input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>expected</code> <code>dict[str, Any]</code> <p>The expected input parameters.</p> required <code>received</code> <code>dict[str, Any]</code> <p>The received input parameters.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not all expected inputs are present in the received inputs.         If the received inputs are not lists or have different lengths.</p> Source code in <code>dynamiq/components/evaluators/llm_evaluator.py</code> <pre><code>@staticmethod\ndef validate_input_parameters(\n    expected: dict[str, Any], received: dict[str, Any]\n) -&gt; None:\n    \"\"\"\n    Validate the input parameters.\n\n    Args:\n        expected (dict[str, Any]): The expected input parameters.\n        received (dict[str, Any]): The received input parameters.\n\n    Raises:\n        ValueError: If not all expected inputs are present in the received inputs.\n                    If the received inputs are not lists or have different lengths.\n    \"\"\"\n    for param in expected.keys():\n        if param not in received:\n            msg = f\"LLM evaluator expected input parameter '{param}' but received only {received.keys()}.\"\n            raise ValueError(msg)\n\n    if not all(isinstance(_input, list) for _input in received.values()):\n        msg = (\n            \"LLM evaluator expects all input values to be lists but received \"\n            f\"{[type(_input) for _input in received.values()]}.\"\n        )\n        raise ValueError(msg)\n\n    inputs = received.values()\n    length = len(next(iter(inputs)))\n    if not all(len(_input) == length for _input in inputs):\n        msg = (\n            f\"LLM evaluator expects all input lists to have the same length but received {inputs} with lengths \"\n            f\"{[len(_input) for _input in inputs]}.\"\n        )\n        raise ValueError(msg)\n</code></pre>"},{"location":"dynamiq/components/retrievers/chroma/","title":"Chroma","text":""},{"location":"dynamiq/components/retrievers/chroma/#dynamiq.components.retrievers.chroma.ChromaDocumentRetriever","title":"<code>ChromaDocumentRetriever</code>","text":"<p>Document Retriever using Chroma.</p> Source code in <code>dynamiq/components/retrievers/chroma.py</code> <pre><code>class ChromaDocumentRetriever:\n    \"\"\"\n    Document Retriever using Chroma.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: ChromaVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Chroma vector store with optional filtering.\n\n        Args:\n            vector_store (ChromaVectorStore): An instance of ChromaVectorStore to interface with Chroma vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `ChromaVectorStore`.\n\n        This initializer checks if the `vector_store` provided is an instance of the expected `ChromaVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, ChromaVectorStore):\n            msg = \"document_store must be an instance of ChromaVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n    ):\n        \"\"\"\n        Retrieves documents from the ChromaVectorStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        query_embeddings = [query_embedding]\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store.search_embeddings(\n            query_embeddings=query_embeddings,\n            filters=filters,\n            top_k=top_k,\n        )[0]\n\n        if exclude_document_embeddings:\n            for doc in docs:\n                doc.embedding = None\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/chroma/#dynamiq.components.retrievers.chroma.ChromaDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10)</code>","text":"<p>Initializes a component for retrieving documents from a Chroma vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>ChromaVectorStore</code> <p>An instance of ChromaVectorStore to interface with Chroma vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>ChromaVectorStore</code>.</p> <p>This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>ChromaVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/chroma.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: ChromaVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Chroma vector store with optional filtering.\n\n    Args:\n        vector_store (ChromaVectorStore): An instance of ChromaVectorStore to interface with Chroma vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `ChromaVectorStore`.\n\n    This initializer checks if the `vector_store` provided is an instance of the expected `ChromaVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, ChromaVectorStore):\n        msg = \"document_store must be an instance of ChromaVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n</code></pre>"},{"location":"dynamiq/components/retrievers/chroma/#dynamiq.components.retrievers.chroma.ChromaDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None)</code>","text":"<p>Retrieves documents from the ChromaVectorStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/chroma.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n):\n    \"\"\"\n    Retrieves documents from the ChromaVectorStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    query_embeddings = [query_embedding]\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store.search_embeddings(\n        query_embeddings=query_embeddings,\n        filters=filters,\n        top_k=top_k,\n    )[0]\n\n    if exclude_document_embeddings:\n        for doc in docs:\n            doc.embedding = None\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/milvus/","title":"Milvus","text":""},{"location":"dynamiq/components/retrievers/milvus/#dynamiq.components.retrievers.milvus.MilvusDocumentRetriever","title":"<code>MilvusDocumentRetriever</code>","text":"<p>Document Retriever using Milvus.</p> Source code in <code>dynamiq/components/retrievers/milvus.py</code> <pre><code>class MilvusDocumentRetriever:\n    \"\"\"\n    Document Retriever using Milvus.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: MilvusVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Milvus vector store with optional filtering.\n\n        Args:\n            vector_store (MilvusVectorStore): An instance of MilvusVectorStore to interface with Milvus vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `MilvusVectorStore`.\n        \"\"\"\n        if not isinstance(vector_store, MilvusVectorStore):\n            raise ValueError(\"vector_store must be an instance of MilvusVectorStore\")\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n    ) -&gt; dict[str, list[Document]]:\n        \"\"\"\n        Retrieves documents from the MilvusVectorStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n        Returns:\n            Dict[str, List[Document]]: A dictionary containing a list of Document instances sorted by their relevance\n            to the query_embedding.\n        \"\"\"\n        query_embeddings = [query_embedding]\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store.search_embeddings(\n            query_embeddings=query_embeddings,\n            filters=filters,\n            top_k=top_k,\n        )\n\n        # Optionally exclude embeddings from the retrieved documents\n        if exclude_document_embeddings:\n            for doc in docs:\n                doc.embedding = None\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/milvus/#dynamiq.components.retrievers.milvus.MilvusDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10)</code>","text":"<p>Initializes a component for retrieving documents from a Milvus vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>MilvusVectorStore</code> <p>An instance of MilvusVectorStore to interface with Milvus vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>MilvusVectorStore</code>.</p> Source code in <code>dynamiq/components/retrievers/milvus.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: MilvusVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Milvus vector store with optional filtering.\n\n    Args:\n        vector_store (MilvusVectorStore): An instance of MilvusVectorStore to interface with Milvus vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `MilvusVectorStore`.\n    \"\"\"\n    if not isinstance(vector_store, MilvusVectorStore):\n        raise ValueError(\"vector_store must be an instance of MilvusVectorStore\")\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n</code></pre>"},{"location":"dynamiq/components/retrievers/milvus/#dynamiq.components.retrievers.milvus.MilvusDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None)</code>","text":"<p>Retrieves documents from the MilvusVectorStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Document]]</code> <p>Dict[str, List[Document]]: A dictionary containing a list of Document instances sorted by their relevance</p> <code>dict[str, list[Document]]</code> <p>to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/milvus.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n) -&gt; dict[str, list[Document]]:\n    \"\"\"\n    Retrieves documents from the MilvusVectorStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n    Returns:\n        Dict[str, List[Document]]: A dictionary containing a list of Document instances sorted by their relevance\n        to the query_embedding.\n    \"\"\"\n    query_embeddings = [query_embedding]\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store.search_embeddings(\n        query_embeddings=query_embeddings,\n        filters=filters,\n        top_k=top_k,\n    )\n\n    # Optionally exclude embeddings from the retrieved documents\n    if exclude_document_embeddings:\n        for doc in docs:\n            doc.embedding = None\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/components/retrievers/pinecone/#dynamiq.components.retrievers.pinecone.PineconeDocumentRetriever","title":"<code>PineconeDocumentRetriever</code>","text":"<p>Document Retriever using Pinecone.</p> Source code in <code>dynamiq/components/retrievers/pinecone.py</code> <pre><code>class PineconeDocumentRetriever:\n    \"\"\"\n    Document Retriever using Pinecone.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: PineconeVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Pinecone vector store with optional filtering.\n\n        Args:\n            vector_store (PineconeVectorStore): An instance of PineconeVectorStore to interface with Pinecone vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `PineconeVectorStore`.\n\n        This initializer checks if the `vector_store` provided is an instance of the expected `PineconeVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, PineconeVectorStore):\n            msg = \"document_store must be an instance of PineconeVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n    ):\n        \"\"\"\n        Retrieves documents from the PineconeDocumentStore that are similar to the provided query embedding.\n\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n        )\n        logger.debug(f\"Retrieved {len(docs)} documents from Pinecone Vector Store.\")\n\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/pinecone/#dynamiq.components.retrievers.pinecone.PineconeDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10)</code>","text":"<p>Initializes a component for retrieving documents from a Pinecone vector store with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>PineconeVectorStore</code> <p>An instance of PineconeVectorStore to interface with Pinecone vectors.</p> required <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>vector_store</code> is not an instance of <code>PineconeVectorStore</code>.</p> <p>This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>PineconeVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/pinecone.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: PineconeVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Pinecone vector store with optional filtering.\n\n    Args:\n        vector_store (PineconeVectorStore): An instance of PineconeVectorStore to interface with Pinecone vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `PineconeVectorStore`.\n\n    This initializer checks if the `vector_store` provided is an instance of the expected `PineconeVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, PineconeVectorStore):\n        msg = \"document_store must be an instance of PineconeVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n</code></pre>"},{"location":"dynamiq/components/retrievers/pinecone/#dynamiq.components.retrievers.pinecone.PineconeDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None)</code>","text":"<p>Retrieves documents from the PineconeDocumentStore that are similar to the provided query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>List[float]</code> <p>The embedding vector of the query for which similar documents are to be</p> required <code>exclude_document_embeddings</code> <code>bool</code> <p>Specifies whether to exclude the embeddings of the retrieved</p> <code>True</code> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[dict[str, Any]]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/pinecone.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n):\n    \"\"\"\n    Retrieves documents from the PineconeDocumentStore that are similar to the provided query embedding.\n\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store._embedding_retrieval(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        exclude_document_embeddings=exclude_document_embeddings,\n    )\n    logger.debug(f\"Retrieved {len(docs)} documents from Pinecone Vector Store.\")\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/components/retrievers/qdrant/#dynamiq.components.retrievers.qdrant.QdrantDocumentRetriever","title":"<code>QdrantDocumentRetriever</code>","text":"<p>Document Retriever using Qdrant</p> Source code in <code>dynamiq/components/retrievers/qdrant.py</code> <pre><code>class QdrantDocumentRetriever:\n    \"\"\"\n    Document Retriever using Qdrant\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: QdrantVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Qdrant vector store with optional filtering.\n        Args:\n            vector_store (QdrantVectorStore): An instance of QdrantVectorStore to interface with Qdrant vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `QdrantVectorStore`.\n        This initializer checks if the `vector_store` provided is an instance of the expected `QdrantVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, QdrantVectorStore):\n            msg = \"document_store must be an instance of QdrantVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n    ):\n        \"\"\"\n        Retrieves documents from the QdrantDocumentStore that are similar to the provided query embedding.\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (Optional[int], optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]], optional): Filters to apply\n                for retrieving specific documents. Defaults to None.\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store._query_by_embedding(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            return_embedding=not exclude_document_embeddings,\n        )\n        logger.debug(f\"Retrieved {len(docs)} documents from Qdrant Vector Store.\")\n\n        return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/qdrant/#dynamiq.components.retrievers.qdrant.QdrantDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10)</code>","text":"<p>Initializes a component for retrieving documents from a Qdrant vector store with optional filtering. Args:     vector_store (QdrantVectorStore): An instance of QdrantVectorStore to interface with Qdrant vectors.     filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.     top_k (int): The maximum number of documents to return. Defaults to 10. Raises:     ValueError: If the <code>vector_store</code> is not an instance of <code>QdrantVectorStore</code>. This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>QdrantVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/qdrant.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: QdrantVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Qdrant vector store with optional filtering.\n    Args:\n        vector_store (QdrantVectorStore): An instance of QdrantVectorStore to interface with Qdrant vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `QdrantVectorStore`.\n    This initializer checks if the `vector_store` provided is an instance of the expected `QdrantVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, QdrantVectorStore):\n        msg = \"document_store must be an instance of QdrantVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n</code></pre>"},{"location":"dynamiq/components/retrievers/qdrant/#dynamiq.components.retrievers.qdrant.QdrantDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None)</code>","text":"<p>Retrieves documents from the QdrantDocumentStore that are similar to the provided query embedding. Args:     query_embedding (List[float]): The embedding vector of the query for which similar documents are to be     retrieved.     exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved     documents from the output.     top_k (Optional[int], optional): The maximum number of documents to return. Defaults to None.     filters (Optional[dict[str, Any]], optional): Filters to apply         for retrieving specific documents. Defaults to None. Returns:     List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/qdrant.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n):\n    \"\"\"\n    Retrieves documents from the QdrantDocumentStore that are similar to the provided query embedding.\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (Optional[int], optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]], optional): Filters to apply\n            for retrieving specific documents. Defaults to None.\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store._query_by_embedding(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        return_embedding=not exclude_document_embeddings,\n    )\n    logger.debug(f\"Retrieved {len(docs)} documents from Qdrant Vector Store.\")\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever","title":"<code>WeaviateDocumentRetriever</code>","text":"<p>Document Retriever using Weaviate</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>class WeaviateDocumentRetriever:\n    \"\"\"\n    Document Retriever using Weaviate\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vector_store: WeaviateVectorStore,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n    ):\n        \"\"\"\n        Initializes a component for retrieving documents from a Weaviate vector store with optional filtering.\n        Args:\n            vector_store (WeaviateVectorStore): An instance of WeaviateVectorStore to interface with Weaviate vectors.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n            top_k (int): The maximum number of documents to return. Defaults to 10.\n        Raises:\n            ValueError: If the `vector_store` is not an instance of `WeaviateVectorStore`.\n        This initializer checks if the `vector_store` provided is an instance of the expected `WeaviateVectorStore`\n        class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n        \"\"\"\n        if not isinstance(vector_store, WeaviateVectorStore):\n            msg = \"document_store must be an instance of WeaviateVectorStore\"\n            raise ValueError(msg)\n\n        self.vector_store = vector_store\n        self.filters = filters or {}\n        self.top_k = top_k\n\n    def run(\n        self,\n        query_embedding: list[float],\n        exclude_document_embeddings: bool = True,\n        top_k: int | None = None,\n        filters: dict[str, Any] | None = None,\n    ):\n        \"\"\"\n        Retrieves documents from the WeaviateDocumentStore that are similar to the provided query embedding.\n        Args:\n            query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n            retrieved.\n            exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n            documents from the output.\n            top_k (int, optional): The maximum number of documents to return. Defaults to None.\n            filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n        Returns:\n            List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n        \"\"\"\n        top_k = top_k or self.top_k\n        filters = filters or self.filters\n\n        docs = self.vector_store._embedding_retrieval(\n            query_embedding=query_embedding,\n            filters=filters,\n            top_k=top_k,\n            exclude_document_embeddings=exclude_document_embeddings,\n        )\n        logger.debug(f\"Retrieved {len(docs)} documents from Weaviate Vector Store.\")\n\n        return {\"documents\": docs}\n\n    def close(self):\n        \"\"\"\n        Closes the WeaviateDocumentRetriever component.\n        \"\"\"\n        self.vector_store.close()\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever.__init__","title":"<code>__init__(*, vector_store, filters=None, top_k=10)</code>","text":"<p>Initializes a component for retrieving documents from a Weaviate vector store with optional filtering. Args:     vector_store (WeaviateVectorStore): An instance of WeaviateVectorStore to interface with Weaviate vectors.     filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.     top_k (int): The maximum number of documents to return. Defaults to 10. Raises:     ValueError: If the <code>vector_store</code> is not an instance of <code>WeaviateVectorStore</code>. This initializer checks if the <code>vector_store</code> provided is an instance of the expected <code>WeaviateVectorStore</code> class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vector_store: WeaviateVectorStore,\n    filters: dict[str, Any] | None = None,\n    top_k: int = 10,\n):\n    \"\"\"\n    Initializes a component for retrieving documents from a Weaviate vector store with optional filtering.\n    Args:\n        vector_store (WeaviateVectorStore): An instance of WeaviateVectorStore to interface with Weaviate vectors.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n        top_k (int): The maximum number of documents to return. Defaults to 10.\n    Raises:\n        ValueError: If the `vector_store` is not an instance of `WeaviateVectorStore`.\n    This initializer checks if the `vector_store` provided is an instance of the expected `WeaviateVectorStore`\n    class, sets up filtering conditions if any, and defines how many top results to retrieve in document queries.\n    \"\"\"\n    if not isinstance(vector_store, WeaviateVectorStore):\n        msg = \"document_store must be an instance of WeaviateVectorStore\"\n        raise ValueError(msg)\n\n    self.vector_store = vector_store\n    self.filters = filters or {}\n    self.top_k = top_k\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever.close","title":"<code>close()</code>","text":"<p>Closes the WeaviateDocumentRetriever component.</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes the WeaviateDocumentRetriever component.\n    \"\"\"\n    self.vector_store.close()\n</code></pre>"},{"location":"dynamiq/components/retrievers/weaviate/#dynamiq.components.retrievers.weaviate.WeaviateDocumentRetriever.run","title":"<code>run(query_embedding, exclude_document_embeddings=True, top_k=None, filters=None)</code>","text":"<p>Retrieves documents from the WeaviateDocumentStore that are similar to the provided query embedding. Args:     query_embedding (List[float]): The embedding vector of the query for which similar documents are to be     retrieved.     exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved     documents from the output.     top_k (int, optional): The maximum number of documents to return. Defaults to None.     filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.</p> <p>Returns:</p> Type Description <p>List[Document]: A list of Document instances sorted by their relevance to the query_embedding.</p> Source code in <code>dynamiq/components/retrievers/weaviate.py</code> <pre><code>def run(\n    self,\n    query_embedding: list[float],\n    exclude_document_embeddings: bool = True,\n    top_k: int | None = None,\n    filters: dict[str, Any] | None = None,\n):\n    \"\"\"\n    Retrieves documents from the WeaviateDocumentStore that are similar to the provided query embedding.\n    Args:\n        query_embedding (List[float]): The embedding vector of the query for which similar documents are to be\n        retrieved.\n        exclude_document_embeddings (bool, optional): Specifies whether to exclude the embeddings of the retrieved\n        documents from the output.\n        top_k (int, optional): The maximum number of documents to return. Defaults to None.\n        filters (Optional[dict[str, Any]]): Filters to apply for retrieving specific documents. Defaults to None.\n\n    Returns:\n        List[Document]: A list of Document instances sorted by their relevance to the query_embedding.\n    \"\"\"\n    top_k = top_k or self.top_k\n    filters = filters or self.filters\n\n    docs = self.vector_store._embedding_retrieval(\n        query_embedding=query_embedding,\n        filters=filters,\n        top_k=top_k,\n        exclude_document_embeddings=exclude_document_embeddings,\n    )\n    logger.debug(f\"Retrieved {len(docs)} documents from Weaviate Vector Store.\")\n\n    return {\"documents\": docs}\n</code></pre>"},{"location":"dynamiq/components/splitters/document/","title":"Document","text":""},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitBy","title":"<code>DocumentSplitBy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum class for document splitting methods.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>class DocumentSplitBy(str, enum.Enum):\n    \"\"\"Enum class for document splitting methods.\"\"\"\n\n    WORD = \"word\"\n    SENTENCE = \"sentence\"\n    PAGE = \"page\"\n    PASSAGE = \"passage\"\n    TITLE = \"title\"\n    CHARACTER = \"character\"\n</code></pre>"},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitter","title":"<code>DocumentSplitter</code>","text":"<p>Splits a list of text documents into a list of text documents with shorter texts.</p> <p>Splitting documents with long texts is a common preprocessing step during indexing. This allows Embedders to create significant semantic representations and avoids exceeding the maximum context length of language models.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>class DocumentSplitter:\n    \"\"\"\n    Splits a list of text documents into a list of text documents with shorter texts.\n\n    Splitting documents with long texts is a common preprocessing step during indexing.\n    This allows Embedders to create significant semantic representations\n    and avoids exceeding the maximum context length of language models.\n    \"\"\"\n\n    def __init__(\n        self,\n        split_by: DocumentSplitBy = DocumentSplitBy.PASSAGE,\n        split_length: int = 10,\n        split_overlap: int = 0,\n    ):\n        \"\"\"\n        Initializes an object for splitting documents into smaller parts based on specified criteria.\n\n        Args:\n            split_by (DocumentSplitBy): Determines the unit by which the document should be split.\n                Defaults to DocumentSplitBy.PASSAGE.\n            split_length (int): Specifies the maximum number of units to include in each split.\n                Defaults to 10.\n            split_overlap (int): Specifies the number of units that should overlap between consecutive\n                splits. Defaults to 0.\n\n        Raises:\n            ValueError: If split_length is less than or equal to 0.\n            ValueError: If split_overlap is less than 0.\n        \"\"\"\n        self.split_by = split_by\n        if split_length &lt;= 0:\n            raise ValueError(\"split_length must be greater than 0.\")\n        self.split_length = split_length\n        if split_overlap &lt; 0:\n            raise ValueError(\"split_overlap must be greater than or equal to 0.\")\n        self.split_overlap = split_overlap\n\n    def run(self, documents: list[Document]) -&gt; dict:\n        \"\"\"\n        Splits the provided documents into smaller parts based on the specified configuration.\n\n        Args:\n            documents (list[Document]): The list of documents to be split.\n\n        Returns:\n            dict: A dictionary containing one key, 'documents', which is a list of the split Documents.\n\n        Raises:\n            TypeError: If the input is not a list of Document instances.\n            ValueError: If the content of any document is None.\n        \"\"\"\n        if not isinstance(documents, list) or (\n            documents and not isinstance(documents[0], Document)\n        ):\n            raise TypeError(\"DocumentSplitter expects a List of Documents as input.\")\n\n        split_docs = []\n        for doc in documents:\n            if doc.content is None:\n                raise ValueError(\n                    f\"DocumentSplitter only works with text documents but document.content for document \"\n                    f\"ID {doc.id} is None.\"\n                )\n            units = self._split_into_units(doc.content, self.split_by)\n            text_splits = self._concatenate_units(\n                units, self.split_length, self.split_overlap\n            )\n            if doc.metadata is None:\n                doc.metadata = {}\n            metadata = deepcopy(doc.metadata)\n            metadata[\"source_id\"] = doc.id\n            split_docs += [\n                Document(content=txt, metadata=metadata) for txt in text_splits\n            ]\n        return {\"documents\": split_docs}\n\n    def _split_into_units(self, text: str, split_by: DocumentSplitBy) -&gt; list[str]:\n        \"\"\"\n        Splits the input text into units based on the specified split_by method.\n\n        Args:\n            text (str): The input text to be split.\n            split_by (DocumentSplitBy): The method to use for splitting the text.\n\n        Returns:\n            list[str]: A list of text units after splitting.\n        \"\"\"\n        split_at = SPLIT_STR_BY_SPLIT_TYPE[split_by]\n        if split_by == DocumentSplitBy.CHARACTER:\n            return [char for char in text]\n        else:\n            units = text.split(split_at)\n        # Add the delimiter back to all units except the last one\n        for i in range(len(units) - 1):\n            if split_at == \"\\n#\":\n                units[i] = \"\\n# \" + units[i]\n            else:\n                units[i] += split_at\n        return units\n\n    def _concatenate_units(\n        self, elements: list[str], split_length: int, split_overlap: int\n    ) -&gt; list[str]:\n        \"\"\"\n        Concatenates the elements into parts of split_length units.\n\n        Args:\n            elements (list[str]): The list of text units to be concatenated.\n            split_length (int): The maximum number of units in each split.\n            split_overlap (int): The number of overlapping units between splits.\n\n        Returns:\n            list[str]: A list of concatenated text splits.\n        \"\"\"\n        text_splits = []\n        segments = windowed(elements, n=split_length, step=split_length - split_overlap)\n        for seg in segments:\n            current_units = [unit for unit in seg if unit is not None]\n            txt = \"\".join(current_units)\n            if len(txt) &gt; 0:\n                text_splits.append(txt)\n        return text_splits\n</code></pre>"},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitter.__init__","title":"<code>__init__(split_by=DocumentSplitBy.PASSAGE, split_length=10, split_overlap=0)</code>","text":"<p>Initializes an object for splitting documents into smaller parts based on specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>split_by</code> <code>DocumentSplitBy</code> <p>Determines the unit by which the document should be split. Defaults to DocumentSplitBy.PASSAGE.</p> <code>PASSAGE</code> <code>split_length</code> <code>int</code> <p>Specifies the maximum number of units to include in each split. Defaults to 10.</p> <code>10</code> <code>split_overlap</code> <code>int</code> <p>Specifies the number of units that should overlap between consecutive splits. Defaults to 0.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If split_length is less than or equal to 0.</p> <code>ValueError</code> <p>If split_overlap is less than 0.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>def __init__(\n    self,\n    split_by: DocumentSplitBy = DocumentSplitBy.PASSAGE,\n    split_length: int = 10,\n    split_overlap: int = 0,\n):\n    \"\"\"\n    Initializes an object for splitting documents into smaller parts based on specified criteria.\n\n    Args:\n        split_by (DocumentSplitBy): Determines the unit by which the document should be split.\n            Defaults to DocumentSplitBy.PASSAGE.\n        split_length (int): Specifies the maximum number of units to include in each split.\n            Defaults to 10.\n        split_overlap (int): Specifies the number of units that should overlap between consecutive\n            splits. Defaults to 0.\n\n    Raises:\n        ValueError: If split_length is less than or equal to 0.\n        ValueError: If split_overlap is less than 0.\n    \"\"\"\n    self.split_by = split_by\n    if split_length &lt;= 0:\n        raise ValueError(\"split_length must be greater than 0.\")\n    self.split_length = split_length\n    if split_overlap &lt; 0:\n        raise ValueError(\"split_overlap must be greater than or equal to 0.\")\n    self.split_overlap = split_overlap\n</code></pre>"},{"location":"dynamiq/components/splitters/document/#dynamiq.components.splitters.document.DocumentSplitter.run","title":"<code>run(documents)</code>","text":"<p>Splits the provided documents into smaller parts based on the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>The list of documents to be split.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing one key, 'documents', which is a list of the split Documents.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a list of Document instances.</p> <code>ValueError</code> <p>If the content of any document is None.</p> Source code in <code>dynamiq/components/splitters/document.py</code> <pre><code>def run(self, documents: list[Document]) -&gt; dict:\n    \"\"\"\n    Splits the provided documents into smaller parts based on the specified configuration.\n\n    Args:\n        documents (list[Document]): The list of documents to be split.\n\n    Returns:\n        dict: A dictionary containing one key, 'documents', which is a list of the split Documents.\n\n    Raises:\n        TypeError: If the input is not a list of Document instances.\n        ValueError: If the content of any document is None.\n    \"\"\"\n    if not isinstance(documents, list) or (\n        documents and not isinstance(documents[0], Document)\n    ):\n        raise TypeError(\"DocumentSplitter expects a List of Documents as input.\")\n\n    split_docs = []\n    for doc in documents:\n        if doc.content is None:\n            raise ValueError(\n                f\"DocumentSplitter only works with text documents but document.content for document \"\n                f\"ID {doc.id} is None.\"\n            )\n        units = self._split_into_units(doc.content, self.split_by)\n        text_splits = self._concatenate_units(\n            units, self.split_length, self.split_overlap\n        )\n        if doc.metadata is None:\n            doc.metadata = {}\n        metadata = deepcopy(doc.metadata)\n        metadata[\"source_id\"] = doc.id\n        split_docs += [\n            Document(content=txt, metadata=metadata) for txt in text_splits\n        ]\n    return {\"documents\": split_docs}\n</code></pre>"},{"location":"dynamiq/connections/connections/","title":"Connections","text":""},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AzureAI","title":"<code>AzureAI</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class AzureAI(BaseApiKeyConnection):\n    type: Literal[ConnectionType.AzureAI] = ConnectionType.AzureAI\n    api_key: str = Field(default_factory=partial(get_env_var, \"AZURE_API_KEY\"))\n    url: str = Field(default_factory=partial(get_env_var, \"AZURE_URL\"))\n    api_version: str = Field(default_factory=partial(get_env_var, \"AZURE_API_VERSION\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing\n\n                -the API key with the key 'api_key'.\n\n                -the base url with the key 'api_base'.\n\n                -the API version with the key 'api_version'.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n            \"api_version\": self.api_version,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.AzureAI.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing</p> <p>-the API key with the key 'api_key'.</p> <p>-the base url with the key 'api_base'.</p> <p>-the API version with the key 'api_version'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseApiKeyConnection","title":"<code>BaseApiKeyConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a base connection class that uses an API key for authentication.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key used for authentication.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class BaseApiKeyConnection(BaseConnection):\n    \"\"\"\n    Represents a base connection class that uses an API key for authentication.\n\n    Attributes:\n        api_key (str): The API key used for authentication.\n    \"\"\"\n    api_key: str\n\n    @abstractmethod\n    def connect(self):\n        \"\"\"\n        Connects to the service.\n\n        This method should be implemented by subclasses to establish a connection to the service using\n        the provided API key.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing the API key with the key 'api_key'.\n        \"\"\"\n        return {\"api_key\": self.api_key}\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseApiKeyConnection.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the API key with the key 'api_key'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseApiKeyConnection.connect","title":"<code>connect()</code>  <code>abstractmethod</code>","text":"<p>Connects to the service.</p> <p>This method should be implemented by subclasses to establish a connection to the service using the provided API key.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by a subclass.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@abstractmethod\ndef connect(self):\n    \"\"\"\n    Connects to the service.\n\n    This method should be implemented by subclasses to establish a connection to the service using\n    the provided API key.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection","title":"<code>BaseConnection</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a base connection class.</p> <p>This class should be subclassed to provide specific implementations for different types of connections.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for the connection, generated using <code>generate_uuid</code>.</p> <code>type</code> <code>ConnectionType</code> <p>The type of connection.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class BaseConnection(BaseModel, ABC):\n    \"\"\"Represents a base connection class.\n\n    This class should be subclassed to provide specific implementations for different types of\n    connections.\n\n    Attributes:\n        id (str): A unique identifier for the connection, generated using `generate_uuid`.\n        type (ConnectionType): The type of connection.\n    \"\"\"\n    id: str = Field(default_factory=generate_uuid)\n    type: ConnectionType\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: An empty dictionary.\n        \"\"\"\n        return {}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the connection instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the connection instance.\n        \"\"\"\n        return self.model_dump(**kwargs)\n\n    @abstractmethod\n    def connect(self):\n        \"\"\"Connects to the service.\n\n        This method should be implemented by subclasses to establish a connection to the service.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>An empty dictionary.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection.connect","title":"<code>connect()</code>  <code>abstractmethod</code>","text":"<p>Connects to the service.</p> <p>This method should be implemented by subclasses to establish a connection to the service.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by a subclass.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>@abstractmethod\ndef connect(self):\n    \"\"\"Connects to the service.\n\n    This method should be implemented by subclasses to establish a connection to the service.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.BaseConnection.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the connection instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the connection instance.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the connection instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the connection instance.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Chroma","title":"<code>Chroma</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to the Chroma service.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Chroma]</code> <p>The type of connection, which is always 'Chroma'.</p> <code>host</code> <code>str</code> <p>The host address of the Chroma service, fetched from the environment variable 'CHROMA_HOST'.</p> <code>port</code> <code>int</code> <p>The port number of the Chroma service, fetched from the environment variable 'CHROMA_PORT'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Chroma(BaseConnection):\n    \"\"\"\n    Represents a connection to the Chroma service.\n\n    Attributes:\n        type (Literal[ConnectionType.Chroma]): The type of connection, which is always 'Chroma'.\n        host (str): The host address of the Chroma service, fetched from the environment variable 'CHROMA_HOST'.\n        port (int): The port number of the Chroma service, fetched from the environment variable 'CHROMA_PORT'.\n    \"\"\"\n\n    type: Literal[ConnectionType.Chroma] = ConnectionType.Chroma\n    host: str = Field(default_factory=partial(get_env_var, \"CHROMA_HOST\"))\n    port: int = Field(default_factory=partial(get_env_var, \"CHROMA_PORT\"))\n\n    @property\n    def vector_store_cls(self):\n        \"\"\"\n        Returns the ChromaVectorStore class.\n\n        This property dynamically imports and returns the ChromaVectorStore class\n        from the 'dynamiq.storages.vector' module.\n\n        Returns:\n            type: The ChromaVectorStore class.\n        \"\"\"\n        from dynamiq.storages.vector import ChromaVectorStore\n\n        return ChromaVectorStore\n\n    def connect(self) -&gt; \"ChromaClient\":\n        \"\"\"\n        Connects to the Chroma service.\n\n        This method establishes a connection to the Chroma service using the provided host and port.\n\n        Returns:\n            ChromaClient: An instance of the ChromaClient connected to the specified host and port.\n        \"\"\"\n        # Import in runtime to save memory\n        from chromadb import HttpClient\n\n        chroma_client = HttpClient(host=self.host, port=self.port)\n        logger.debug(f\"Connected to Chroma with host={self.host} and port={str(self.port)}\")\n        return chroma_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Chroma.vector_store_cls","title":"<code>vector_store_cls</code>  <code>property</code>","text":"<p>Returns the ChromaVectorStore class.</p> <p>This property dynamically imports and returns the ChromaVectorStore class from the 'dynamiq.storages.vector' module.</p> <p>Returns:</p> Name Type Description <code>type</code> <p>The ChromaVectorStore class.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Chroma.connect","title":"<code>connect()</code>","text":"<p>Connects to the Chroma service.</p> <p>This method establishes a connection to the Chroma service using the provided host and port.</p> <p>Returns:</p> Name Type Description <code>ChromaClient</code> <code>ClientAPI</code> <p>An instance of the ChromaClient connected to the specified host and port.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"ChromaClient\":\n    \"\"\"\n    Connects to the Chroma service.\n\n    This method establishes a connection to the Chroma service using the provided host and port.\n\n    Returns:\n        ChromaClient: An instance of the ChromaClient connected to the specified host and port.\n    \"\"\"\n    # Import in runtime to save memory\n    from chromadb import HttpClient\n\n    chroma_client = HttpClient(host=self.host, port=self.port)\n    logger.debug(f\"Connected to Chroma with host={self.host} and port={str(self.port)}\")\n    return chroma_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ConnectionType","title":"<code>ConnectionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>This enum defines various connection types for different services and databases.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ConnectionType(str, enum.Enum):\n    \"\"\"\n    This enum defines various connection types for different services and databases.\n    \"\"\"\n    Anthropic = \"Anthropic\"\n    AWS = \"AWS\"\n    Chroma = \"Chroma\"\n    Cohere = \"Cohere\"\n    Gemini = \"Gemini\"\n    GeminiVertexAI = \"GeminiVertexAI\"\n    HttpApiKey = \"HttpApiKey\"\n    Http = \"Http\"\n    Mistral = \"Mistral\"\n    MySQL = \"MySQL\"\n    OpenAI = \"OpenAI\"\n    Pinecone = \"Pinecone\"\n    Unstructured = \"Unstructured\"\n    Weaviate = \"Weaviate\"\n    Whisper = \"Whisper\"\n    ElevenLabs = \"ElevenLabs\"\n    Tavily = \"Tavily\"\n    ScaleSerp = \"ScaleSerp\"\n    ZenRows = \"ZenRows\"\n    Groq = \"Groq\"\n    TogetherAI = \"TogetherAI\"\n    Anyscale = \"Anyscale\"\n    HuggingFace = \"HuggingFace\"\n    WatsonX = \"WatsonX\"\n    AzureAI = \"AzureAI\"\n    Firecrawl = \"Firecrawl\"\n    E2B = \"E2B\"\n    DeepInfra = \"DeepInfra\"\n    Cerebras = \"Cerebras\"\n    Replicate = \"Replicate\"\n    AI21 = \"AI21\"\n    Qdrant = \"Qdrant\"\n    SambaNova = \"SambaNova\"\n    Milvus = \"Milvus\"\n    Perplexity = \"Perplexity\"\n    DeepSeek = \"DeepSeek\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ElevenLabs","title":"<code>ElevenLabs</code>","text":"<p>               Bases: <code>Http</code></p> <p>Represents a connection to the ElevenLabs API using an HTTP request.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[ElevenLabs]</code> <p>Type of the connection, which is always \"ElevenLabs\".</p> <code>url</code> <code>str</code> <p>URL of the ElevenLabs API.</p> <code>method</code> <code>str</code> <p>HTTP method used for the request, defaults to HTTPMethod.POST.</p> <code>api_key</code> <code>str</code> <p>API key for authentication, fetched from the environment variable \"ELEVENLABS_API_KEY\".</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ElevenLabs(Http):\n    \"\"\"\n    Represents a connection to the ElevenLabs API using an HTTP request.\n\n    Attributes:\n        type (Literal[ConnectionType.ElevenLabs]): Type of the connection, which is always \"ElevenLabs\".\n        url (str): URL of the ElevenLabs API.\n        method (str): HTTP method used for the request, defaults to HTTPMethod.POST.\n        api_key (str): API key for authentication, fetched from the environment variable \"ELEVENLABS_API_KEY\".\n    \"\"\"\n\n    type: Literal[ConnectionType.ElevenLabs] = ConnectionType.ElevenLabs\n    url: str = Field(\n        default_factory=partial(\n            get_env_var,\n            \"ELEVENLABS_URL\",\n            \"https://api.elevenlabs.io/v1/\",\n        )\n    )\n    method: str = HTTPMethod.POST\n    api_key: str = Field(default_factory=partial(get_env_var, \"ELEVENLABS_API_KEY\"))\n\n    def connect(self):\n        \"\"\"\n        Connects to the ElevenLabs API.\n\n        Returns:\n            requests: The `requests` module for making HTTP requests.\n        \"\"\"\n        self.headers.update({\"xi-api-key\": self.api_key})\n        return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ElevenLabs.connect","title":"<code>connect()</code>","text":"<p>Connects to the ElevenLabs API.</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>The <code>requests</code> module for making HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the ElevenLabs API.\n\n    Returns:\n        requests: The `requests` module for making HTTP requests.\n    \"\"\"\n    self.headers.update({\"xi-api-key\": self.api_key})\n    return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Firecrawl","title":"<code>Firecrawl</code>","text":"<p>               Bases: <code>Http</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Firecrawl(Http):\n    type: Literal[ConnectionType.Firecrawl] = ConnectionType.Firecrawl\n    url: str = Field(default=\"https://api.firecrawl.dev/v0/\")\n    api_key: str = Field(default_factory=lambda: get_env_var(\"FIRECRAWL_API_KEY\"))\n    method: Literal[HTTPMethod.POST] = HTTPMethod.POST\n\n    def connect(self):\n        \"\"\"\n        Returns the requests module for making HTTP requests.\n        \"\"\"\n        self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n        return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Firecrawl.connect","title":"<code>connect()</code>","text":"<p>Returns the requests module for making HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Returns the requests module for making HTTP requests.\n    \"\"\"\n    self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n    return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.GeminiVertexAI","title":"<code>GeminiVertexAI</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to the Gemini Vertex AI service.</p> <p>This connection requires additional GCP application credentials. The credentials should be set in the <code>application_default_credentials.json</code> file. The path to this credentials file should be defined in the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable.</p> <p>Attributes:</p> Name Type Description <code>project_id</code> <code>str</code> <p>The GCP project ID.</p> <code>project_location</code> <code>str</code> <p>The location of the GCP project.</p> <code>type</code> <code>Literal[GeminiVertexAI]</code> <p>The type of connection, which is always 'GeminiVertexAI'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class GeminiVertexAI(BaseConnection):\n    \"\"\"\n    Represents a connection to the Gemini Vertex AI service.\n\n    This connection requires additional GCP application credentials. The credentials should be set in the\n    `application_default_credentials.json` file. The path to this credentials file should be defined in the\n    `GOOGLE_APPLICATION_CREDENTIALS` environment variable.\n\n    Attributes:\n        project_id (str): The GCP project ID.\n        project_location (str): The location of the GCP project.\n        type (Literal[ConnectionType.GeminiVertexAI]): The type of connection, which is always 'GeminiVertexAI'.\n    \"\"\"\n\n    project_id: str\n    project_location: str\n    type: Literal[ConnectionType.GeminiVertexAI] = ConnectionType.GeminiVertexAI\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self):\n        \"\"\"\n        Returns the parameters required for the connection.\n\n        This property returns a dictionary containing the project ID and project location.\n\n        Returns:\n            dict: A dictionary with the keys 'vertex_project' and 'vertex_location'.\n        \"\"\"\n        return {\n            \"vertex_project\": self.project_id,\n            \"vertex_location\": self.project_location,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.GeminiVertexAI.conn_params","title":"<code>conn_params</code>  <code>property</code>","text":"<p>Returns the parameters required for the connection.</p> <p>This property returns a dictionary containing the project ID and project location.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the keys 'vertex_project' and 'vertex_location'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HTTPMethod","title":"<code>HTTPMethod</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>This enum defines various method types for different HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class HTTPMethod(str, enum.Enum):\n    \"\"\"\n    This enum defines various method types for different HTTP requests.\n    \"\"\"\n\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    DELETE = \"DELETE\"\n    PATCH = \"PATCH\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Http","title":"<code>Http</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to an API.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[HttpConnection]</code> <p>The type of connection, always 'HttpConnection'.</p> <code>url</code> <code>str</code> <p>The URL of the API.</p> <code>method</code> <code>str</code> <p>HTTP method used for the request, defaults to HTTPMethod.POST.</p> <code>headers</code> <code>dict[str, Any]</code> <p>Additional headers to include in the request, defaults to an empty dictionary.</p> <code>params</code> <code>Optional[dict[str, Any]]</code> <p>Parameters to include in the request, defaults to an empty dictionary.</p> <code>data</code> <code>Optional[dict[str, Any]]</code> <p>Data to include in the request, defaults to an empty dictionary.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Http(BaseConnection):\n    \"\"\"\n    Represents a connection to an API.\n\n    Attributes:\n        type (Literal[ConnectionType.HttpConnection]): The type of connection, always 'HttpConnection'.\n        url (str): The URL of the API.\n        method (str): HTTP method used for the request, defaults to HTTPMethod.POST.\n        headers (dict[str, Any]): Additional headers to include in the request, defaults to an empty dictionary.\n        params (Optional[dict[str, Any]]): Parameters to include in the request, defaults to an empty dictionary.\n        data (Optional[dict[str, Any]]): Data to include in the request, defaults to an empty dictionary.\n    \"\"\"\n\n    type: Literal[ConnectionType.Http] = ConnectionType.Http\n    url: str = \"\"\n    method: HTTPMethod\n    headers: dict[str, Any] = Field(default_factory=dict)\n    params: dict[str, Any] | None = Field(default_factory=dict)\n    data: dict[str, Any] | None = Field(default_factory=dict)\n\n    def connect(self):\n        \"\"\"\n        Connects to the API.\n\n        This method establishes a connection to the API using the provided URL and returns a requests\n        session.\n\n        Returns:\n            requests: A requests module for making HTTP requests to the API.\n        \"\"\"\n        import requests\n\n        return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Http.connect","title":"<code>connect()</code>","text":"<p>Connects to the API.</p> <p>This method establishes a connection to the API using the provided URL and returns a requests session.</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>A requests module for making HTTP requests to the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the API.\n\n    This method establishes a connection to the API using the provided URL and returns a requests\n    session.\n\n    Returns:\n        requests: A requests module for making HTTP requests to the API.\n    \"\"\"\n    import requests\n\n    return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HttpApiKey","title":"<code>HttpApiKey</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to an API that uses an HTTP API key for authentication.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[HttpApiKey]</code> <p>The type of connection, always 'HttpApiKey'.</p> <code>url</code> <code>str</code> <p>The URL of the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class HttpApiKey(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to an API that uses an HTTP API key for authentication.\n\n    Attributes:\n        type (Literal[ConnectionType.HttpApiKey]): The type of connection, always 'HttpApiKey'.\n        url (str): The URL of the API.\n    \"\"\"\n\n    type: Literal[ConnectionType.HttpApiKey] = ConnectionType.HttpApiKey\n    url: str\n\n    def connect(self):\n        \"\"\"\n        Connects to the API.\n\n        This method establishes a connection to the API using the provided URL and returns a requests\n        session.\n\n        Returns:\n            requests: A requests module for making HTTP requests to the API.\n        \"\"\"\n        import requests\n\n        return requests\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing the API key with the key 'api_key' and base url with the key 'api_base'.\n        \"\"\"\n        return {\n            \"api_base\": self.url,\n            \"api_key\": self.api_key,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HttpApiKey.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the API key with the key 'api_key' and base url with the key 'api_base'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.HttpApiKey.connect","title":"<code>connect()</code>","text":"<p>Connects to the API.</p> <p>This method establishes a connection to the API using the provided URL and returns a requests session.</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>A requests module for making HTTP requests to the API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the API.\n\n    This method establishes a connection to the API using the provided URL and returns a requests\n    session.\n\n    Returns:\n        requests: A requests module for making HTTP requests to the API.\n    \"\"\"\n    import requests\n\n    return requests\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Milvus","title":"<code>Milvus</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to the Milvus service.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Milvus]</code> <p>The type of connection, always 'Milvus'.</p> <code>deployment_type</code> <code>MilvusDeploymentType</code> <p>The deployment type of the Milvus service</p> <code>api_key</code> <code>Optional[str]</code> <p>The API key for Milvus</p> <code>uri</code> <code>str</code> <p>The URI for the Milvus instance (file path or host URL).</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Milvus(BaseConnection):\n    \"\"\"\n    Represents a connection to the Milvus service.\n\n    Attributes:\n        type (Literal[ConnectionType.Milvus]): The type of connection, always 'Milvus'.\n        deployment_type (MilvusDeploymentType): The deployment type of the Milvus service\n        api_key (Optional[str]): The API key for Milvus\n        uri (str): The URI for the Milvus instance (file path or host URL).\n    \"\"\"\n\n    type: Literal[ConnectionType.Milvus] = ConnectionType.Milvus\n    deployment_type: MilvusDeploymentType = MilvusDeploymentType.HOST\n    uri: str = Field(default_factory=partial(get_env_var, \"MILVUS_URI\", \"http://localhost:19530\"))\n    api_key: str | None = Field(default_factory=partial(get_env_var, \"MILVUS_API_TOKEN\", None))\n\n    @field_validator(\"uri\")\n    @classmethod\n    def validate_uri(cls, uri: str, values: ValidationInfo) -&gt; str:\n        deployment_type = values.data.get(\"deployment_type\")\n\n        if deployment_type == MilvusDeploymentType.FILE and not uri.endswith(\".db\"):\n            raise ValueError(\"For FILE deployment, URI should point to a file ending with '.db'.\")\n\n        return uri\n\n    def connect(self):\n        from pymilvus import MilvusClient\n\n        if self.deployment_type == MilvusDeploymentType.FILE:\n            milvus_client = MilvusClient(uri=self.uri)\n\n        elif self.deployment_type == MilvusDeploymentType.HOST:\n            if self.api_key:\n                milvus_client = MilvusClient(uri=self.uri, token=self.api_key)\n            else:\n                milvus_client = MilvusClient(uri=self.uri)\n\n        else:\n            raise ValueError(\"Invalid deployment type for Milvus connection.\")\n\n        return milvus_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.MilvusDeploymentType","title":"<code>MilvusDeploymentType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Defines general deployment types for Milvus deployments. Attributes:     FILE (str): Represents a file-based deployment, validated with a .db suffix.     HOST (str): Represents a host-based deployment, which could be a cloud, cluster,                 or single machine with or without authentication.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class MilvusDeploymentType(str, enum.Enum):\n    \"\"\"\n    Defines general deployment types for Milvus deployments.\n    Attributes:\n        FILE (str): Represents a file-based deployment, validated with a .db suffix.\n        HOST (str): Represents a host-based deployment, which could be a cloud, cluster,\n                    or single machine with or without authentication.\n    \"\"\"\n\n    FILE = \"file\"\n    HOST = \"host\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the OpenAI service.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[OpenAI]</code> <p>The type of connection, which is always 'OpenAI'.</p> <code>api_key</code> <code>str</code> <p>The API key for the OpenAI service, fetched from the environment variable 'OPENAI_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class OpenAI(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the OpenAI service.\n\n    Attributes:\n        type (Literal[ConnectionType.OpenAI]): The type of connection, which is always 'OpenAI'.\n        api_key (str): The API key for the OpenAI service, fetched from the environment variable 'OPENAI_API_KEY'.\n    \"\"\"\n    type: Literal[ConnectionType.OpenAI] = ConnectionType.OpenAI\n    api_key: str = Field(default_factory=partial(get_env_var, \"OPENAI_API_KEY\"))\n\n    def connect(self) -&gt; \"OpenAIClient\":\n        \"\"\"\n        Connects to the OpenAI service.\n\n        This method establishes a connection to the OpenAI service using the provided API key.\n\n        Returns:\n            OpenAIClient: An instance of the OpenAIClient connected with the specified API key.\n        \"\"\"\n        # Import in runtime to save memory\n        from openai import OpenAI as OpenAIClient\n        openai_client = OpenAIClient(api_key=self.api_key)\n        logger.debug(\"Connected to OpenAI\")\n        return openai_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.OpenAI.connect","title":"<code>connect()</code>","text":"<p>Connects to the OpenAI service.</p> <p>This method establishes a connection to the OpenAI service using the provided API key.</p> <p>Returns:</p> Name Type Description <code>OpenAIClient</code> <code>OpenAI</code> <p>An instance of the OpenAIClient connected with the specified API key.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"OpenAIClient\":\n    \"\"\"\n    Connects to the OpenAI service.\n\n    This method establishes a connection to the OpenAI service using the provided API key.\n\n    Returns:\n        OpenAIClient: An instance of the OpenAIClient connected with the specified API key.\n    \"\"\"\n    # Import in runtime to save memory\n    from openai import OpenAI as OpenAIClient\n    openai_client = OpenAIClient(api_key=self.api_key)\n    logger.debug(\"Connected to OpenAI\")\n    return openai_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Pinecone","title":"<code>Pinecone</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the Pinecone service.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Pinecone]</code> <p>The type of connection, always 'Pinecone'.</p> <code>api_key</code> <code>str</code> <p>The API key for the service. Defaults to the environment variable 'PINECONE_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Pinecone(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the Pinecone service.\n\n    Attributes:\n        type (Literal[ConnectionType.Pinecone]): The type of connection, always 'Pinecone'.\n        api_key (str): The API key for the service.\n            Defaults to the environment variable 'PINECONE_API_KEY'.\n    \"\"\"\n\n    type: Literal[ConnectionType.Pinecone] = ConnectionType.Pinecone\n    api_key: str = Field(default_factory=partial(get_env_var, \"PINECONE_API_KEY\"))\n\n    def connect(self) -&gt; \"PineconeClient\":\n        \"\"\"\n        Connects to the Pinecone service.\n\n        This method establishes a connection to the Pinecone service using the provided API key.\n\n        Returns:\n            PineconeClient: An instance of the PineconeClient connected to the service.\n        \"\"\"\n        # Import in runtime to save memory\n        from pinecone import Pinecone as PineconeClient\n        pinecone_client = PineconeClient(self.api_key)\n        logger.debug(\"Connected to Pinecone\")\n        return pinecone_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Pinecone.connect","title":"<code>connect()</code>","text":"<p>Connects to the Pinecone service.</p> <p>This method establishes a connection to the Pinecone service using the provided API key.</p> <p>Returns:</p> Name Type Description <code>PineconeClient</code> <code>Pinecone</code> <p>An instance of the PineconeClient connected to the service.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"PineconeClient\":\n    \"\"\"\n    Connects to the Pinecone service.\n\n    This method establishes a connection to the Pinecone service using the provided API key.\n\n    Returns:\n        PineconeClient: An instance of the PineconeClient connected to the service.\n    \"\"\"\n    # Import in runtime to save memory\n    from pinecone import Pinecone as PineconeClient\n    pinecone_client = PineconeClient(self.api_key)\n    logger.debug(\"Connected to Pinecone\")\n    return pinecone_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Qdrant","title":"<code>Qdrant</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the Qdrant service.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Qdrant]</code> <p>The type of connection, always 'Qdrant'.</p> <code>url</code> <code>str</code> <p>The URL of the Qdrant service. Defaults to the environment variable 'QDRANT_URL'.</p> <code>api_key</code> <code>str</code> <p>The API key for the Qdrant service. Defaults to the environment variable 'QDRANT_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Qdrant(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the Qdrant service.\n\n    Attributes:\n        type (Literal[ConnectionType.Qdrant]): The type of connection, always 'Qdrant'.\n        url (str): The URL of the Qdrant service.\n            Defaults to the environment variable 'QDRANT_URL'.\n        api_key (str): The API key for the Qdrant service.\n            Defaults to the environment variable 'QDRANT_API_KEY'.\n    \"\"\"\n\n    type: Literal[ConnectionType.Qdrant] = ConnectionType.Qdrant\n    url: str = Field(default_factory=partial(get_env_var, \"QDRANT_URL\"))\n    api_key: str = Field(default_factory=partial(get_env_var, \"QDRANT_API_KEY\"))\n\n    def connect(self) -&gt; \"QdrantClient\":\n        from qdrant_client import QdrantClient\n\n        qdrant_client = QdrantClient(\n            url=self.url,\n            api_key=self.api_key,\n        )\n\n        return qdrant_client\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ScaleSerp","title":"<code>ScaleSerp</code>","text":"<p>               Bases: <code>Http</code></p> <p>Connection class for Scale SERP Search API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ScaleSerp(Http):\n    \"\"\"\n    Connection class for Scale SERP Search API.\n    \"\"\"\n\n    type: Literal[ConnectionType.ScaleSerp] = ConnectionType.ScaleSerp\n    url: str = \"https://api.scaleserp.com\"\n    api_key: str = Field(default_factory=partial(get_env_var, \"SERP_API_KEY\"))\n    method: str = HTTPMethod.GET\n\n    # Common parameters\n    search_type: SearchType = SearchType.WEB\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def connect(self):\n        \"\"\"\n        Returns the requests module for making HTTP requests.\n        \"\"\"\n        self.params.update({\"api_key\": self.api_key})\n        return super().connect()\n\n    def get_params(self, query: str | None = None, url: str | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Prepare the parameters for the API request.\n        \"\"\"\n        params = {\"api_key\": self.api_key, \"search_type\": self.search_type, **kwargs}\n        if self.search_type == SearchType.WEB:\n            params.pop(\"search_type\")\n\n        if query:\n            params[\"q\"] = query\n        elif url:\n            params[\"url\"] = url\n\n        return {k: v for k, v in params.items() if v is not None}\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ScaleSerp.connect","title":"<code>connect()</code>","text":"<p>Returns the requests module for making HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Returns the requests module for making HTTP requests.\n    \"\"\"\n    self.params.update({\"api_key\": self.api_key})\n    return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ScaleSerp.get_params","title":"<code>get_params(query=None, url=None, **kwargs)</code>","text":"<p>Prepare the parameters for the API request.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def get_params(self, query: str | None = None, url: str | None = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare the parameters for the API request.\n    \"\"\"\n    params = {\"api_key\": self.api_key, \"search_type\": self.search_type, **kwargs}\n    if self.search_type == SearchType.WEB:\n        params.pop(\"search_type\")\n\n    if query:\n        params[\"q\"] = query\n    elif url:\n        params[\"url\"] = url\n\n    return {k: v for k, v in params.items() if v is not None}\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Tavily","title":"<code>Tavily</code>","text":"<p>               Bases: <code>Http</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Tavily(Http):\n    type: Literal[ConnectionType.Tavily] = ConnectionType.Tavily\n    url: str = Field(default=\"https://api.tavily.com\")\n    api_key: str = Field(default_factory=partial(get_env_var, \"TAVILY_API_KEY\"))\n    method: Literal[HTTPMethod.POST] = HTTPMethod.POST\n\n    def connect(self):\n        \"\"\"\n        Returns the requests module for making HTTP requests.\n        \"\"\"\n        self.data.update({\"api_key\": self.api_key})\n        return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Tavily.connect","title":"<code>connect()</code>","text":"<p>Returns the requests module for making HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Returns the requests module for making HTTP requests.\n    \"\"\"\n    self.data.update({\"api_key\": self.api_key})\n    return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Unstructured","title":"<code>Unstructured</code>","text":"<p>               Bases: <code>HttpApiKey</code></p> <p>Represents a connection to the Unstructured API.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Unstructured]</code> <p>The type of connection, which is always 'Unstructured'.</p> <code>url</code> <code>str</code> <p>The URL of the Unstructured API, fetched from the environment variable 'UNSTRUCTURED_API_URL'.</p> <code>api_key</code> <code>str</code> <p>The API key for the Unstructured API, fetched from the environment variable 'UNSTRUCTURED_API_KEY'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Unstructured(HttpApiKey):\n    \"\"\"\n    Represents a connection to the Unstructured API.\n\n    Attributes:\n        type (Literal[ConnectionType.Unstructured]): The type of connection, which is always 'Unstructured'.\n        url (str): The URL of the Unstructured API, fetched from the environment variable 'UNSTRUCTURED_API_URL'.\n        api_key (str): The API key for the Unstructured API, fetched from the environment\n            variable 'UNSTRUCTURED_API_KEY'.\n    \"\"\"\n\n    type: Literal[ConnectionType.Unstructured] = ConnectionType.Unstructured\n    url: str = Field(\n        default_factory=partial(\n            get_env_var,\n            \"UNSTRUCTURED_API_URL\",\n            \"https://api.unstructured.io/\",\n        )\n    )\n    api_key: str = Field(default_factory=partial(get_env_var, \"UNSTRUCTURED_API_KEY\"))\n\n    def connect(self):\n        \"\"\"\n        Connects to the Unstructured API.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Unstructured.connect","title":"<code>connect()</code>","text":"<p>Connects to the Unstructured API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connects to the Unstructured API.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.WatsonX","title":"<code>WatsonX</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class WatsonX(BaseApiKeyConnection):\n    type: Literal[ConnectionType.WatsonX] = ConnectionType.WatsonX\n    api_key: str = Field(default_factory=partial(get_env_var, \"WATSONX_API_KEY\"))\n    project_id: str = Field(default_factory=partial(get_env_var, \"WATSONX_PROJECT_ID\"))\n    url: str = Field(default_factory=partial(get_env_var, \"WATSONX_URL\"))\n\n    def connect(self):\n        pass\n\n    @property\n    def conn_params(self) -&gt; dict:\n        \"\"\"\n        Returns the parameters required for connection.\n\n        Returns:\n            dict: A dictionary containing\n\n                -the API key with the key 'api_key'.\n\n                -the project ID with the key 'project_id'.\n\n                -the url with the key 'url'.\n        \"\"\"\n        return {\n            \"apikey\": self.api_key,\n            \"project_id\": self.project_id,\n            \"url\": self.url,\n        }\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.WatsonX.conn_params","title":"<code>conn_params: dict</code>  <code>property</code>","text":"<p>Returns the parameters required for connection.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing</p> <p>-the API key with the key 'api_key'.</p> <p>-the project ID with the key 'project_id'.</p> <p>-the url with the key 'url'.</p>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Weaviate","title":"<code>Weaviate</code>","text":"<p>               Bases: <code>BaseApiKeyConnection</code></p> <p>Represents a connection to the Weaviate service.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Weaviate]</code> <p>The type of connection, always 'Weaviate'.</p> <code>deployment_type</code> <code>WeaviateDeploymentType</code> <p>The deployment type of the service.</p> <code>api_key</code> <code>str</code> <p>The API key for the service. Defaults to the environment variable 'WEAVIATE_API_KEY'.</p> <code>url</code> <code>str</code> <p>The URL of the service. Defaults to the environment variable 'WEAVIATE_URL'.</p> <code>http_host</code> <code>str</code> <p>The HTTP host for the service. Defaults to the environment variable 'WEAVIATE_HTTP_HOST'.</p> <code>http_port</code> <code>int</code> <p>The HTTP port for the service. Defaults to the environment variable 'WEAVIATE_HTTP_PORT'.</p> <code>grpc_host</code> <code>str</code> <p>The gRPC host for the service. Defaults to the environment variable 'WEAVIATE_GRPC_HOST'.</p> <code>grpc_port</code> <code>int</code> <p>The gRPC port for the service. Defaults to the environment variable 'WEAVIATE_GRPC_PORT'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Weaviate(BaseApiKeyConnection):\n    \"\"\"\n    Represents a connection to the Weaviate service.\n\n    Attributes:\n        type (Literal[ConnectionType.Weaviate]): The type of connection, always 'Weaviate'.\n        deployment_type (WeaviateDeploymentType): The deployment type of the service.\n        api_key (str): The API key for the service.\n            Defaults to the environment variable 'WEAVIATE_API_KEY'.\n        url (str): The URL of the service.\n            Defaults to the environment variable 'WEAVIATE_URL'.\n        http_host (str): The HTTP host for the service.\n            Defaults to the environment variable 'WEAVIATE_HTTP_HOST'.\n        http_port (int): The HTTP port for the service.\n            Defaults to the environment variable 'WEAVIATE_HTTP_PORT'.\n        grpc_host (str): The gRPC host for the service.\n            Defaults to the environment variable 'WEAVIATE_GRPC_HOST'.\n        grpc_port (int): The gRPC port for the service.\n            Defaults to the environment variable 'WEAVIATE_GRPC_PORT'.\n    \"\"\"\n\n    type: Literal[ConnectionType.Weaviate] = ConnectionType.Weaviate\n    deployment_type: WeaviateDeploymentType = WeaviateDeploymentType.WEAVIATE_CLOUD\n    api_key: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_API_KEY\"))\n    url: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_URL\"))\n    http_host: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_HTTP_HOST\"))\n    http_port: int = Field(default_factory=partial(get_env_var, \"WEAVIATE_HTTP_PORT\", 443))\n    grpc_host: str = Field(default_factory=partial(get_env_var, \"WEAVIATE_GRPC_HOST\"))\n    grpc_port: int = Field(default_factory=partial(get_env_var, \"WEAVIATE_GRPC_PORT\", 50051))\n\n    def connect(self) -&gt; \"WeaviateClient\":\n        \"\"\"\n        Connects to the Weaviate service.\n\n        This method establishes a connection to the Weaviate service using the provided URL and API key.\n\n        Returns:\n            WeaviateClient: An instance of the WeaviateClient connected to the specified URL.\n        \"\"\"\n        # Import in runtime to save memory\n        from weaviate import connect_to_custom, connect_to_weaviate_cloud\n        from weaviate.classes.init import AdditionalConfig, Auth, Timeout\n\n        if self.deployment_type == WeaviateDeploymentType.WEAVIATE_CLOUD:\n            weaviate_client = connect_to_weaviate_cloud(\n                cluster_url=self.url,\n                auth_credentials=Auth.api_key(self.api_key),\n            )\n            logger.debug(f\"Connected to Weaviate with url={self.url}\")\n            return weaviate_client\n\n        elif self.deployment_type == WeaviateDeploymentType.CUSTOM:\n            weaviate_client = connect_to_custom(\n                http_host=self.http_host,\n                http_port=self.http_port,\n                http_secure=True,\n                grpc_host=self.grpc_host,\n                grpc_port=self.grpc_port,\n                grpc_secure=True,\n                auth_credentials=Auth.api_key(self.api_key),\n                additional_config=AdditionalConfig(\n                    timeout=Timeout(init=30, query=60, insert=120),  # Values in seconds\n                ),\n                skip_init_checks=False,\n            )\n            logger.debug(f\"Connected to Weaviate with http_host={self.http_host}\")\n            return weaviate_client\n        else:\n            raise ValueError(\"Invalid deployment type\")\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Weaviate.connect","title":"<code>connect()</code>","text":"<p>Connects to the Weaviate service.</p> <p>This method establishes a connection to the Weaviate service using the provided URL and API key.</p> <p>Returns:</p> Name Type Description <code>WeaviateClient</code> <code>WeaviateClient</code> <p>An instance of the WeaviateClient connected to the specified URL.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self) -&gt; \"WeaviateClient\":\n    \"\"\"\n    Connects to the Weaviate service.\n\n    This method establishes a connection to the Weaviate service using the provided URL and API key.\n\n    Returns:\n        WeaviateClient: An instance of the WeaviateClient connected to the specified URL.\n    \"\"\"\n    # Import in runtime to save memory\n    from weaviate import connect_to_custom, connect_to_weaviate_cloud\n    from weaviate.classes.init import AdditionalConfig, Auth, Timeout\n\n    if self.deployment_type == WeaviateDeploymentType.WEAVIATE_CLOUD:\n        weaviate_client = connect_to_weaviate_cloud(\n            cluster_url=self.url,\n            auth_credentials=Auth.api_key(self.api_key),\n        )\n        logger.debug(f\"Connected to Weaviate with url={self.url}\")\n        return weaviate_client\n\n    elif self.deployment_type == WeaviateDeploymentType.CUSTOM:\n        weaviate_client = connect_to_custom(\n            http_host=self.http_host,\n            http_port=self.http_port,\n            http_secure=True,\n            grpc_host=self.grpc_host,\n            grpc_port=self.grpc_port,\n            grpc_secure=True,\n            auth_credentials=Auth.api_key(self.api_key),\n            additional_config=AdditionalConfig(\n                timeout=Timeout(init=30, query=60, insert=120),  # Values in seconds\n            ),\n            skip_init_checks=False,\n        )\n        logger.debug(f\"Connected to Weaviate with http_host={self.http_host}\")\n        return weaviate_client\n    else:\n        raise ValueError(\"Invalid deployment type\")\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.WeaviateDeploymentType","title":"<code>WeaviateDeploymentType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Defines various deployment types for different Weaviate deployments.</p> <p>Attributes:</p> Name Type Description <code>WEAVIATE_CLOUD</code> <code>str</code> <p>Represents a deployment on Weaviate Cloud. Value is 'weaviate_cloud'.</p> <code>CUSTOM</code> <code>str</code> <p>Represents a custom deployment. Value is 'custom'.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class WeaviateDeploymentType(str, enum.Enum):\n    \"\"\"\n    Defines various deployment types for different Weaviate deployments.\n\n    Attributes:\n        WEAVIATE_CLOUD (str): Represents a deployment on Weaviate Cloud.\n            Value is 'weaviate_cloud'.\n        CUSTOM (str): Represents a custom deployment.\n            Value is 'custom'.\n    \"\"\"\n\n    WEAVIATE_CLOUD = \"weaviate_cloud\"\n    CUSTOM = \"custom\"\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Whisper","title":"<code>Whisper</code>","text":"<p>               Bases: <code>Http</code></p> <p>Represents a connection to the Whisper API using an HTTP request.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[Whisper]</code> <p>Type of the connection, which is always \"Whisper\".</p> <code>url</code> <code>str</code> <p>URL of the Whisper API, fetched from the environment variable \"WHISPER_URL\".</p> <code>method</code> <code>str</code> <p>HTTP method used for the request, defaults to HTTPMethod.POST.</p> <code>api_key</code> <code>str</code> <p>API key for authentication, fetched from the environment variable \"OPENAI_API_KEY\".</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class Whisper(Http):\n    \"\"\"\n    Represents a connection to the Whisper API using an HTTP request.\n\n    Attributes:\n        type (Literal[ConnectionType.Whisper]): Type of the connection, which is always \"Whisper\".\n        url (str): URL of the Whisper API, fetched from the environment variable \"WHISPER_URL\".\n        method (str): HTTP method used for the request, defaults to HTTPMethod.POST.\n        api_key (str): API key for authentication, fetched from the environment variable \"OPENAI_API_KEY\".\n    \"\"\"\n    type: Literal[ConnectionType.Whisper] = ConnectionType.Whisper\n    url: str = Field(\n        default_factory=partial(\n            get_env_var, \"WHISPER_URL\", \"https://api.openai.com/v1/\"\n        )\n    )\n    method: str = HTTPMethod.POST\n    api_key: str = Field(default_factory=partial(get_env_var, \"OPENAI_API_KEY\"))\n\n    def connect(self):\n        \"\"\"\n        Configures the request authorization header with the API key for authentication\n\n        Returns:\n            requests: The `requests` module for making HTTP requests.\n        \"\"\"\n        self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n        return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.Whisper.connect","title":"<code>connect()</code>","text":"<p>Configures the request authorization header with the API key for authentication</p> <p>Returns:</p> Name Type Description <code>requests</code> <p>The <code>requests</code> module for making HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Configures the request authorization header with the API key for authentication\n\n    Returns:\n        requests: The `requests` module for making HTTP requests.\n    \"\"\"\n    self.headers.update({\"Authorization\": f\"Bearer {self.api_key}\"})\n    return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ZenRows","title":"<code>ZenRows</code>","text":"<p>               Bases: <code>Http</code></p> <p>Connection class for ZenRows Scrape API.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>class ZenRows(Http):\n    \"\"\"\n    Connection class for ZenRows Scrape API.\n    \"\"\"\n\n    type: Literal[ConnectionType.ZenRows] = ConnectionType.ZenRows\n    url: str = \"https://api.zenrows.com/v1/\"\n    api_key: str = Field(default_factory=partial(get_env_var, \"ZENROWS_API_KEY\"))\n    method: str = HTTPMethod.GET\n\n    def connect(self):\n        \"\"\"\n        Returns the requests module for making HTTP requests.\n        \"\"\"\n        self.params.update({\"apikey\": self.api_key})\n        return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/connections/#dynamiq.connections.connections.ZenRows.connect","title":"<code>connect()</code>","text":"<p>Returns the requests module for making HTTP requests.</p> Source code in <code>dynamiq/connections/connections.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Returns the requests module for making HTTP requests.\n    \"\"\"\n    self.params.update({\"apikey\": self.api_key})\n    return super().connect()\n</code></pre>"},{"location":"dynamiq/connections/managers/","title":"Managers","text":""},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionClientInitType","title":"<code>ConnectionClientInitType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of connection client initialization types.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>class ConnectionClientInitType(str, enum.Enum):\n    \"\"\"Enumeration of connection client initialization types.\"\"\"\n    DEFAULT = \"DEFAULT\"\n    VECTOR_STORE = \"VECTOR_STORE\"\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager","title":"<code>ConnectionManager</code>","text":"<p>Manages connections to various services and databases.</p> <p>This class handles the creation, retrieval, and management of connection clients for different types of services and databases.</p> <p>Attributes:</p> Name Type Description <code>serializer</code> <p>An object used for serializing and deserializing data.</p> <code>connection_clients</code> <code>dict[str, Any]</code> <p>A dictionary storing initialized connection clients.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>class ConnectionManager:\n    \"\"\"\n    Manages connections to various services and databases.\n\n    This class handles the creation, retrieval, and management of connection clients\n    for different types of services and databases.\n\n    Attributes:\n        serializer: An object used for serializing and deserializing data.\n        connection_clients: A dictionary storing initialized connection clients.\n    \"\"\"\n\n    def __init__(self, serializer: Any | None = None):\n        \"\"\"\n        Initializes the ConnectionManager.\n\n        Args:\n            serializer: An optional serializer object. If not provided, JsonPickleSerializer is used.\n        \"\"\"\n        self.serializer = serializer or JsonPickleSerializer()\n        self.connection_clients: dict[str, Any] = {}\n\n    @staticmethod\n    def get_connection_by_type(conn_type: str) -&gt; type[BaseConnection]:\n        \"\"\"\n        Retrieves the connection class based on the given connection type.\n\n        Args:\n            conn_type: The type of connection to retrieve.\n\n        Returns:\n            The connection class corresponding to the given type.\n\n        Raises:\n            ValueError: If the connection type is not found.\n        \"\"\"\n        try:\n            entity_module, entity_name = conn_type.rsplit(\".\", 1)\n            imported_module = importlib.import_module(entity_module)\n            if entity := getattr(imported_module, entity_name, None):\n                return entity\n        except (ModuleNotFoundError, ImportError):\n            raise ValueError(f\"Connection type {conn_type} not found\")\n\n    def get_connection_client(\n        self,\n        connection: BaseConnection,\n        init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n    ) -&gt; Any | None:\n        \"\"\"\n        Retrieves or initializes a connection client for the given connection.\n\n        Args:\n            connection: The connection object.\n            init_type: The initialization type for the connection client.\n\n        Returns:\n            The initialized connection client.\n\n        Raises:\n            ConnectionManagerException: If the connection does not support the specified initialization type.\n        \"\"\"\n        logger.debug(\n            f\"Get connection client for '{connection.id}-{connection.type.value}' \"\n            f\"with '{init_type.value.lower()}' initialization\"\n        )\n        conn_id = self.get_connection_id(connection, init_type)\n        if conn_client := self.connection_clients.get(conn_id):\n            return conn_client\n\n        logger.debug(\n            f\"Init connection client for '{connection.id}-{connection.type.value}' \"\n            f\"with '{init_type.value.lower()}' initialization\"\n        )\n        conn_method_name = CONNECTION_METHOD_BY_INIT_TYPE[init_type]\n        if not (\n            conn_method := getattr(connection, conn_method_name, None)\n        ) or not callable(conn_method):\n            raise ConnectionManagerException(\n                f\"Connection '{connection.id}-{connection.type.value}' not support '{init_type.value}' initialization\"\n            )\n\n        conn_client = conn_method()\n        self.connection_clients[conn_id] = conn_client\n\n        return conn_client\n\n    def get_connection_id(\n        self,\n        connection: BaseConnection,\n        init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n    ) -&gt; str:\n        \"\"\"\n        Generates a unique connection ID based on the connection and initialization type.\n\n        Args:\n            connection: The connection object.\n            init_type: The initialization type for the connection client.\n\n        Returns:\n            A unique string identifier for the connection.\n        \"\"\"\n        conn_hash = self.hash(connection.model_dump_json())\n        return f\"{connection.type.lower()}:{init_type.lower()}:{conn_hash}\"\n\n    def close(self):\n        \"\"\"\n        Closes all open connection clients and clears the connection_clients dictionary.\n        \"\"\"\n        logger.debug(\"Close connection clients\")\n        for conn_client in self.connection_clients.values():\n            if hasattr(conn_client, \"close\"):\n                conn_client.close()\n        self.connection_clients = {}\n\n    @staticmethod\n    def hash(data: str) -&gt; str:\n        \"\"\"\n        Generates a SHA256 hash of the input string.\n\n        Args:\n            data: The input string to hash.\n\n        Returns:\n            The hexadecimal representation of the SHA256 hash.\n        \"\"\"\n        return hashlib.sha256(data.encode()).hexdigest()\n\n    def dumps(self, data: Any):\n        \"\"\"\n        Serializes the given data using the serializer.\n\n        Args:\n            data: The data to serialize.\n\n        Returns:\n            The serialized data.\n        \"\"\"\n        return self.serializer.dumps(data)\n\n    def loads(self, value: str):\n        \"\"\"\n        Deserializes the given value using the serializer.\n\n        Args:\n            value: The serialized string to deserialize.\n\n        Returns:\n            The deserialized data.\n        \"\"\"\n        return self.serializer.loads(value)\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.__init__","title":"<code>__init__(serializer=None)</code>","text":"<p>Initializes the ConnectionManager.</p> <p>Parameters:</p> Name Type Description Default <code>serializer</code> <code>Any | None</code> <p>An optional serializer object. If not provided, JsonPickleSerializer is used.</p> <code>None</code> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def __init__(self, serializer: Any | None = None):\n    \"\"\"\n    Initializes the ConnectionManager.\n\n    Args:\n        serializer: An optional serializer object. If not provided, JsonPickleSerializer is used.\n    \"\"\"\n    self.serializer = serializer or JsonPickleSerializer()\n    self.connection_clients: dict[str, Any] = {}\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.close","title":"<code>close()</code>","text":"<p>Closes all open connection clients and clears the connection_clients dictionary.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes all open connection clients and clears the connection_clients dictionary.\n    \"\"\"\n    logger.debug(\"Close connection clients\")\n    for conn_client in self.connection_clients.values():\n        if hasattr(conn_client, \"close\"):\n            conn_client.close()\n    self.connection_clients = {}\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.dumps","title":"<code>dumps(data)</code>","text":"<p>Serializes the given data using the serializer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to serialize.</p> required <p>Returns:</p> Type Description <p>The serialized data.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def dumps(self, data: Any):\n    \"\"\"\n    Serializes the given data using the serializer.\n\n    Args:\n        data: The data to serialize.\n\n    Returns:\n        The serialized data.\n    \"\"\"\n    return self.serializer.dumps(data)\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.get_connection_by_type","title":"<code>get_connection_by_type(conn_type)</code>  <code>staticmethod</code>","text":"<p>Retrieves the connection class based on the given connection type.</p> <p>Parameters:</p> Name Type Description Default <code>conn_type</code> <code>str</code> <p>The type of connection to retrieve.</p> required <p>Returns:</p> Type Description <code>type[BaseConnection]</code> <p>The connection class corresponding to the given type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the connection type is not found.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>@staticmethod\ndef get_connection_by_type(conn_type: str) -&gt; type[BaseConnection]:\n    \"\"\"\n    Retrieves the connection class based on the given connection type.\n\n    Args:\n        conn_type: The type of connection to retrieve.\n\n    Returns:\n        The connection class corresponding to the given type.\n\n    Raises:\n        ValueError: If the connection type is not found.\n    \"\"\"\n    try:\n        entity_module, entity_name = conn_type.rsplit(\".\", 1)\n        imported_module = importlib.import_module(entity_module)\n        if entity := getattr(imported_module, entity_name, None):\n            return entity\n    except (ModuleNotFoundError, ImportError):\n        raise ValueError(f\"Connection type {conn_type} not found\")\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.get_connection_client","title":"<code>get_connection_client(connection, init_type=ConnectionClientInitType.DEFAULT)</code>","text":"<p>Retrieves or initializes a connection client for the given connection.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>BaseConnection</code> <p>The connection object.</p> required <code>init_type</code> <code>ConnectionClientInitType</code> <p>The initialization type for the connection client.</p> <code>DEFAULT</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>The initialized connection client.</p> <p>Raises:</p> Type Description <code>ConnectionManagerException</code> <p>If the connection does not support the specified initialization type.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def get_connection_client(\n    self,\n    connection: BaseConnection,\n    init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n) -&gt; Any | None:\n    \"\"\"\n    Retrieves or initializes a connection client for the given connection.\n\n    Args:\n        connection: The connection object.\n        init_type: The initialization type for the connection client.\n\n    Returns:\n        The initialized connection client.\n\n    Raises:\n        ConnectionManagerException: If the connection does not support the specified initialization type.\n    \"\"\"\n    logger.debug(\n        f\"Get connection client for '{connection.id}-{connection.type.value}' \"\n        f\"with '{init_type.value.lower()}' initialization\"\n    )\n    conn_id = self.get_connection_id(connection, init_type)\n    if conn_client := self.connection_clients.get(conn_id):\n        return conn_client\n\n    logger.debug(\n        f\"Init connection client for '{connection.id}-{connection.type.value}' \"\n        f\"with '{init_type.value.lower()}' initialization\"\n    )\n    conn_method_name = CONNECTION_METHOD_BY_INIT_TYPE[init_type]\n    if not (\n        conn_method := getattr(connection, conn_method_name, None)\n    ) or not callable(conn_method):\n        raise ConnectionManagerException(\n            f\"Connection '{connection.id}-{connection.type.value}' not support '{init_type.value}' initialization\"\n        )\n\n    conn_client = conn_method()\n    self.connection_clients[conn_id] = conn_client\n\n    return conn_client\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.get_connection_id","title":"<code>get_connection_id(connection, init_type=ConnectionClientInitType.DEFAULT)</code>","text":"<p>Generates a unique connection ID based on the connection and initialization type.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>BaseConnection</code> <p>The connection object.</p> required <code>init_type</code> <code>ConnectionClientInitType</code> <p>The initialization type for the connection client.</p> <code>DEFAULT</code> <p>Returns:</p> Type Description <code>str</code> <p>A unique string identifier for the connection.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def get_connection_id(\n    self,\n    connection: BaseConnection,\n    init_type: ConnectionClientInitType = ConnectionClientInitType.DEFAULT,\n) -&gt; str:\n    \"\"\"\n    Generates a unique connection ID based on the connection and initialization type.\n\n    Args:\n        connection: The connection object.\n        init_type: The initialization type for the connection client.\n\n    Returns:\n        A unique string identifier for the connection.\n    \"\"\"\n    conn_hash = self.hash(connection.model_dump_json())\n    return f\"{connection.type.lower()}:{init_type.lower()}:{conn_hash}\"\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.hash","title":"<code>hash(data)</code>  <code>staticmethod</code>","text":"<p>Generates a SHA256 hash of the input string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The input string to hash.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The hexadecimal representation of the SHA256 hash.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>@staticmethod\ndef hash(data: str) -&gt; str:\n    \"\"\"\n    Generates a SHA256 hash of the input string.\n\n    Args:\n        data: The input string to hash.\n\n    Returns:\n        The hexadecimal representation of the SHA256 hash.\n    \"\"\"\n    return hashlib.sha256(data.encode()).hexdigest()\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManager.loads","title":"<code>loads(value)</code>","text":"<p>Deserializes the given value using the serializer.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The serialized string to deserialize.</p> required <p>Returns:</p> Type Description <p>The deserialized data.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>def loads(self, value: str):\n    \"\"\"\n    Deserializes the given value using the serializer.\n\n    Args:\n        value: The serialized string to deserialize.\n\n    Returns:\n        The deserialized data.\n    \"\"\"\n    return self.serializer.loads(value)\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.ConnectionManagerException","title":"<code>ConnectionManagerException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in the ConnectionManager.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>class ConnectionManagerException(Exception):\n    \"\"\"Exception raised for errors in the ConnectionManager.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/connections/managers/#dynamiq.connections.managers.get_connection_manager","title":"<code>get_connection_manager()</code>","text":"<p>A context manager that yields a ConnectionManager instance and ensures it's closed properly.</p> <p>Yields:</p> Type Description <p>A ConnectionManager instance.</p> Source code in <code>dynamiq/connections/managers.py</code> <pre><code>@contextmanager\ndef get_connection_manager():\n    \"\"\"\n    A context manager that yields a ConnectionManager instance and ensures it's closed properly.\n\n    Yields:\n        A ConnectionManager instance.\n    \"\"\"\n    cm = ConnectionManager()\n    yield cm\n    cm.close()\n</code></pre>"},{"location":"dynamiq/connections/storages/","title":"Storages","text":""},{"location":"dynamiq/connections/storages/#dynamiq.connections.storages.RedisConnection","title":"<code>RedisConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Represents a connection to a Redis database.</p> <p>This class inherits from BaseConnection and provides specific attributes for connecting to a Redis database.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>The hostname or IP address of the Redis server.</p> <code>port</code> <code>int</code> <p>The port number on which the Redis server is listening.</p> <code>db</code> <code>int</code> <p>The Redis database number to connect to.</p> <code>username</code> <code>str | None</code> <p>The username for authentication (optional).</p> <code>password</code> <code>str | None</code> <p>The password for authentication (optional).</p> <code>type</code> <code>Literal[Redis]</code> <p>The connection type, always set to Redis.</p> Source code in <code>dynamiq/connections/storages.py</code> <pre><code>class RedisConnection(BaseConnection):\n    \"\"\"\n    Represents a connection to a Redis database.\n\n    This class inherits from BaseConnection and provides specific attributes\n    for connecting to a Redis database.\n\n    Attributes:\n        host (str): The hostname or IP address of the Redis server.\n        port (int): The port number on which the Redis server is listening.\n        db (int): The Redis database number to connect to.\n        username (str | None): The username for authentication (optional).\n        password (str | None): The password for authentication (optional).\n        type (Literal[StorageConnectionType.Redis]): The connection type, always set to Redis.\n    \"\"\"\n\n    host: str\n    port: int\n    db: int\n    username: str | None = None\n    password: str | None = None\n    type: Literal[StorageConnectionType.Redis] = StorageConnectionType.Redis\n\n    def connect(self):\n        \"\"\"\n        Establishes a connection to the Redis database.\n\n        This method is responsible for creating and initializing the connection\n        to the Redis server using the provided connection details.\n\n        Note:\n            This method is currently a placeholder and does not contain\n            the actual implementation for connecting to Redis.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/connections/storages/#dynamiq.connections.storages.RedisConnection.connect","title":"<code>connect()</code>","text":"<p>Establishes a connection to the Redis database.</p> <p>This method is responsible for creating and initializing the connection to the Redis server using the provided connection details.</p> Note <p>This method is currently a placeholder and does not contain the actual implementation for connecting to Redis.</p> Source code in <code>dynamiq/connections/storages.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Establishes a connection to the Redis database.\n\n    This method is responsible for creating and initializing the connection\n    to the Redis server using the provided connection details.\n\n    Note:\n        This method is currently a placeholder and does not contain\n        the actual implementation for connecting to Redis.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/connections/storages/#dynamiq.connections.storages.StorageConnectionType","title":"<code>StorageConnectionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of storage connection types.</p> <p>Attributes:</p> Name Type Description <code>Redis</code> <code>str</code> <p>Represents a Redis connection type.</p> Source code in <code>dynamiq/connections/storages.py</code> <pre><code>class StorageConnectionType(str, enum.Enum):\n    \"\"\"\n    Enumeration of storage connection types.\n\n    Attributes:\n        Redis (str): Represents a Redis connection type.\n    \"\"\"\n    Redis = \"Redis\"\n</code></pre>"},{"location":"dynamiq/executors/base/","title":"Base","text":""},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor","title":"<code>BaseExecutor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for executors that run nodes in a workflow.</p> <p>Attributes:</p> Name Type Description <code>max_workers</code> <code>int | None</code> <p>Maximum number of concurrent workers. None means no limit.</p> Source code in <code>dynamiq/executors/base.py</code> <pre><code>class BaseExecutor(ABC):\n    \"\"\"\n    Abstract base class for executors that run nodes in a workflow.\n\n    Attributes:\n        max_workers (int | None): Maximum number of concurrent workers. None means no limit.\n    \"\"\"\n\n    def __init__(self, max_workers: int | None = None):\n        \"\"\"\n        Initialize the BaseExecutor.\n\n        Args:\n            max_workers (int | None, optional): Maximum number of concurrent workers. Defaults to None.\n        \"\"\"\n        self.max_workers = max_workers\n\n    @abstractmethod\n    def shutdown(self, wait: bool = True):\n        \"\"\"\n        Shut down the executor.\n\n        Args:\n            wait (bool, optional): Whether to wait for pending tasks to complete. Defaults to True.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def execute(\n        self,\n        ready_nodes: list[NodeReadyToRun],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Execute the given nodes that are ready to run.\n\n        Args:\n            ready_nodes (list[NodeReadyToRun]): List of nodes ready for execution.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, RunnableResult]: A dictionary mapping node IDs to their execution results.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor.__init__","title":"<code>__init__(max_workers=None)</code>","text":"<p>Initialize the BaseExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int | None</code> <p>Maximum number of concurrent workers. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/base.py</code> <pre><code>def __init__(self, max_workers: int | None = None):\n    \"\"\"\n    Initialize the BaseExecutor.\n\n    Args:\n        max_workers (int | None, optional): Maximum number of concurrent workers. Defaults to None.\n    \"\"\"\n    self.max_workers = max_workers\n</code></pre>"},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor.execute","title":"<code>execute(ready_nodes, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute the given nodes that are ready to run.</p> <p>Parameters:</p> Name Type Description Default <code>ready_nodes</code> <code>list[NodeReadyToRun]</code> <p>List of nodes ready for execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>dict[str, RunnableResult]: A dictionary mapping node IDs to their execution results.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>dynamiq/executors/base.py</code> <pre><code>@abstractmethod\ndef execute(\n    self,\n    ready_nodes: list[NodeReadyToRun],\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Execute the given nodes that are ready to run.\n\n    Args:\n        ready_nodes (list[NodeReadyToRun]): List of nodes ready for execution.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, RunnableResult]: A dictionary mapping node IDs to their execution results.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/executors/base/#dynamiq.executors.base.BaseExecutor.shutdown","title":"<code>shutdown(wait=True)</code>  <code>abstractmethod</code>","text":"<p>Shut down the executor.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>Whether to wait for pending tasks to complete. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>dynamiq/executors/base.py</code> <pre><code>@abstractmethod\ndef shutdown(self, wait: bool = True):\n    \"\"\"\n    Shut down the executor.\n\n    Args:\n        wait (bool, optional): Whether to wait for pending tasks to complete. Defaults to True.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/executors/pool/","title":"Pool","text":""},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor","title":"<code>PoolExecutor</code>","text":"<p>               Bases: <code>BaseExecutor</code></p> <p>A pool executor that manages concurrent execution of nodes using either ThreadPoolExecutor or ProcessPoolExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>pool_executor</code> <code>type</code> <p>The type of pool executor to use (ThreadPoolExecutor or ProcessPoolExecutor).</p> required <code>max_workers</code> <code>int</code> <p>The maximum number of workers in the pool. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>class PoolExecutor(BaseExecutor):\n    \"\"\"\n    A pool executor that manages concurrent execution of nodes using either ThreadPoolExecutor or\n    ProcessPoolExecutor.\n\n    Args:\n        pool_executor (type): The type of pool executor to use (ThreadPoolExecutor or\n            ProcessPoolExecutor).\n        max_workers (int, optional): The maximum number of workers in the pool. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        pool_executor: (\n            type[futures.ThreadPoolExecutor] | type[futures.ProcessPoolExecutor]\n        ),\n        max_workers: int | None = None,\n    ):\n        super().__init__(max_workers=max_workers)\n        self.executor = pool_executor(max_workers=max_workers)\n        self.node_by_future = {}\n\n    def shutdown(self, wait: bool = True):\n        \"\"\"\n        Shuts down the executor.\n\n        Args:\n            wait (bool, optional): Whether to wait for pending futures to complete. Defaults to True.\n        \"\"\"\n        self.executor.shutdown(wait=wait)\n\n    def execute(\n        self,\n        ready_nodes: list[NodeReadyToRun],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Executes the given ready nodes and returns their results.\n\n        Args:\n            ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n        \"\"\"\n        self.run_nodes(ready_nodes=ready_nodes, config=config, **kwargs)\n        completed_node_futures, _ = futures.wait(\n            fs=self.node_by_future.keys(), return_when=futures.FIRST_COMPLETED\n        )\n        results = self.complete_nodes(completed_node_futures=completed_node_futures)\n\n        return results\n\n    def run_nodes(\n        self,\n        ready_nodes: list[NodeReadyToRun],\n        config: RunnableConfig = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Submits ready nodes for execution.\n\n        Args:\n            ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for ready_node in ready_nodes:\n            if ready_node.is_ready:\n                future = self.run_node(ready_node=ready_node, config=config, **kwargs)\n                self.node_by_future[future] = ready_node.node\n            else:\n                logger.error(\n                    f\"Node {ready_node.node.name} - {ready_node.node.id}: not ready to run.\"\n                )\n\n    def complete_nodes(\n        self, completed_node_futures: list[futures.Future]\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Processes completed node futures and returns their results.\n\n        Args:\n            completed_node_futures (list[futures.Future]): List of completed node futures.\n\n        Returns:\n            dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n        \"\"\"\n        results = {}\n        for f in completed_node_futures:\n            node = self.node_by_future.pop(f)\n            try:\n                node_result: RunnableResult = f.result()\n            except Exception as e:\n                logger.error(\n                    f\"Node {node.name} - {node.id}: execution failed due the unexpected error. Error: {e}\"\n                )\n                node_result = RunnableResult(status=RunnableStatus.FAILURE)\n\n            results[node.id] = node_result\n\n        return results\n\n    def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Submits ready node for execution.\n\n        Args:\n            ready_node (NodeReadyToRun): node ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        return self.executor.submit(\n            ready_node.node.run,\n            input_data=ready_node.input_data,\n            config=config,\n            depends_result=ready_node.depends_result,\n            **kwargs,\n        )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.complete_nodes","title":"<code>complete_nodes(completed_node_futures)</code>","text":"<p>Processes completed node futures and returns their results.</p> <p>Parameters:</p> Name Type Description Default <code>completed_node_futures</code> <code>list[Future]</code> <p>List of completed node futures.</p> required <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>dict[str, RunnableResult]: A dictionary of node IDs and their execution results.</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def complete_nodes(\n    self, completed_node_futures: list[futures.Future]\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Processes completed node futures and returns their results.\n\n    Args:\n        completed_node_futures (list[futures.Future]): List of completed node futures.\n\n    Returns:\n        dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n    \"\"\"\n    results = {}\n    for f in completed_node_futures:\n        node = self.node_by_future.pop(f)\n        try:\n            node_result: RunnableResult = f.result()\n        except Exception as e:\n            logger.error(\n                f\"Node {node.name} - {node.id}: execution failed due the unexpected error. Error: {e}\"\n            )\n            node_result = RunnableResult(status=RunnableStatus.FAILURE)\n\n        results[node.id] = node_result\n\n    return results\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.execute","title":"<code>execute(ready_nodes, config=None, **kwargs)</code>","text":"<p>Executes the given ready nodes and returns their results.</p> <p>Parameters:</p> Name Type Description Default <code>ready_nodes</code> <code>list[NodeReadyToRun]</code> <p>List of nodes ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>dict[str, RunnableResult]: A dictionary of node IDs and their execution results.</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def execute(\n    self,\n    ready_nodes: list[NodeReadyToRun],\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Executes the given ready nodes and returns their results.\n\n    Args:\n        ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, RunnableResult]: A dictionary of node IDs and their execution results.\n    \"\"\"\n    self.run_nodes(ready_nodes=ready_nodes, config=config, **kwargs)\n    completed_node_futures, _ = futures.wait(\n        fs=self.node_by_future.keys(), return_when=futures.FIRST_COMPLETED\n    )\n    results = self.complete_nodes(completed_node_futures=completed_node_futures)\n\n    return results\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.run_node","title":"<code>run_node(ready_node, config=None, **kwargs)</code>","text":"<p>Submits ready node for execution.</p> <p>Parameters:</p> Name Type Description Default <code>ready_node</code> <code>NodeReadyToRun</code> <p>node ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Submits ready node for execution.\n\n    Args:\n        ready_node (NodeReadyToRun): node ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    return self.executor.submit(\n        ready_node.node.run,\n        input_data=ready_node.input_data,\n        config=config,\n        depends_result=ready_node.depends_result,\n        **kwargs,\n    )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.run_nodes","title":"<code>run_nodes(ready_nodes, config=None, **kwargs)</code>","text":"<p>Submits ready nodes for execution.</p> <p>Parameters:</p> Name Type Description Default <code>ready_nodes</code> <code>list[NodeReadyToRun]</code> <p>List of nodes ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def run_nodes(\n    self,\n    ready_nodes: list[NodeReadyToRun],\n    config: RunnableConfig = None,\n    **kwargs,\n):\n    \"\"\"\n    Submits ready nodes for execution.\n\n    Args:\n        ready_nodes (list[NodeReadyToRun]): List of nodes ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for ready_node in ready_nodes:\n        if ready_node.is_ready:\n            future = self.run_node(ready_node=ready_node, config=config, **kwargs)\n            self.node_by_future[future] = ready_node.node\n        else:\n            logger.error(\n                f\"Node {ready_node.node.name} - {ready_node.node.id}: not ready to run.\"\n            )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.PoolExecutor.shutdown","title":"<code>shutdown(wait=True)</code>","text":"<p>Shuts down the executor.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>Whether to wait for pending futures to complete. Defaults to True.</p> <code>True</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def shutdown(self, wait: bool = True):\n    \"\"\"\n    Shuts down the executor.\n\n    Args:\n        wait (bool, optional): Whether to wait for pending futures to complete. Defaults to True.\n    \"\"\"\n    self.executor.shutdown(wait=wait)\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor","title":"<code>ProcessExecutor</code>","text":"<p>               Bases: <code>PoolExecutor</code></p> <p>A process-based pool executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker processes. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>class ProcessExecutor(PoolExecutor):\n    \"\"\"\n    A process-based pool executor.\n\n    Args:\n        max_workers (int, optional): The maximum number of worker processes. Defaults to None.\n    \"\"\"\n\n    def __init__(self, max_workers: int | None = None):\n        max_workers = max_workers or MAX_WORKERS_PROCESS_POOL_EXECUTOR\n        super().__init__(\n            pool_executor=futures.ProcessPoolExecutor, max_workers=max_workers\n        )\n\n    @staticmethod\n    def serialize_node(node: Node) -&gt; str:\n        \"\"\"\n        Serializes node data.\n\n        Args:\n            node (Node): Node instance.\n\n        Returns:\n            str: Serialized node data\n        \"\"\"\n        return jsonpickle.encode(node)\n\n    @staticmethod\n    def deserialize_node(node_data: str) -&gt; Node:\n        \"\"\"\n        Deserializes node data.\n\n        Args:\n            node_data (str): Serialized node data.\n\n        Returns:\n            Node: Node instance.\n        \"\"\"\n        return jsonpickle.decode(node_data)  # nosec\n\n    @classmethod\n    def _run_node(cls, node_data: str, **kwargs) -&gt; RunnableResult:\n        \"\"\"\n        Deserializes node data and runs the node.\n\n        Args:\n            node_data (str): Serialized node data.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        node_instance = cls.deserialize_node(node_data)\n        return node_instance.run(**kwargs)\n\n    def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Submits ready node for execution.\n\n        Args:\n            ready_node (NodeReadyToRun): node ready to run.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        return self.executor.submit(\n            self._run_node,\n            node_data=self.serialize_node(ready_node.node),\n            input_data=ready_node.input_data,\n            config=config,\n            depends_result=ready_node.depends_result,\n            **kwargs,\n        )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor.deserialize_node","title":"<code>deserialize_node(node_data)</code>  <code>staticmethod</code>","text":"<p>Deserializes node data.</p> <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>str</code> <p>Serialized node data.</p> required <p>Returns:</p> Name Type Description <code>Node</code> <code>Node</code> <p>Node instance.</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>@staticmethod\ndef deserialize_node(node_data: str) -&gt; Node:\n    \"\"\"\n    Deserializes node data.\n\n    Args:\n        node_data (str): Serialized node data.\n\n    Returns:\n        Node: Node instance.\n    \"\"\"\n    return jsonpickle.decode(node_data)  # nosec\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor.run_node","title":"<code>run_node(ready_node, config=None, **kwargs)</code>","text":"<p>Submits ready node for execution.</p> <p>Parameters:</p> Name Type Description Default <code>ready_node</code> <code>NodeReadyToRun</code> <p>node ready to run.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>def run_node(self, ready_node: NodeReadyToRun, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Submits ready node for execution.\n\n    Args:\n        ready_node (NodeReadyToRun): node ready to run.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    return self.executor.submit(\n        self._run_node,\n        node_data=self.serialize_node(ready_node.node),\n        input_data=ready_node.input_data,\n        config=config,\n        depends_result=ready_node.depends_result,\n        **kwargs,\n    )\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ProcessExecutor.serialize_node","title":"<code>serialize_node(node)</code>  <code>staticmethod</code>","text":"<p>Serializes node data.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>Node instance.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Serialized node data</p> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>@staticmethod\ndef serialize_node(node: Node) -&gt; str:\n    \"\"\"\n    Serializes node data.\n\n    Args:\n        node (Node): Node instance.\n\n    Returns:\n        str: Serialized node data\n    \"\"\"\n    return jsonpickle.encode(node)\n</code></pre>"},{"location":"dynamiq/executors/pool/#dynamiq.executors.pool.ThreadExecutor","title":"<code>ThreadExecutor</code>","text":"<p>               Bases: <code>PoolExecutor</code></p> <p>A thread-based pool executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker threads. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/executors/pool.py</code> <pre><code>class ThreadExecutor(PoolExecutor):\n    \"\"\"\n    A thread-based pool executor.\n\n    Args:\n        max_workers (int, optional): The maximum number of worker threads. Defaults to None.\n    \"\"\"\n\n    def __init__(self, max_workers: int | None = None):\n        max_workers = max_workers or MAX_WORKERS_THREAD_POOL_EXECUTOR\n        super().__init__(\n            pool_executor=futures.ThreadPoolExecutor, max_workers=max_workers\n        )\n</code></pre>"},{"location":"dynamiq/flows/base/","title":"Base","text":""},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow","title":"<code>BaseFlow</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Runnable</code>, <code>ABC</code></p> <p>Base class for flow implementations.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the flow, generated using UUID.</p> Source code in <code>dynamiq/flows/base.py</code> <pre><code>class BaseFlow(BaseModel, Runnable, ABC):\n    \"\"\"\n    Base class for flow implementations.\n\n    Attributes:\n        id (str): Unique identifier for the flow, generated using UUID.\n\n    \"\"\"\n\n    id: str = Field(default_factory=generate_uuid)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the BaseFlow instance.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._results = {}\n\n    def reset_run_state(self):\n        \"\"\"Reset the internal run state by clearing the results dictionary.\"\"\"\n        self._results = {}\n\n    def run_on_flow_start(\n        self, input_data: Any, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"\n        Execute callbacks when the flow starts.\n\n        Args:\n            input_data (Any): The input data for the flow.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments to be passed to the callbacks.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                callback.on_flow_start(self.model_dump(), input_data, **kwargs)\n\n    def run_on_flow_end(\n        self, output_data: Any, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"\n        Execute callbacks when the flow ends.\n\n        Args:\n            output_data (Any): The output data from the flow.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments to be passed to the callbacks.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                callback.on_flow_end(self.model_dump(), output_data, **kwargs)\n\n    def run_on_flow_error(\n        self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"\n        Execute callbacks when an error occurs in the flow.\n\n        Args:\n            error (BaseException): The error that occurred during the flow execution.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments to be passed to the callbacks.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                callback.on_flow_error(self.model_dump(), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BaseFlow instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent constructor.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the BaseFlow instance.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._results = {}\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the internal run state by clearing the results dictionary.</p> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Reset the internal run state by clearing the results dictionary.\"\"\"\n    self._results = {}\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.run_on_flow_end","title":"<code>run_on_flow_end(output_data, config=None, **kwargs)</code>","text":"<p>Execute callbacks when the flow ends.</p> <p>Parameters:</p> Name Type Description Default <code>output_data</code> <code>Any</code> <p>The output data from the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the callbacks.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def run_on_flow_end(\n    self, output_data: Any, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"\n    Execute callbacks when the flow ends.\n\n    Args:\n        output_data (Any): The output data from the flow.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments to be passed to the callbacks.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            callback.on_flow_end(self.model_dump(), output_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.run_on_flow_error","title":"<code>run_on_flow_error(error, config=None, **kwargs)</code>","text":"<p>Execute callbacks when an error occurs in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred during the flow execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the callbacks.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def run_on_flow_error(\n    self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"\n    Execute callbacks when an error occurs in the flow.\n\n    Args:\n        error (BaseException): The error that occurred during the flow execution.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments to be passed to the callbacks.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            callback.on_flow_error(self.model_dump(), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/base/#dynamiq.flows.base.BaseFlow.run_on_flow_start","title":"<code>run_on_flow_start(input_data, config=None, **kwargs)</code>","text":"<p>Execute callbacks when the flow starts.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data for the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the callbacks.</p> <code>{}</code> Source code in <code>dynamiq/flows/base.py</code> <pre><code>def run_on_flow_start(\n    self, input_data: Any, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"\n    Execute callbacks when the flow starts.\n\n    Args:\n        input_data (Any): The input data for the flow.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments to be passed to the callbacks.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            callback.on_flow_start(self.model_dump(), input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/flows/flow/","title":"Flow","text":""},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow","title":"<code>Flow</code>","text":"<p>               Bases: <code>BaseFlow</code></p> <p>Represents a flow of nodes to be executed.</p> <p>Attributes:</p> Name Type Description <code>nodes</code> <code>list[Node]</code> <p>List of nodes in the flow.</p> <code>executor</code> <code>type[BaseExecutor]</code> <p>Executor class for running nodes. Defaults to ThreadExecutor.</p> <code>max_node_workers</code> <code>int | None</code> <p>Maximum number of concurrent node workers. Defaults to None.</p> <code>connection_manager</code> <code>ConnectionManager</code> <p>Manager for handling connections. Defaults to ConnectionManager().</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>class Flow(BaseFlow):\n    \"\"\"\n    Represents a flow of nodes to be executed.\n\n    Attributes:\n        nodes (list[Node]): List of nodes in the flow.\n        executor (type[BaseExecutor]): Executor class for running nodes. Defaults to ThreadExecutor.\n        max_node_workers (int | None): Maximum number of concurrent node workers. Defaults to None.\n        connection_manager (ConnectionManager): Manager for handling connections. Defaults to ConnectionManager().\n    \"\"\"\n\n    nodes: list[Node] = []\n    executor: type[BaseExecutor] = ThreadExecutor\n    max_node_workers: int | None = None\n    connection_manager: ConnectionManager = ConnectionManager()\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the Flow instance.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._node_by_id = {node.id: node for node in self.nodes}\n        self._ts = None\n\n        self._init_components()\n        self.reset_run_state()\n\n    @field_validator(\"nodes\")\n    @classmethod\n    def validate_nodes(cls, nodes: list[Node]) -&gt; list[Node]:\n        \"\"\"\n        Validates the list of nodes in the flow.\n\n        Args:\n            nodes (list[Node]): List of nodes to validate.\n\n        Returns:\n            list[Node]: Validated list of nodes.\n\n        Raises:\n            ValueError: If there are duplicate node IDs or invalid dependencies.\n        \"\"\"\n        nodes_ids_unique = set()\n        nodes_deps_ids_unique = set()\n        for node in nodes:\n            if node.id in nodes_ids_unique:\n                raise ValueError(\n                    f\"Flow has nodes with duplicated ids: '{node.id}'. Node ids must be unique.\"\n                )\n\n            nodes_ids_unique.add(node.id)\n            node_deps_ids = [dep.node.id for dep in node.depends]\n            if len(set(node_deps_ids)) != len(node_deps_ids):\n                raise ValueError(\n                    f\"Flow node '{node.id}' has duplicated dependency ids. Node dependencies ids must be unique.\"\n                )\n\n            nodes_deps_ids_unique.update(node_deps_ids)\n\n        if not nodes_deps_ids_unique.issubset(nodes_ids_unique):\n            raise ValueError(\n                \"Flow nodes have dependencies that are not present in the flow.\"\n            )\n\n        return nodes\n\n    def _init_components(self):\n        \"\"\"Initializes components for nodes with postponed initialization.\"\"\"\n        for node in self.nodes:\n            if node.is_postponed_component_init:\n                node.init_components(self.connection_manager)\n\n    def _get_nodes_ready_to_run(self, input_data: Any) -&gt; list[NodeReadyToRun]:\n        \"\"\"\n        Gets the list of nodes that are ready to run.\n\n        Args:\n            input_data (Any): Input data for the nodes.\n\n        Returns:\n            list[NodeReadyToRun]: List of nodes ready to run.\n        \"\"\"\n        ready_ts_nodes = self._ts.get_ready()\n        ready_nodes = []\n        for node_id in ready_ts_nodes:\n            node = self._node_by_id[node_id]\n            depends_result = {}\n            is_ready = True\n            for dep in node.depends:\n                if (\n                    dep_result := self._results.get(dep.node.id)\n                ) and dep_result.status != RunnableStatus.UNDEFINED:\n                    depends_result[dep.node.id] = dep_result\n                else:\n                    is_ready = False\n\n            ready_node = NodeReadyToRun(\n                node=node,\n                is_ready=is_ready,\n                input_data=input_data,\n                depends_result=depends_result,\n            )\n            ready_nodes.append(ready_node)\n\n        return ready_nodes\n\n    def _get_output(self) -&gt; dict[str, dict]:\n        \"\"\"\n        Gets the output of the flow.\n\n        Returns:\n            dict[str, dict]: Output of the flow.\n        \"\"\"\n        return {\n            node_id: result.to_dict(skip_format_types={BytesIO, bytes}) for node_id, result in self._results.items()\n        }\n\n    @staticmethod\n    def init_node_topological_sorter(nodes: list[Node]):\n        \"\"\"\n        Initializes a topological sorter for the given nodes.\n\n        Args:\n            nodes (list[Node]): List of nodes to sort.\n\n        Returns:\n            TopologicalSorter: Initialized topological sorter.\n\n        Raises:\n            CycleError: If a cycle is detected in node dependencies.\n        \"\"\"\n        topological_sorter = TopologicalSorter()\n        for node in nodes:\n            topological_sorter.add(node.id, *[d.node.id for d in node.depends])\n\n        try:\n            topological_sorter.prepare()\n        except CycleError as e:\n            logger.error(f\"Node dependencies cycle detected. Error: {e}\")\n            raise\n\n        return topological_sorter\n\n    def reset_run_state(self):\n        \"\"\"Resets the run state of the flow.\"\"\"\n        self._results = {\n            node.id: RunnableResult(status=RunnableStatus.UNDEFINED)\n            for node in self.nodes\n        }\n        self._ts = self.init_node_topological_sorter(nodes=self.nodes)\n\n    def run(self, input_data: Any, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Runs the flow with the given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the flow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the flow execution.\n        \"\"\"\n        self.reset_run_state()\n        run_id = uuid4()\n        merged_kwargs = kwargs | {\n            \"run_id\": run_id,\n            \"parent_run_id\": kwargs.get(\"parent_run_id\", run_id),\n        }\n\n        logger.info(f\"Flow {self.id}: execution started.\")\n        self.run_on_flow_start(input_data, config, **merged_kwargs)\n        time_start = datetime.now()\n\n        try:\n            if self.nodes:\n                max_workers = (\n                    config.max_node_workers if config else self.max_node_workers\n                )\n                run_executor = self.executor(max_workers=max_workers)\n\n                while self._ts.is_active():\n                    ready_nodes = self._get_nodes_ready_to_run(input_data=input_data)\n                    results = run_executor.execute(\n                        ready_nodes=ready_nodes,\n                        config=config,\n                        **(merged_kwargs | {\"parent_run_id\": run_id}),\n                    )\n                    self._results.update(results)\n                    self._ts.done(*results.keys())\n\n                run_executor.shutdown()\n\n            output = self._get_output()\n            self.run_on_flow_end(self._get_output(), config, **merged_kwargs)\n            logger.info(\n                f\"Flow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n            )\n            return RunnableResult(\n                status=RunnableStatus.SUCCESS, input=input_data, output=output\n            )\n        except Exception as e:\n            self.run_on_flow_error(e, config, **merged_kwargs)\n            logger.error(\n                f\"Flow {self.id}: execution failed in \"\n                f\"{format_duration(time_start, datetime.now())}.\"\n            )\n            return RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=input_data,\n            )\n\n    def get_dependant_nodes(\n        self, nodes_types_to_skip: set[str] | None = None\n    ) -&gt; list[Node]:\n        \"\"\"\n        Gets the list of dependent nodes in the flow.\n\n        Args:\n            nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n        Returns:\n            list[Node]: List of dependent nodes.\n        \"\"\"\n        if not nodes_types_to_skip:\n            nodes_types_to_skip = set()\n\n        return [\n            dep.node\n            for node in self.nodes\n            if node.type not in nodes_types_to_skip\n            for dep in node.depends\n        ]\n\n    def get_non_dependant_nodes(\n        self, nodes_types_to_skip: set[str] | None = None\n    ) -&gt; list[Node]:\n        \"\"\"\n        Gets the list of non-dependent nodes in the flow.\n\n        Args:\n            nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n        Returns:\n            list[Node]: List of non-dependent nodes.\n        \"\"\"\n        if not nodes_types_to_skip:\n            nodes_types_to_skip = set()\n\n        dependant_nodes = self.get_dependant_nodes(\n            nodes_types_to_skip=nodes_types_to_skip\n        )\n        return [\n            node\n            for node in self.nodes\n            if node.type not in nodes_types_to_skip and node not in dependant_nodes\n        ]\n\n    def add_nodes(self, nodes: Node | list[Node]):\n        \"\"\"\n        Add one or more nodes to the flow.\n\n        Args:\n            nodes (Node or list[Node]): Node(s) to add to the flow.\n\n        Raises:\n            TypeError: If 'nodes' is not a Node or a list of Node.\n            ValueError: If 'nodes' is an empty list, if a node with the same id already exists in the flow,\n                        or if there are duplicate node ids in the input list.\n        \"\"\"\n\n        if nodes is None:\n            raise ValueError(\"No node provided. Nodes cannot be None.\")\n\n        # Convert a single Node to a list for consistent handling\n        if isinstance(nodes, Node):\n            nodes = [nodes]\n\n        # Check if it's a valid list of nodes\n        if not isinstance(nodes, list) or not all(isinstance(n, Node) for n in nodes):\n            raise TypeError(\"Nodes must be a Node instance or a list of Node instances.\")\n\n        if not nodes:\n            raise ValueError(\"Cannot add an empty list of nodes to the flow.\")\n\n        # Add nodes to the flow, checking for duplicates in the flow\n        for node in nodes:\n            if node.id in self._node_by_id:\n                raise ValueError(f\"Node with id {node.id} already exists in the flow.\")\n\n            self.nodes.append(node)\n            self._node_by_id[node.id] = node\n            if node.is_postponed_component_init:\n                node.init_components(self.connection_manager)\n        self.reset_run_state()\n\n        return self  # enable chaining\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the Flow instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the Flow instance.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._node_by_id = {node.id: node for node in self.nodes}\n    self._ts = None\n\n    self._init_components()\n    self.reset_run_state()\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Add one or more nodes to the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Node or list[Node]</code> <p>Node(s) to add to the flow.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'nodes' is not a Node or a list of Node.</p> <code>ValueError</code> <p>If 'nodes' is an empty list, if a node with the same id already exists in the flow,         or if there are duplicate node ids in the input list.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def add_nodes(self, nodes: Node | list[Node]):\n    \"\"\"\n    Add one or more nodes to the flow.\n\n    Args:\n        nodes (Node or list[Node]): Node(s) to add to the flow.\n\n    Raises:\n        TypeError: If 'nodes' is not a Node or a list of Node.\n        ValueError: If 'nodes' is an empty list, if a node with the same id already exists in the flow,\n                    or if there are duplicate node ids in the input list.\n    \"\"\"\n\n    if nodes is None:\n        raise ValueError(\"No node provided. Nodes cannot be None.\")\n\n    # Convert a single Node to a list for consistent handling\n    if isinstance(nodes, Node):\n        nodes = [nodes]\n\n    # Check if it's a valid list of nodes\n    if not isinstance(nodes, list) or not all(isinstance(n, Node) for n in nodes):\n        raise TypeError(\"Nodes must be a Node instance or a list of Node instances.\")\n\n    if not nodes:\n        raise ValueError(\"Cannot add an empty list of nodes to the flow.\")\n\n    # Add nodes to the flow, checking for duplicates in the flow\n    for node in nodes:\n        if node.id in self._node_by_id:\n            raise ValueError(f\"Node with id {node.id} already exists in the flow.\")\n\n        self.nodes.append(node)\n        self._node_by_id[node.id] = node\n        if node.is_postponed_component_init:\n            node.init_components(self.connection_manager)\n    self.reset_run_state()\n\n    return self  # enable chaining\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.get_dependant_nodes","title":"<code>get_dependant_nodes(nodes_types_to_skip=None)</code>","text":"<p>Gets the list of dependent nodes in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_types_to_skip</code> <code>set[NodeType] | None</code> <p>Set of node types to skip. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Node]</code> <p>list[Node]: List of dependent nodes.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def get_dependant_nodes(\n    self, nodes_types_to_skip: set[str] | None = None\n) -&gt; list[Node]:\n    \"\"\"\n    Gets the list of dependent nodes in the flow.\n\n    Args:\n        nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n    Returns:\n        list[Node]: List of dependent nodes.\n    \"\"\"\n    if not nodes_types_to_skip:\n        nodes_types_to_skip = set()\n\n    return [\n        dep.node\n        for node in self.nodes\n        if node.type not in nodes_types_to_skip\n        for dep in node.depends\n    ]\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.get_non_dependant_nodes","title":"<code>get_non_dependant_nodes(nodes_types_to_skip=None)</code>","text":"<p>Gets the list of non-dependent nodes in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_types_to_skip</code> <code>set[NodeType] | None</code> <p>Set of node types to skip. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Node]</code> <p>list[Node]: List of non-dependent nodes.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def get_non_dependant_nodes(\n    self, nodes_types_to_skip: set[str] | None = None\n) -&gt; list[Node]:\n    \"\"\"\n    Gets the list of non-dependent nodes in the flow.\n\n    Args:\n        nodes_types_to_skip (set[NodeType] | None, optional): Set of node types to skip. Defaults to None.\n\n    Returns:\n        list[Node]: List of non-dependent nodes.\n    \"\"\"\n    if not nodes_types_to_skip:\n        nodes_types_to_skip = set()\n\n    dependant_nodes = self.get_dependant_nodes(\n        nodes_types_to_skip=nodes_types_to_skip\n    )\n    return [\n        node\n        for node in self.nodes\n        if node.type not in nodes_types_to_skip and node not in dependant_nodes\n    ]\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.init_node_topological_sorter","title":"<code>init_node_topological_sorter(nodes)</code>  <code>staticmethod</code>","text":"<p>Initializes a topological sorter for the given nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[Node]</code> <p>List of nodes to sort.</p> required <p>Returns:</p> Name Type Description <code>TopologicalSorter</code> <p>Initialized topological sorter.</p> <p>Raises:</p> Type Description <code>CycleError</code> <p>If a cycle is detected in node dependencies.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>@staticmethod\ndef init_node_topological_sorter(nodes: list[Node]):\n    \"\"\"\n    Initializes a topological sorter for the given nodes.\n\n    Args:\n        nodes (list[Node]): List of nodes to sort.\n\n    Returns:\n        TopologicalSorter: Initialized topological sorter.\n\n    Raises:\n        CycleError: If a cycle is detected in node dependencies.\n    \"\"\"\n    topological_sorter = TopologicalSorter()\n    for node in nodes:\n        topological_sorter.add(node.id, *[d.node.id for d in node.depends])\n\n    try:\n        topological_sorter.prepare()\n    except CycleError as e:\n        logger.error(f\"Node dependencies cycle detected. Error: {e}\")\n        raise\n\n    return topological_sorter\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Resets the run state of the flow.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Resets the run state of the flow.\"\"\"\n    self._results = {\n        node.id: RunnableResult(status=RunnableStatus.UNDEFINED)\n        for node in self.nodes\n    }\n    self._ts = self.init_node_topological_sorter(nodes=self.nodes)\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.run","title":"<code>run(input_data, config=None, **kwargs)</code>","text":"<p>Runs the flow with the given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the flow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <p>Result of the flow execution.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>def run(self, input_data: Any, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Runs the flow with the given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the flow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the flow execution.\n    \"\"\"\n    self.reset_run_state()\n    run_id = uuid4()\n    merged_kwargs = kwargs | {\n        \"run_id\": run_id,\n        \"parent_run_id\": kwargs.get(\"parent_run_id\", run_id),\n    }\n\n    logger.info(f\"Flow {self.id}: execution started.\")\n    self.run_on_flow_start(input_data, config, **merged_kwargs)\n    time_start = datetime.now()\n\n    try:\n        if self.nodes:\n            max_workers = (\n                config.max_node_workers if config else self.max_node_workers\n            )\n            run_executor = self.executor(max_workers=max_workers)\n\n            while self._ts.is_active():\n                ready_nodes = self._get_nodes_ready_to_run(input_data=input_data)\n                results = run_executor.execute(\n                    ready_nodes=ready_nodes,\n                    config=config,\n                    **(merged_kwargs | {\"parent_run_id\": run_id}),\n                )\n                self._results.update(results)\n                self._ts.done(*results.keys())\n\n            run_executor.shutdown()\n\n        output = self._get_output()\n        self.run_on_flow_end(self._get_output(), config, **merged_kwargs)\n        logger.info(\n            f\"Flow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n        )\n        return RunnableResult(\n            status=RunnableStatus.SUCCESS, input=input_data, output=output\n        )\n    except Exception as e:\n        self.run_on_flow_error(e, config, **merged_kwargs)\n        logger.error(\n            f\"Flow {self.id}: execution failed in \"\n            f\"{format_duration(time_start, datetime.now())}.\"\n        )\n        return RunnableResult(\n            status=RunnableStatus.FAILURE,\n            input=input_data,\n        )\n</code></pre>"},{"location":"dynamiq/flows/flow/#dynamiq.flows.flow.Flow.validate_nodes","title":"<code>validate_nodes(nodes)</code>  <code>classmethod</code>","text":"<p>Validates the list of nodes in the flow.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[Node]</code> <p>List of nodes to validate.</p> required <p>Returns:</p> Type Description <code>list[Node]</code> <p>list[Node]: Validated list of nodes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are duplicate node IDs or invalid dependencies.</p> Source code in <code>dynamiq/flows/flow.py</code> <pre><code>@field_validator(\"nodes\")\n@classmethod\ndef validate_nodes(cls, nodes: list[Node]) -&gt; list[Node]:\n    \"\"\"\n    Validates the list of nodes in the flow.\n\n    Args:\n        nodes (list[Node]): List of nodes to validate.\n\n    Returns:\n        list[Node]: Validated list of nodes.\n\n    Raises:\n        ValueError: If there are duplicate node IDs or invalid dependencies.\n    \"\"\"\n    nodes_ids_unique = set()\n    nodes_deps_ids_unique = set()\n    for node in nodes:\n        if node.id in nodes_ids_unique:\n            raise ValueError(\n                f\"Flow has nodes with duplicated ids: '{node.id}'. Node ids must be unique.\"\n            )\n\n        nodes_ids_unique.add(node.id)\n        node_deps_ids = [dep.node.id for dep in node.depends]\n        if len(set(node_deps_ids)) != len(node_deps_ids):\n            raise ValueError(\n                f\"Flow node '{node.id}' has duplicated dependency ids. Node dependencies ids must be unique.\"\n            )\n\n        nodes_deps_ids_unique.update(node_deps_ids)\n\n    if not nodes_deps_ids_unique.issubset(nodes_ids_unique):\n        raise ValueError(\n            \"Flow nodes have dependencies that are not present in the flow.\"\n        )\n\n    return nodes\n</code></pre>"},{"location":"dynamiq/loaders/yaml/","title":"Yaml","text":""},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader","title":"<code>WorkflowYAMLLoader</code>","text":"<p>Loader class for parsing YAML files and creating workflow components.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>class WorkflowYAMLLoader:\n    \"\"\"Loader class for parsing YAML files and creating workflow components.\"\"\"\n\n    @classmethod\n    def get_entity_by_type(\n        cls, entity_type: str, entity_registry: dict[str, Any] | None = None\n    ) -&gt; Any:\n        \"\"\"\n        Try to get entity by type and update mutable shared registry.\n\n        Args:\n            entity_type (str): The type of entity to retrieve.\n            entity_registry (dict[str, Any] | None): A registry of entities.\n\n        Returns:\n            Any: The retrieved entity.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the entity is not valid or cannot be found.\n        \"\"\"\n        if entity_registry is None:\n            entity_registry = {}\n\n        if entity := entity_registry.get(entity_type):\n            return entity\n\n        try:\n            entity = ConnectionManager.get_connection_by_type(entity_type)\n        except ValueError:\n            pass\n\n        if not entity:\n            try:\n                entity = NodeManager.get_node_by_type(entity_type)\n            except ValueError:\n                pass\n\n        if not entity:\n            raise WorkflowYAMLLoaderException(f\"Entity '{entity_type}' is not valid.\")\n\n        entity_registry[entity_type] = entity\n        return entity\n\n    @classmethod\n    def get_connections(\n        cls, data: dict[str, dict], registry: dict[str, Any]\n    ) -&gt; dict[str, BaseConnection]:\n        \"\"\"\n        Get connections from the provided data.\n\n        Args:\n            data (dict[str, dict]): The data containing connection information.\n            registry (dict[str, Any]): A registry of entities.\n\n        Returns:\n            dict[str, BaseConnection]: A dictionary of connections.\n\n        Raises:\n            WorkflowYAMLLoaderException: If there's an error in connection data or initialization.\n        \"\"\"\n        connections = {}\n        for conn_id, conn_data in data.get(\"connections\", {}).items():\n            if conn_id in connections:\n                raise WorkflowYAMLLoaderException(\n                    f\"Connection '{conn_id}' already exists\"\n                )\n            if not (conn_type := conn_data.get(\"type\")):\n                raise WorkflowYAMLLoaderException(\n                    f\"Value 'type' not found for connection '{conn_id}'\"\n                )\n\n            conn_cls = cls.get_entity_by_type(\n                entity_type=conn_type, entity_registry=registry\n            )\n            conn_init_data = conn_data | {\"id\": conn_id}\n            conn_init_data.pop(\"type\", None)\n            try:\n                connection = conn_cls(**conn_init_data)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(\n                    f\"Connection '{conn_id}' data is invalid. Data: '{conn_data}'. Error: {e}\"\n                )\n\n            connections[conn_id] = connection\n\n        return connections\n\n    @classmethod\n    def init_prompt(cls, prompt_init_data: dict) -&gt; Prompt:\n        \"\"\"\n        Initialize a prompt from the provided data.\n\n        Args:\n            prompt_init_data (dict): The data for the prompt.\n\n        Returns:\n            Prompt: The initialized prompt.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified prompt is not found.\n        \"\"\"\n        try:\n            return Prompt(**prompt_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(f\"Prompt data is invalid. Data: {prompt_init_data}. \" f\"Error: {e}\")\n\n    @classmethod\n    def get_prompts(cls, data: dict[str, dict]) -&gt; dict[str, Prompt]:\n        \"\"\"\n        Get prompts from the provided data.\n\n        Args:\n            data (dict[str, dict]): The data containing prompt information.\n\n        Returns:\n            dict[str, Prompt]: A dictionary of prompts.\n\n        Raises:\n            WorkflowYAMLLoaderException: If there's an error in prompt data or initialization.\n        \"\"\"\n        prompts = {}\n        for prompt_id, prompt_data in data.get(\"prompts\", {}).items():\n            if prompt_id in prompts:\n                raise WorkflowYAMLLoaderException(\n                    f\"Prompt '{prompt_id}' already exists\"\n                )\n            prompts[prompt_id] = cls.init_prompt(prompt_data | {\"id\": prompt_id})\n\n        return prompts\n\n    @classmethod\n    def get_node_prompt(\n        cls, node_id: str, node_data: dict, prompts: dict[id, Prompt]\n    ) -&gt; Prompt | None:\n        \"\"\"\n        Get the prompt for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            prompts (dict[id, Prompt]): A dictionary of available prompts.\n\n        Returns:\n            Prompt | None: The prompt for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified prompt is not found.\n        \"\"\"\n        prompt = None\n        if prompt_id := node_data.get(\"prompt\"):\n            prompt = prompts.get(prompt_id)\n            if not prompt:\n                raise WorkflowYAMLLoaderException(\n                    f\"Prompt '{prompt_id}' for node '{node_id}' not found\"\n                )\n        return prompt\n\n    @classmethod\n    def get_node_connection(\n        cls, node_id: str, node_data: dict, connections: dict[id, BaseConnection]\n    ) -&gt; BaseConnection | None:\n        \"\"\"\n        Get the connection for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            connections (dict[id, BaseConnection]): A dictionary of available connections.\n\n        Returns:\n            BaseConnection | None: The connection for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified connection is not found.\n        \"\"\"\n        conn = None\n        if conn_id := node_data.get(\"connection\"):\n            conn = connections.get(conn_id)\n            if not conn:\n                raise WorkflowYAMLLoaderException(\n                    f\"Connection '{conn_id}' for node '{node_id}' not found\"\n                )\n        return conn\n\n    @classmethod\n    def get_node_vector_store_connection(\n        cls, node_id: str, node_data: dict, connections: dict[id, BaseConnection]\n    ) -&gt; Any | None:\n        \"\"\"\n        Get the vector store connection for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            connections (dict[id, BaseConnection]): A dictionary of available connections.\n\n        Returns:\n            Any | None: The vector store connection for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified vector store connection is not found or\n                                         does not support vector store initialization.\n        \"\"\"\n        if conn := cls.get_node_connection(\n            node_id=node_id, node_data=node_data, connections=connections\n        ):\n            if not (\n                conn_to_vs := getattr(conn, \"connect_to_vector_store\", None)\n            ) or not callable(conn_to_vs):\n                raise WorkflowYAMLLoaderException(\n                    f\"Vector store connection '{conn.id}' for node '{node_id}' not support vector store initialization\"\n                )\n        return conn\n\n    @classmethod\n    def get_node_flow(\n        cls, node_id: str, node_data: dict, flows: dict[id, Flow]\n    ) -&gt; Flow | None:\n        \"\"\"\n        Get the flow for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            flows (dict[id, Flow]): A dictionary of available flows.\n\n        Returns:\n            Flow | None: The flow for the node, or None if not found.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the specified flow is not found.\n        \"\"\"\n        flow = None\n        if flow_id := node_data.get(\"flow\"):\n            flow = flows.get(flow_id)\n            if not flow:\n                raise WorkflowYAMLLoaderException(\n                    f\"Flow '{flow_id}' for node '{node_id}' not found\"\n                )\n        return flow\n\n    @classmethod\n    def get_node_flows(\n        cls, node_id: str, node_data: dict, flows: dict[id, Flow]\n    ) -&gt; list[Flow]:\n        \"\"\"\n        Get the flows for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            flows (dict[id, Flow]): A dictionary of available flows.\n\n        Returns:\n            list[Flow]: A list of flows for the node.\n\n        Raises:\n            WorkflowYAMLLoaderException: If any specified flow is not found.\n        \"\"\"\n        node_flows = []\n        for flow_id in node_data.get(\"flows\", []):\n            node_flow = flows.get(flow_id)\n            if not node_flow:\n                raise WorkflowYAMLLoaderException(\n                    f\"Flow '{flow_id}' for node '{node_id}' not found\"\n                )\n            node_flows.append(node_flow)\n        return node_flows\n\n    @classmethod\n    def get_node_dependencies(\n        cls, node_id: str, node_data: dict, nodes: dict[str, Node]\n    ):\n        \"\"\"\n        Get the dependencies for a node.\n\n        Args:\n            node_id (str): The ID of the node.\n            node_data (dict): The data for the node.\n            nodes (dict[str, Node]): A dictionary of available nodes.\n\n        Returns:\n            list[NodeDependency]: A list of node dependencies.\n\n        Raises:\n            WorkflowYAMLLoaderException: If there's an error in dependency data or initialization.\n        \"\"\"\n        node_depends = []\n        for dependency_data in node_data.get(\"depends\", []):\n            dependency_node = nodes.get(dependency_data.get(\"node\"))\n            dependency_init_data = dependency_data | {\"node\": dependency_node}\n            try:\n                dependency = NodeDependency(**dependency_init_data)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency data for node '{node_id}' is invalid. Data: {dependency_data}. Error: {e}\"\n                )\n\n            if dependency.option:\n                if not (dep_options := getattr(dependency_node, \"options\", [])):\n                    raise WorkflowYAMLLoaderException(\n                        f\"Dependency '{dependency.node}' with option '{dependency.option}' \"\n                        f\"for node '{node_id}' not found\"\n                    )\n\n                if not any(opt.id == dependency.option for opt in dep_options):\n                    raise WorkflowYAMLLoaderException(\n                        f\"Dependency '{dependency.node}' with option '{dependency.option}' \"\n                        f\"for node '{node_id}' not found\"\n                    )\n\n            node_depends.append(dependency)\n        return node_depends\n\n    @classmethod\n    def get_updated_node_init_data_with_initialized_nodes(\n        cls,\n        node_init_data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ):\n        \"\"\"\n        Get node init data with initialized nodes components (llms, agents, etc)\n\n        Args:\n            node_init_data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            A dictionary of newly created nodes with dependencies.\n        \"\"\"\n        updated_node_init_data = copy.deepcopy(node_init_data)\n        for param_name, param_data in node_init_data.items():\n            # TODO: dummy fix, revisit this!\n            # We had to add this condition because both input and output nodes have a `schema` param,\n            # which has a `type` field that contains types supported by JSON schema (e.g., string, object).\n            if param_name == \"schema\":\n                continue\n\n            if isinstance(param_data, dict) and param_data.get(\"type\"):\n                param_id = param_data.get(\"id\")\n                updated_node_init_data[param_name] = cls.get_nodes_without_depends(\n                    data={param_id: param_data},\n                    nodes=nodes,\n                    flows=flows,\n                    connections=connections,\n                    prompts=prompts,\n                    registry=registry,\n                    connection_manager=connection_manager,\n                    init_components=init_components,\n                )[param_id]\n            if isinstance(param_data, list):\n                updated_items = []\n                for item in param_data:\n                    if isinstance(item, dict) and (item_id := item.get(\"id\")):\n                        updated_items.append(\n                            cls.get_updated_node_init_data_with_initialized_nodes(\n                                node_init_data={item_id: item},\n                                nodes=nodes,\n                                flows=flows,\n                                connections=connections,\n                                prompts=prompts,\n                                registry=registry,\n                                connection_manager=connection_manager,\n                                init_components=init_components,\n                            )[item_id]\n                        )\n                    else:\n                        updated_items.append(item)\n                updated_node_init_data[param_name] = updated_items\n\n        return updated_node_init_data\n\n    @classmethod\n    def get_nodes_without_depends(\n        cls,\n        data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ) -&gt; dict[str, Node]:\n        \"\"\"\n        Create nodes without dependencies from the given data.\n\n        Args:\n            data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            A dictionary of newly created nodes without dependencies.\n\n        Raises:\n            WorkflowYAMLLoaderException: If node data is invalid or duplicates are found.\n        \"\"\"\n        new_nodes = {}\n        for node_id, node_data in data.items():\n            if node_id in nodes:\n                continue\n\n            if node_id in new_nodes:\n                raise WorkflowYAMLLoaderException(f\"Node '{node_id}' already exists\")\n\n            if not (node_type := node_data.get(\"type\")):\n                raise WorkflowYAMLLoaderException(\n                    f\"Value 'type' for node '{node_id}' not found\"\n                )\n\n            node_cls = cls.get_entity_by_type(\n                entity_type=node_type, entity_registry=registry\n            )\n\n            # Init node params\n            node_init_data = copy.deepcopy(node_data) | {\n                \"id\": node_id,\n                \"is_postponed_component_init\": node_data.get(\n                    \"is_postponed_component_init\", True\n                ),\n            }\n            node_init_data.pop(\"type\", None)\n            node_init_data.pop(\"depends\", None)\n\n            if \"connection\" in node_init_data:\n                get_node_conn = (\n                    cls.get_node_vector_store_connection\n                    if isinstance(node_cls, ConnectionNode)\n                    else cls.get_node_connection\n                )\n                node_init_data[\"connection\"] = get_node_conn(\n                    node_id=node_id, node_data=node_data, connections=connections\n                )\n            if prompt_data := node_init_data.get(\"prompt\"):\n                node_init_data[\"prompt\"] = (\n                    cls.get_node_prompt(node_id=node_id, node_data=node_data, prompts=prompts)\n                    if isinstance(prompt_data, str)\n                    else cls.init_prompt(prompt_data)\n                )\n            if \"flow\" in node_init_data:\n                node_init_data[\"flow\"] = cls.get_node_flow(\n                    node_id=node_id, node_data=node_data, flows=flows\n                )\n            if \"flows\" in node_init_data:\n                node_init_data[\"flows\"] = cls.get_node_flows(\n                    node_id=node_id, node_data=node_data, flows=flows\n                )\n            try:\n                node_init_data = cls.get_updated_node_init_data_with_initialized_nodes(\n                    node_init_data=node_init_data,\n                    nodes=nodes,\n                    flows=flows,\n                    connections=connections,\n                    prompts=prompts,\n                    registry=registry,\n                    connection_manager=connection_manager,\n                    init_components=init_components,\n                )\n                node = node_cls(**node_init_data)\n                if init_components:\n                    node.init_components(connection_manager=connection_manager)\n                    node.is_postponed_component_init = False\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(\n                    f\"Node '{node_id}' data is invalid. Data: {node_data}. Error: {e}\"\n                )\n\n            new_nodes[node_id] = node\n        return new_nodes\n\n    @classmethod\n    def get_nodes(\n        cls,\n        nodes_data: dict,\n        nodes: dict[str, Node],\n        flows: dict[str, Flow],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ):\n        \"\"\"\n        Create nodes with dependencies from the given data.\n\n        Args:\n            nodes_data: Dictionary containing node data.\n            nodes: Existing nodes dictionary.\n            flows: Existing flows dictionary.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            A dictionary of newly created nodes with dependencies.\n        \"\"\"\n        new_nodes = cls.get_nodes_without_depends(\n            data=nodes_data,\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n        all_nodes = nodes | new_nodes\n        for node_id, node in new_nodes.items():\n            node.depends = cls.get_node_dependencies(\n                node_id=node_id, node_data=nodes_data[node_id], nodes=all_nodes\n            )\n\n        return new_nodes\n\n    @classmethod\n    def get_dependant_nodes(\n        cls,\n        nodes_data: dict[str, dict],\n        flows_data: dict[str, dict],\n        connections: dict[str, BaseConnection],\n        prompts: dict[str, Prompt],\n        registry: dict[str, Any],\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ) -&gt; dict[str, Node]:\n        \"\"\"\n        Get nodes that are dependent on flows.\n\n        Args:\n            nodes_data: Dictionary containing node data.\n            flows_data: Dictionary containing flow data.\n            connections: Existing connections dictionary.\n            prompts: Existing prompts dictionary.\n            registry: Registry of node types.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            A dictionary of nodes that are dependent on flows.\n        \"\"\"\n        dependant_nodes, dependant_nodes_data = {}, {}\n        dependant_flow_ids = []\n\n        for node_id, node_data in nodes_data.items():\n            if \"flow\" in node_data:\n                dependant_nodes_data[node_id] = node_data\n                dependant_flow_ids.append(node_data[\"flow\"])\n            if \"flows\" in node_data:\n                dependant_nodes_data[node_id] = node_data\n                dependant_flow_ids.extend(node_data[\"flows\"])\n\n        # Get nodes from dependant flows\n        if dependant_flow_ids:\n            dependant_flows_nodes_ids = []\n            for flow_id, flow_data in flows_data.items():\n                if flow_id in dependant_flow_ids:\n                    dependant_flows_nodes_ids.extend(flow_data.get(\"nodes\", []))\n\n            dependant_flows_nodes_data = {\n                node_id: node_data\n                for node_id, node_data in nodes_data.items()\n                if node_id in dependant_flows_nodes_ids\n            }\n            dependant_nodes = cls.get_nodes(\n                nodes_data=dependant_flows_nodes_data,\n                nodes={},\n                flows={},\n                connections=connections,\n                prompts=prompts,\n                registry=registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n            )\n\n        return dependant_nodes\n\n    @classmethod\n    def get_flows(\n        cls,\n        data: dict,\n        flows: dict[str, Flow],\n        nodes: dict[str, Node],\n        connection_manager: ConnectionManager | None = None,\n    ) -&gt; dict[str, Flow]:\n        \"\"\"\n        Create flows from the given data.\n\n        Args:\n            data: Dictionary containing flow data.\n            flows: Existing flows dictionary.\n            nodes: Existing nodes dictionary.\n            connection_manager: Optional connection manager.\n\n        Returns:\n            A dictionary of newly created flows.\n\n        Raises:\n            WorkflowYAMLLoaderException: If flow data is invalid or duplicates are found.\n        \"\"\"\n        new_flows = {}\n        for flow_id, flow_data in data.items():\n            if flow_id in flows:\n                continue\n\n            if flow_id in new_flows:\n                raise WorkflowYAMLLoaderException(f\"Flow {flow_id} already exists\")\n\n            flow_node_ids = flow_data.get(\"nodes\", [])\n            flow_node_ids = set(flow_node_ids)\n            dep_node_ids = set()\n            for node_id in flow_node_ids:\n                if node_id not in nodes:\n                    raise WorkflowYAMLLoaderException(\n                        f\"Node '{node_id}' for flow '{flow_id}' not found\"\n                    )\n\n                dep_node_ids.update({dep.node.id for dep in nodes[node_id].depends})\n\n            for node_id in dep_node_ids:\n                if node_id not in flow_node_ids:\n                    raise WorkflowYAMLLoaderException(\n                        f\"Dependency node '{node_id}' in the flow '{flow_id}' node list not found\"\n                    )\n\n            flow_init_data = flow_data | {\n                \"id\": flow_id,\n                \"nodes\": [nodes[node_id] for node_id in flow_node_ids],\n            }\n            if connection_manager:\n                flow_init_data[\"connection_manager\"] = connection_manager\n\n            try:\n                flow = Flow(**flow_init_data)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(\n                    f\"Flow '{flow_id}' data is invalid. Data: {flow_data}. Error: {e}\"\n                )\n\n            new_flows[flow_id] = flow\n        return new_flows\n\n    @classmethod\n    def get_dependant_flows(\n        cls,\n        nodes_data: dict[str, dict],\n        flows_data: dict[str, dict],\n        dependant_nodes: dict[str, Node],\n        connection_manager: ConnectionManager | None = None,\n    ) -&gt; dict[str, Flow]:\n        \"\"\"\n        Get flows that are dependent on nodes.\n\n        Args:\n            nodes_data: Dictionary containing node data.\n            flows_data: Dictionary containing flow data.\n            dependant_nodes: Dictionary of dependent nodes.\n            connection_manager: Optional connection manager.\n\n        Returns:\n            A dictionary of flows that are dependent on nodes.\n        \"\"\"\n        dependant_flows = {}\n        dependant_flow_ids = []\n\n        for node_id, node_data in nodes_data.items():\n            if \"flow\" in node_data:\n                dependant_flow_ids.append(node_data[\"flow\"])\n            if \"flows\" in node_data:\n                dependant_flow_ids.extend(node_data[\"flows\"])\n\n        if dependant_flow_ids:\n            dependant_flows_data = {\n                flow_id: flow_data\n                for flow_id, flow_data in flows_data.items()\n                if flow_id in dependant_flow_ids\n            }\n            dependant_flows = cls.get_flows(\n                data=dependant_flows_data,\n                flows={},\n                nodes=dependant_nodes,\n                connection_manager=connection_manager,\n            )\n\n        return dependant_flows\n\n    @classmethod\n    def get_workflows(cls, data: dict, flows: dict[str, Flow]) -&gt; dict[str, Workflow]:\n        \"\"\"\n        Create workflows from the given data.\n\n        Args:\n            data: Dictionary containing workflow data.\n            flows: Existing flows dictionary.\n\n        Returns:\n            A dictionary of newly created workflows.\n\n        Raises:\n            WorkflowYAMLLoaderException: If workflow data is invalid.\n        \"\"\"\n        workflows = {}\n        for wf_id, wf_data in data.get(\"workflows\", {}).items():\n            if not (flow_id := wf_data.get(\"flow\")):\n                raise WorkflowYAMLLoaderException(\n                    f\"Value 'flow' for dynamiq '{wf_id}' not found \"\n                )\n            if not (flow := flows.get(flow_id)):\n                raise WorkflowYAMLLoaderException(\n                    f\"Flow '{flow_id}' for dynamiq '{wf_id}' not found\"\n                )\n            if version := wf_data.get(\"version\"):\n                version = str(version)\n\n            try:\n                wf = Workflow(id=wf_id, flow=flow, version=version)\n            except Exception as e:\n                raise WorkflowYAMLLoaderException(\n                    f\"Workflow '{wf_id}' data is invalid. Data: {wf_data}. Error: {e}\"\n                )\n\n            workflows[wf_id] = wf\n        return workflows\n\n    @classmethod\n    def load(\n        cls,\n        file_path: str | PathLike,\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ) -&gt; WorkflowYamlLoaderData:\n        \"\"\"\n        Load data from a YAML file and parse it.\n\n        Args:\n            file_path: Path to the YAML file.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            Parsed WorkflowYamlLoaderData object.\n        \"\"\"\n        data = cls.loads(file_path)\n        return cls.parse(\n            data=data,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n\n    @classmethod\n    def loads(cls, file_path: str | PathLike):\n        \"\"\"\n        Load data from a YAML file.\n\n        Args:\n            file_path: Path to the YAML file.\n\n        Returns:\n            Parsed data from the YAML file.\n\n        Raises:\n            WorkflowYAMLLoaderException: If the file is not found.\n        \"\"\"\n        from omegaconf import OmegaConf\n\n        try:\n            conf = OmegaConf.load(file_path)\n            logger.debug(f\"Loaded config from '{file_path}'\")\n\n            data = OmegaConf.to_container(conf, resolve=True)\n        except FileNotFoundError:\n            raise WorkflowYAMLLoaderException(f\"File '{file_path}' not found\")\n\n        return data\n\n    @classmethod\n    def parse(\n        cls,\n        data: dict,\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ) -&gt; WorkflowYamlLoaderData:\n        \"\"\"\n        Parse dynamiq workflow data.\n\n        Args:\n            data: Dictionary containing workflow data.\n            connection_manager: Optional connection manager.\n            init_components: Flag to initialize components.\n\n        Returns:\n            Parsed WorkflowYamlLoaderData object.\n\n        Raises:\n            WorkflowYAMLLoaderException: If parsing fails.\n        \"\"\"\n        nodes, flows = {}, {}\n        # Mutable shared registry that updates with each new entity.\n        node_registry, connection_registry = {}, {}\n        if init_components and connection_manager is None:\n            connection_manager = ConnectionManager()\n\n        try:\n            connections = cls.get_connections(data=data, registry=connection_registry)\n            prompts = cls.get_prompts(data)\n\n            nodes_data = data.get(\"nodes\", {})\n            flows_data = data.get(\"flows\", {})\n\n            dependant_nodes = cls.get_dependant_nodes(\n                nodes_data=nodes_data,\n                flows_data=flows_data,\n                connections=connections,\n                prompts=prompts,\n                registry=node_registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n            )\n            nodes.update(dependant_nodes)\n\n            dependant_flows = cls.get_dependant_flows(\n                nodes_data=nodes_data,\n                flows_data=flows_data,\n                dependant_nodes=dependant_nodes,\n                connection_manager=connection_manager,\n            )\n            flows.update(dependant_flows)\n\n            non_dependant_nodes = cls.get_nodes(\n                nodes_data=nodes_data,\n                nodes=nodes,\n                flows=flows,\n                connections=connections,\n                prompts=prompts,\n                registry=node_registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n            )\n            nodes.update(non_dependant_nodes)\n\n            non_dependant_flows = cls.get_flows(\n                data=flows_data,\n                flows=flows,\n                nodes=nodes,\n                connection_manager=connection_manager,\n            )\n            flows.update(non_dependant_flows)\n\n            workflows = cls.get_workflows(data, flows)\n        except WorkflowYAMLLoaderException:\n            raise\n        except Exception:\n            logger.exception(\"Failed to parse Yaml data with unexpected error\")\n            raise\n\n        return WorkflowYamlLoaderData(\n            connections=connections,\n            nodes=nodes,\n            flows=flows,\n            workflows=workflows,\n        )\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_connections","title":"<code>get_connections(data, registry)</code>  <code>classmethod</code>","text":"<p>Get connections from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, dict]</code> <p>The data containing connection information.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>A registry of entities.</p> required <p>Returns:</p> Type Description <code>dict[str, BaseConnection]</code> <p>dict[str, BaseConnection]: A dictionary of connections.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If there's an error in connection data or initialization.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_connections(\n    cls, data: dict[str, dict], registry: dict[str, Any]\n) -&gt; dict[str, BaseConnection]:\n    \"\"\"\n    Get connections from the provided data.\n\n    Args:\n        data (dict[str, dict]): The data containing connection information.\n        registry (dict[str, Any]): A registry of entities.\n\n    Returns:\n        dict[str, BaseConnection]: A dictionary of connections.\n\n    Raises:\n        WorkflowYAMLLoaderException: If there's an error in connection data or initialization.\n    \"\"\"\n    connections = {}\n    for conn_id, conn_data in data.get(\"connections\", {}).items():\n        if conn_id in connections:\n            raise WorkflowYAMLLoaderException(\n                f\"Connection '{conn_id}' already exists\"\n            )\n        if not (conn_type := conn_data.get(\"type\")):\n            raise WorkflowYAMLLoaderException(\n                f\"Value 'type' not found for connection '{conn_id}'\"\n            )\n\n        conn_cls = cls.get_entity_by_type(\n            entity_type=conn_type, entity_registry=registry\n        )\n        conn_init_data = conn_data | {\"id\": conn_id}\n        conn_init_data.pop(\"type\", None)\n        try:\n            connection = conn_cls(**conn_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Connection '{conn_id}' data is invalid. Data: '{conn_data}'. Error: {e}\"\n            )\n\n        connections[conn_id] = connection\n\n    return connections\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_dependant_flows","title":"<code>get_dependant_flows(nodes_data, flows_data, dependant_nodes, connection_manager=None)</code>  <code>classmethod</code>","text":"<p>Get flows that are dependent on nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_data</code> <code>dict[str, dict]</code> <p>Dictionary containing node data.</p> required <code>flows_data</code> <code>dict[str, dict]</code> <p>Dictionary containing flow data.</p> required <code>dependant_nodes</code> <code>dict[str, Node]</code> <p>Dictionary of dependent nodes.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Flow]</code> <p>A dictionary of flows that are dependent on nodes.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_dependant_flows(\n    cls,\n    nodes_data: dict[str, dict],\n    flows_data: dict[str, dict],\n    dependant_nodes: dict[str, Node],\n    connection_manager: ConnectionManager | None = None,\n) -&gt; dict[str, Flow]:\n    \"\"\"\n    Get flows that are dependent on nodes.\n\n    Args:\n        nodes_data: Dictionary containing node data.\n        flows_data: Dictionary containing flow data.\n        dependant_nodes: Dictionary of dependent nodes.\n        connection_manager: Optional connection manager.\n\n    Returns:\n        A dictionary of flows that are dependent on nodes.\n    \"\"\"\n    dependant_flows = {}\n    dependant_flow_ids = []\n\n    for node_id, node_data in nodes_data.items():\n        if \"flow\" in node_data:\n            dependant_flow_ids.append(node_data[\"flow\"])\n        if \"flows\" in node_data:\n            dependant_flow_ids.extend(node_data[\"flows\"])\n\n    if dependant_flow_ids:\n        dependant_flows_data = {\n            flow_id: flow_data\n            for flow_id, flow_data in flows_data.items()\n            if flow_id in dependant_flow_ids\n        }\n        dependant_flows = cls.get_flows(\n            data=dependant_flows_data,\n            flows={},\n            nodes=dependant_nodes,\n            connection_manager=connection_manager,\n        )\n\n    return dependant_flows\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_dependant_nodes","title":"<code>get_dependant_nodes(nodes_data, flows_data, connections, prompts, registry, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Get nodes that are dependent on flows.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_data</code> <code>dict[str, dict]</code> <p>Dictionary containing node data.</p> required <code>flows_data</code> <code>dict[str, dict]</code> <p>Dictionary containing flow data.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Node]</code> <p>A dictionary of nodes that are dependent on flows.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_dependant_nodes(\n    cls,\n    nodes_data: dict[str, dict],\n    flows_data: dict[str, dict],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n) -&gt; dict[str, Node]:\n    \"\"\"\n    Get nodes that are dependent on flows.\n\n    Args:\n        nodes_data: Dictionary containing node data.\n        flows_data: Dictionary containing flow data.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        A dictionary of nodes that are dependent on flows.\n    \"\"\"\n    dependant_nodes, dependant_nodes_data = {}, {}\n    dependant_flow_ids = []\n\n    for node_id, node_data in nodes_data.items():\n        if \"flow\" in node_data:\n            dependant_nodes_data[node_id] = node_data\n            dependant_flow_ids.append(node_data[\"flow\"])\n        if \"flows\" in node_data:\n            dependant_nodes_data[node_id] = node_data\n            dependant_flow_ids.extend(node_data[\"flows\"])\n\n    # Get nodes from dependant flows\n    if dependant_flow_ids:\n        dependant_flows_nodes_ids = []\n        for flow_id, flow_data in flows_data.items():\n            if flow_id in dependant_flow_ids:\n                dependant_flows_nodes_ids.extend(flow_data.get(\"nodes\", []))\n\n        dependant_flows_nodes_data = {\n            node_id: node_data\n            for node_id, node_data in nodes_data.items()\n            if node_id in dependant_flows_nodes_ids\n        }\n        dependant_nodes = cls.get_nodes(\n            nodes_data=dependant_flows_nodes_data,\n            nodes={},\n            flows={},\n            connections=connections,\n            prompts=prompts,\n            registry=registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n\n    return dependant_nodes\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_entity_by_type","title":"<code>get_entity_by_type(entity_type, entity_registry=None)</code>  <code>classmethod</code>","text":"<p>Try to get entity by type and update mutable shared registry.</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of entity to retrieve.</p> required <code>entity_registry</code> <code>dict[str, Any] | None</code> <p>A registry of entities.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The retrieved entity.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the entity is not valid or cannot be found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_entity_by_type(\n    cls, entity_type: str, entity_registry: dict[str, Any] | None = None\n) -&gt; Any:\n    \"\"\"\n    Try to get entity by type and update mutable shared registry.\n\n    Args:\n        entity_type (str): The type of entity to retrieve.\n        entity_registry (dict[str, Any] | None): A registry of entities.\n\n    Returns:\n        Any: The retrieved entity.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the entity is not valid or cannot be found.\n    \"\"\"\n    if entity_registry is None:\n        entity_registry = {}\n\n    if entity := entity_registry.get(entity_type):\n        return entity\n\n    try:\n        entity = ConnectionManager.get_connection_by_type(entity_type)\n    except ValueError:\n        pass\n\n    if not entity:\n        try:\n            entity = NodeManager.get_node_by_type(entity_type)\n        except ValueError:\n            pass\n\n    if not entity:\n        raise WorkflowYAMLLoaderException(f\"Entity '{entity_type}' is not valid.\")\n\n    entity_registry[entity_type] = entity\n    return entity\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_flows","title":"<code>get_flows(data, flows, nodes, connection_manager=None)</code>  <code>classmethod</code>","text":"<p>Create flows from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing flow data.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Flow]</code> <p>A dictionary of newly created flows.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If flow data is invalid or duplicates are found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_flows(\n    cls,\n    data: dict,\n    flows: dict[str, Flow],\n    nodes: dict[str, Node],\n    connection_manager: ConnectionManager | None = None,\n) -&gt; dict[str, Flow]:\n    \"\"\"\n    Create flows from the given data.\n\n    Args:\n        data: Dictionary containing flow data.\n        flows: Existing flows dictionary.\n        nodes: Existing nodes dictionary.\n        connection_manager: Optional connection manager.\n\n    Returns:\n        A dictionary of newly created flows.\n\n    Raises:\n        WorkflowYAMLLoaderException: If flow data is invalid or duplicates are found.\n    \"\"\"\n    new_flows = {}\n    for flow_id, flow_data in data.items():\n        if flow_id in flows:\n            continue\n\n        if flow_id in new_flows:\n            raise WorkflowYAMLLoaderException(f\"Flow {flow_id} already exists\")\n\n        flow_node_ids = flow_data.get(\"nodes\", [])\n        flow_node_ids = set(flow_node_ids)\n        dep_node_ids = set()\n        for node_id in flow_node_ids:\n            if node_id not in nodes:\n                raise WorkflowYAMLLoaderException(\n                    f\"Node '{node_id}' for flow '{flow_id}' not found\"\n                )\n\n            dep_node_ids.update({dep.node.id for dep in nodes[node_id].depends})\n\n        for node_id in dep_node_ids:\n            if node_id not in flow_node_ids:\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency node '{node_id}' in the flow '{flow_id}' node list not found\"\n                )\n\n        flow_init_data = flow_data | {\n            \"id\": flow_id,\n            \"nodes\": [nodes[node_id] for node_id in flow_node_ids],\n        }\n        if connection_manager:\n            flow_init_data[\"connection_manager\"] = connection_manager\n\n        try:\n            flow = Flow(**flow_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Flow '{flow_id}' data is invalid. Data: {flow_data}. Error: {e}\"\n            )\n\n        new_flows[flow_id] = flow\n    return new_flows\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_node_connection","title":"<code>get_node_connection(node_id, node_data, connections)</code>  <code>classmethod</code>","text":"<p>Get the connection for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>connections</code> <code>dict[id, BaseConnection]</code> <p>A dictionary of available connections.</p> required <p>Returns:</p> Type Description <code>BaseConnection | None</code> <p>BaseConnection | None: The connection for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified connection is not found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_connection(\n    cls, node_id: str, node_data: dict, connections: dict[id, BaseConnection]\n) -&gt; BaseConnection | None:\n    \"\"\"\n    Get the connection for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        connections (dict[id, BaseConnection]): A dictionary of available connections.\n\n    Returns:\n        BaseConnection | None: The connection for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified connection is not found.\n    \"\"\"\n    conn = None\n    if conn_id := node_data.get(\"connection\"):\n        conn = connections.get(conn_id)\n        if not conn:\n            raise WorkflowYAMLLoaderException(\n                f\"Connection '{conn_id}' for node '{node_id}' not found\"\n            )\n    return conn\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_node_dependencies","title":"<code>get_node_dependencies(node_id, node_data, nodes)</code>  <code>classmethod</code>","text":"<p>Get the dependencies for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>A dictionary of available nodes.</p> required <p>Returns:</p> Type Description <p>list[NodeDependency]: A list of node dependencies.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If there's an error in dependency data or initialization.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_dependencies(\n    cls, node_id: str, node_data: dict, nodes: dict[str, Node]\n):\n    \"\"\"\n    Get the dependencies for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        nodes (dict[str, Node]): A dictionary of available nodes.\n\n    Returns:\n        list[NodeDependency]: A list of node dependencies.\n\n    Raises:\n        WorkflowYAMLLoaderException: If there's an error in dependency data or initialization.\n    \"\"\"\n    node_depends = []\n    for dependency_data in node_data.get(\"depends\", []):\n        dependency_node = nodes.get(dependency_data.get(\"node\"))\n        dependency_init_data = dependency_data | {\"node\": dependency_node}\n        try:\n            dependency = NodeDependency(**dependency_init_data)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Dependency data for node '{node_id}' is invalid. Data: {dependency_data}. Error: {e}\"\n            )\n\n        if dependency.option:\n            if not (dep_options := getattr(dependency_node, \"options\", [])):\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency '{dependency.node}' with option '{dependency.option}' \"\n                    f\"for node '{node_id}' not found\"\n                )\n\n            if not any(opt.id == dependency.option for opt in dep_options):\n                raise WorkflowYAMLLoaderException(\n                    f\"Dependency '{dependency.node}' with option '{dependency.option}' \"\n                    f\"for node '{node_id}' not found\"\n                )\n\n        node_depends.append(dependency)\n    return node_depends\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_node_flow","title":"<code>get_node_flow(node_id, node_data, flows)</code>  <code>classmethod</code>","text":"<p>Get the flow for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>flows</code> <code>dict[id, Flow]</code> <p>A dictionary of available flows.</p> required <p>Returns:</p> Type Description <code>Flow | None</code> <p>Flow | None: The flow for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified flow is not found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_flow(\n    cls, node_id: str, node_data: dict, flows: dict[id, Flow]\n) -&gt; Flow | None:\n    \"\"\"\n    Get the flow for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        flows (dict[id, Flow]): A dictionary of available flows.\n\n    Returns:\n        Flow | None: The flow for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified flow is not found.\n    \"\"\"\n    flow = None\n    if flow_id := node_data.get(\"flow\"):\n        flow = flows.get(flow_id)\n        if not flow:\n            raise WorkflowYAMLLoaderException(\n                f\"Flow '{flow_id}' for node '{node_id}' not found\"\n            )\n    return flow\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_node_flows","title":"<code>get_node_flows(node_id, node_data, flows)</code>  <code>classmethod</code>","text":"<p>Get the flows for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>flows</code> <code>dict[id, Flow]</code> <p>A dictionary of available flows.</p> required <p>Returns:</p> Type Description <code>list[Flow]</code> <p>list[Flow]: A list of flows for the node.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If any specified flow is not found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_flows(\n    cls, node_id: str, node_data: dict, flows: dict[id, Flow]\n) -&gt; list[Flow]:\n    \"\"\"\n    Get the flows for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        flows (dict[id, Flow]): A dictionary of available flows.\n\n    Returns:\n        list[Flow]: A list of flows for the node.\n\n    Raises:\n        WorkflowYAMLLoaderException: If any specified flow is not found.\n    \"\"\"\n    node_flows = []\n    for flow_id in node_data.get(\"flows\", []):\n        node_flow = flows.get(flow_id)\n        if not node_flow:\n            raise WorkflowYAMLLoaderException(\n                f\"Flow '{flow_id}' for node '{node_id}' not found\"\n            )\n        node_flows.append(node_flow)\n    return node_flows\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_node_prompt","title":"<code>get_node_prompt(node_id, node_data, prompts)</code>  <code>classmethod</code>","text":"<p>Get the prompt for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>prompts</code> <code>dict[id, Prompt]</code> <p>A dictionary of available prompts.</p> required <p>Returns:</p> Type Description <code>Prompt | None</code> <p>Prompt | None: The prompt for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified prompt is not found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_prompt(\n    cls, node_id: str, node_data: dict, prompts: dict[id, Prompt]\n) -&gt; Prompt | None:\n    \"\"\"\n    Get the prompt for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        prompts (dict[id, Prompt]): A dictionary of available prompts.\n\n    Returns:\n        Prompt | None: The prompt for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified prompt is not found.\n    \"\"\"\n    prompt = None\n    if prompt_id := node_data.get(\"prompt\"):\n        prompt = prompts.get(prompt_id)\n        if not prompt:\n            raise WorkflowYAMLLoaderException(\n                f\"Prompt '{prompt_id}' for node '{node_id}' not found\"\n            )\n    return prompt\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_node_vector_store_connection","title":"<code>get_node_vector_store_connection(node_id, node_data, connections)</code>  <code>classmethod</code>","text":"<p>Get the vector store connection for a node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node.</p> required <code>node_data</code> <code>dict</code> <p>The data for the node.</p> required <code>connections</code> <code>dict[id, BaseConnection]</code> <p>A dictionary of available connections.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>Any | None: The vector store connection for the node, or None if not found.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified vector store connection is not found or                          does not support vector store initialization.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_node_vector_store_connection(\n    cls, node_id: str, node_data: dict, connections: dict[id, BaseConnection]\n) -&gt; Any | None:\n    \"\"\"\n    Get the vector store connection for a node.\n\n    Args:\n        node_id (str): The ID of the node.\n        node_data (dict): The data for the node.\n        connections (dict[id, BaseConnection]): A dictionary of available connections.\n\n    Returns:\n        Any | None: The vector store connection for the node, or None if not found.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified vector store connection is not found or\n                                     does not support vector store initialization.\n    \"\"\"\n    if conn := cls.get_node_connection(\n        node_id=node_id, node_data=node_data, connections=connections\n    ):\n        if not (\n            conn_to_vs := getattr(conn, \"connect_to_vector_store\", None)\n        ) or not callable(conn_to_vs):\n            raise WorkflowYAMLLoaderException(\n                f\"Vector store connection '{conn.id}' for node '{node_id}' not support vector store initialization\"\n            )\n    return conn\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_nodes","title":"<code>get_nodes(nodes_data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Create nodes with dependencies from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <p>A dictionary of newly created nodes with dependencies.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_nodes(\n    cls,\n    nodes_data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n):\n    \"\"\"\n    Create nodes with dependencies from the given data.\n\n    Args:\n        nodes_data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        A dictionary of newly created nodes with dependencies.\n    \"\"\"\n    new_nodes = cls.get_nodes_without_depends(\n        data=nodes_data,\n        nodes=nodes,\n        flows=flows,\n        connections=connections,\n        prompts=prompts,\n        registry=registry,\n        connection_manager=connection_manager,\n        init_components=init_components,\n    )\n    all_nodes = nodes | new_nodes\n    for node_id, node in new_nodes.items():\n        node.depends = cls.get_node_dependencies(\n            node_id=node_id, node_data=nodes_data[node_id], nodes=all_nodes\n        )\n\n    return new_nodes\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_nodes_without_depends","title":"<code>get_nodes_without_depends(data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Create nodes without dependencies from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Node]</code> <p>A dictionary of newly created nodes without dependencies.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If node data is invalid or duplicates are found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_nodes_without_depends(\n    cls,\n    data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n) -&gt; dict[str, Node]:\n    \"\"\"\n    Create nodes without dependencies from the given data.\n\n    Args:\n        data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        A dictionary of newly created nodes without dependencies.\n\n    Raises:\n        WorkflowYAMLLoaderException: If node data is invalid or duplicates are found.\n    \"\"\"\n    new_nodes = {}\n    for node_id, node_data in data.items():\n        if node_id in nodes:\n            continue\n\n        if node_id in new_nodes:\n            raise WorkflowYAMLLoaderException(f\"Node '{node_id}' already exists\")\n\n        if not (node_type := node_data.get(\"type\")):\n            raise WorkflowYAMLLoaderException(\n                f\"Value 'type' for node '{node_id}' not found\"\n            )\n\n        node_cls = cls.get_entity_by_type(\n            entity_type=node_type, entity_registry=registry\n        )\n\n        # Init node params\n        node_init_data = copy.deepcopy(node_data) | {\n            \"id\": node_id,\n            \"is_postponed_component_init\": node_data.get(\n                \"is_postponed_component_init\", True\n            ),\n        }\n        node_init_data.pop(\"type\", None)\n        node_init_data.pop(\"depends\", None)\n\n        if \"connection\" in node_init_data:\n            get_node_conn = (\n                cls.get_node_vector_store_connection\n                if isinstance(node_cls, ConnectionNode)\n                else cls.get_node_connection\n            )\n            node_init_data[\"connection\"] = get_node_conn(\n                node_id=node_id, node_data=node_data, connections=connections\n            )\n        if prompt_data := node_init_data.get(\"prompt\"):\n            node_init_data[\"prompt\"] = (\n                cls.get_node_prompt(node_id=node_id, node_data=node_data, prompts=prompts)\n                if isinstance(prompt_data, str)\n                else cls.init_prompt(prompt_data)\n            )\n        if \"flow\" in node_init_data:\n            node_init_data[\"flow\"] = cls.get_node_flow(\n                node_id=node_id, node_data=node_data, flows=flows\n            )\n        if \"flows\" in node_init_data:\n            node_init_data[\"flows\"] = cls.get_node_flows(\n                node_id=node_id, node_data=node_data, flows=flows\n            )\n        try:\n            node_init_data = cls.get_updated_node_init_data_with_initialized_nodes(\n                node_init_data=node_init_data,\n                nodes=nodes,\n                flows=flows,\n                connections=connections,\n                prompts=prompts,\n                registry=registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n            )\n            node = node_cls(**node_init_data)\n            if init_components:\n                node.init_components(connection_manager=connection_manager)\n                node.is_postponed_component_init = False\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Node '{node_id}' data is invalid. Data: {node_data}. Error: {e}\"\n            )\n\n        new_nodes[node_id] = node\n    return new_nodes\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_prompts","title":"<code>get_prompts(data)</code>  <code>classmethod</code>","text":"<p>Get prompts from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, dict]</code> <p>The data containing prompt information.</p> required <p>Returns:</p> Type Description <code>dict[str, Prompt]</code> <p>dict[str, Prompt]: A dictionary of prompts.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If there's an error in prompt data or initialization.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_prompts(cls, data: dict[str, dict]) -&gt; dict[str, Prompt]:\n    \"\"\"\n    Get prompts from the provided data.\n\n    Args:\n        data (dict[str, dict]): The data containing prompt information.\n\n    Returns:\n        dict[str, Prompt]: A dictionary of prompts.\n\n    Raises:\n        WorkflowYAMLLoaderException: If there's an error in prompt data or initialization.\n    \"\"\"\n    prompts = {}\n    for prompt_id, prompt_data in data.get(\"prompts\", {}).items():\n        if prompt_id in prompts:\n            raise WorkflowYAMLLoaderException(\n                f\"Prompt '{prompt_id}' already exists\"\n            )\n        prompts[prompt_id] = cls.init_prompt(prompt_data | {\"id\": prompt_id})\n\n    return prompts\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_updated_node_init_data_with_initialized_nodes","title":"<code>get_updated_node_init_data_with_initialized_nodes(node_init_data, nodes, flows, connections, prompts, registry, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Get node init data with initialized nodes components (llms, agents, etc)</p> <p>Parameters:</p> Name Type Description Default <code>node_init_data</code> <code>dict</code> <p>Dictionary containing node data.</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>Existing nodes dictionary.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <code>connections</code> <code>dict[str, BaseConnection]</code> <p>Existing connections dictionary.</p> required <code>prompts</code> <code>dict[str, Prompt]</code> <p>Existing prompts dictionary.</p> required <code>registry</code> <code>dict[str, Any]</code> <p>Registry of node types.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <p>A dictionary of newly created nodes with dependencies.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_updated_node_init_data_with_initialized_nodes(\n    cls,\n    node_init_data: dict,\n    nodes: dict[str, Node],\n    flows: dict[str, Flow],\n    connections: dict[str, BaseConnection],\n    prompts: dict[str, Prompt],\n    registry: dict[str, Any],\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n):\n    \"\"\"\n    Get node init data with initialized nodes components (llms, agents, etc)\n\n    Args:\n        node_init_data: Dictionary containing node data.\n        nodes: Existing nodes dictionary.\n        flows: Existing flows dictionary.\n        connections: Existing connections dictionary.\n        prompts: Existing prompts dictionary.\n        registry: Registry of node types.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        A dictionary of newly created nodes with dependencies.\n    \"\"\"\n    updated_node_init_data = copy.deepcopy(node_init_data)\n    for param_name, param_data in node_init_data.items():\n        # TODO: dummy fix, revisit this!\n        # We had to add this condition because both input and output nodes have a `schema` param,\n        # which has a `type` field that contains types supported by JSON schema (e.g., string, object).\n        if param_name == \"schema\":\n            continue\n\n        if isinstance(param_data, dict) and param_data.get(\"type\"):\n            param_id = param_data.get(\"id\")\n            updated_node_init_data[param_name] = cls.get_nodes_without_depends(\n                data={param_id: param_data},\n                nodes=nodes,\n                flows=flows,\n                connections=connections,\n                prompts=prompts,\n                registry=registry,\n                connection_manager=connection_manager,\n                init_components=init_components,\n            )[param_id]\n        if isinstance(param_data, list):\n            updated_items = []\n            for item in param_data:\n                if isinstance(item, dict) and (item_id := item.get(\"id\")):\n                    updated_items.append(\n                        cls.get_updated_node_init_data_with_initialized_nodes(\n                            node_init_data={item_id: item},\n                            nodes=nodes,\n                            flows=flows,\n                            connections=connections,\n                            prompts=prompts,\n                            registry=registry,\n                            connection_manager=connection_manager,\n                            init_components=init_components,\n                        )[item_id]\n                    )\n                else:\n                    updated_items.append(item)\n            updated_node_init_data[param_name] = updated_items\n\n    return updated_node_init_data\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.get_workflows","title":"<code>get_workflows(data, flows)</code>  <code>classmethod</code>","text":"<p>Create workflows from the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing workflow data.</p> required <code>flows</code> <code>dict[str, Flow]</code> <p>Existing flows dictionary.</p> required <p>Returns:</p> Type Description <code>dict[str, Workflow]</code> <p>A dictionary of newly created workflows.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If workflow data is invalid.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef get_workflows(cls, data: dict, flows: dict[str, Flow]) -&gt; dict[str, Workflow]:\n    \"\"\"\n    Create workflows from the given data.\n\n    Args:\n        data: Dictionary containing workflow data.\n        flows: Existing flows dictionary.\n\n    Returns:\n        A dictionary of newly created workflows.\n\n    Raises:\n        WorkflowYAMLLoaderException: If workflow data is invalid.\n    \"\"\"\n    workflows = {}\n    for wf_id, wf_data in data.get(\"workflows\", {}).items():\n        if not (flow_id := wf_data.get(\"flow\")):\n            raise WorkflowYAMLLoaderException(\n                f\"Value 'flow' for dynamiq '{wf_id}' not found \"\n            )\n        if not (flow := flows.get(flow_id)):\n            raise WorkflowYAMLLoaderException(\n                f\"Flow '{flow_id}' for dynamiq '{wf_id}' not found\"\n            )\n        if version := wf_data.get(\"version\"):\n            version = str(version)\n\n        try:\n            wf = Workflow(id=wf_id, flow=flow, version=version)\n        except Exception as e:\n            raise WorkflowYAMLLoaderException(\n                f\"Workflow '{wf_id}' data is invalid. Data: {wf_data}. Error: {e}\"\n            )\n\n        workflows[wf_id] = wf\n    return workflows\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.init_prompt","title":"<code>init_prompt(prompt_init_data)</code>  <code>classmethod</code>","text":"<p>Initialize a prompt from the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_init_data</code> <code>dict</code> <p>The data for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The initialized prompt.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the specified prompt is not found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef init_prompt(cls, prompt_init_data: dict) -&gt; Prompt:\n    \"\"\"\n    Initialize a prompt from the provided data.\n\n    Args:\n        prompt_init_data (dict): The data for the prompt.\n\n    Returns:\n        Prompt: The initialized prompt.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the specified prompt is not found.\n    \"\"\"\n    try:\n        return Prompt(**prompt_init_data)\n    except Exception as e:\n        raise WorkflowYAMLLoaderException(f\"Prompt data is invalid. Data: {prompt_init_data}. \" f\"Error: {e}\")\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.load","title":"<code>load(file_path, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Load data from a YAML file and parse it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | PathLike</code> <p>Path to the YAML file.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <code>WorkflowYamlLoaderData</code> <p>Parsed WorkflowYamlLoaderData object.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    file_path: str | PathLike,\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n) -&gt; WorkflowYamlLoaderData:\n    \"\"\"\n    Load data from a YAML file and parse it.\n\n    Args:\n        file_path: Path to the YAML file.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        Parsed WorkflowYamlLoaderData object.\n    \"\"\"\n    data = cls.loads(file_path)\n    return cls.parse(\n        data=data,\n        connection_manager=connection_manager,\n        init_components=init_components,\n    )\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.loads","title":"<code>loads(file_path)</code>  <code>classmethod</code>","text":"<p>Load data from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | PathLike</code> <p>Path to the YAML file.</p> required <p>Returns:</p> Type Description <p>Parsed data from the YAML file.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If the file is not found.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef loads(cls, file_path: str | PathLike):\n    \"\"\"\n    Load data from a YAML file.\n\n    Args:\n        file_path: Path to the YAML file.\n\n    Returns:\n        Parsed data from the YAML file.\n\n    Raises:\n        WorkflowYAMLLoaderException: If the file is not found.\n    \"\"\"\n    from omegaconf import OmegaConf\n\n    try:\n        conf = OmegaConf.load(file_path)\n        logger.debug(f\"Loaded config from '{file_path}'\")\n\n        data = OmegaConf.to_container(conf, resolve=True)\n    except FileNotFoundError:\n        raise WorkflowYAMLLoaderException(f\"File '{file_path}' not found\")\n\n    return data\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoader.parse","title":"<code>parse(data, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Parse dynamiq workflow data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing workflow data.</p> required <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Optional connection manager.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Flag to initialize components.</p> <code>False</code> <p>Returns:</p> Type Description <code>WorkflowYamlLoaderData</code> <p>Parsed WorkflowYamlLoaderData object.</p> <p>Raises:</p> Type Description <code>WorkflowYAMLLoaderException</code> <p>If parsing fails.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>@classmethod\ndef parse(\n    cls,\n    data: dict,\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n) -&gt; WorkflowYamlLoaderData:\n    \"\"\"\n    Parse dynamiq workflow data.\n\n    Args:\n        data: Dictionary containing workflow data.\n        connection_manager: Optional connection manager.\n        init_components: Flag to initialize components.\n\n    Returns:\n        Parsed WorkflowYamlLoaderData object.\n\n    Raises:\n        WorkflowYAMLLoaderException: If parsing fails.\n    \"\"\"\n    nodes, flows = {}, {}\n    # Mutable shared registry that updates with each new entity.\n    node_registry, connection_registry = {}, {}\n    if init_components and connection_manager is None:\n        connection_manager = ConnectionManager()\n\n    try:\n        connections = cls.get_connections(data=data, registry=connection_registry)\n        prompts = cls.get_prompts(data)\n\n        nodes_data = data.get(\"nodes\", {})\n        flows_data = data.get(\"flows\", {})\n\n        dependant_nodes = cls.get_dependant_nodes(\n            nodes_data=nodes_data,\n            flows_data=flows_data,\n            connections=connections,\n            prompts=prompts,\n            registry=node_registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n        nodes.update(dependant_nodes)\n\n        dependant_flows = cls.get_dependant_flows(\n            nodes_data=nodes_data,\n            flows_data=flows_data,\n            dependant_nodes=dependant_nodes,\n            connection_manager=connection_manager,\n        )\n        flows.update(dependant_flows)\n\n        non_dependant_nodes = cls.get_nodes(\n            nodes_data=nodes_data,\n            nodes=nodes,\n            flows=flows,\n            connections=connections,\n            prompts=prompts,\n            registry=node_registry,\n            connection_manager=connection_manager,\n            init_components=init_components,\n        )\n        nodes.update(non_dependant_nodes)\n\n        non_dependant_flows = cls.get_flows(\n            data=flows_data,\n            flows=flows,\n            nodes=nodes,\n            connection_manager=connection_manager,\n        )\n        flows.update(non_dependant_flows)\n\n        workflows = cls.get_workflows(data, flows)\n    except WorkflowYAMLLoaderException:\n        raise\n    except Exception:\n        logger.exception(\"Failed to parse Yaml data with unexpected error\")\n        raise\n\n    return WorkflowYamlLoaderData(\n        connections=connections,\n        nodes=nodes,\n        flows=flows,\n        workflows=workflows,\n    )\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYAMLLoaderException","title":"<code>WorkflowYAMLLoaderException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in the WorkflowYAMLLoader.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>class WorkflowYAMLLoaderException(Exception):\n    \"\"\"Exception raised for errors in the WorkflowYAMLLoader.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/loaders/yaml/#dynamiq.loaders.yaml.WorkflowYamlLoaderData","title":"<code>WorkflowYamlLoaderData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data model for the WorkflowYAMLLoader output.</p> Source code in <code>dynamiq/loaders/yaml.py</code> <pre><code>class WorkflowYamlLoaderData(BaseModel):\n    \"\"\"Data model for the WorkflowYAMLLoader output.\"\"\"\n\n    connections: dict[str, BaseConnection]\n    nodes: dict[str, Node]\n    flows: dict[str, Flow]\n    workflows: dict[str, Workflow]\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/memory/memory/","title":"Memory","text":""},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory","title":"<code>Memory</code>","text":"<p>Manages the storage and retrieval of messages.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>class Memory:\n    \"\"\"Manages the storage and retrieval of messages.\"\"\"\n\n    def __init__(self, search_limit: int = 5, search_filters: dict = None, backend: MemoryBackend = InMemory()):\n        \"\"\"Initializes the Memory with the given search parameters and backend.\n\n        If no backend is provided, an InMemory backend is used by default.\n        \"\"\"\n        if not isinstance(backend, MemoryBackend):\n            raise TypeError(\"backend must be an instance of Backend\")\n\n        self.search_limit = search_limit\n        self.search_filters = search_filters or {}\n        self.backend = backend\n\n    def add(self, role: MessageRole, content: str, metadata: dict | None = None):\n        \"\"\"Adds a message to the memory.\"\"\"\n        try:\n            metadata = metadata or {}\n            metadata[\"timestamp\"] = datetime.utcnow().timestamp()\n            message = Message(role=role, content=content, metadata=metadata)\n            self.backend.add(message)\n            logger.debug(\n                f\"Memory {self.backend.name}: Added message: {message.role}: {message.content[:min(20, len(message.content))]}...\"  # noqa: E501\n            )\n        except Exception as e:\n            logger.error(f\"Error adding message: {e}\")\n            raise\n\n    def get_all(self) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from the memory.\"\"\"\n        messages = self.backend.get_all()\n        logger.debug(f\"Memory {self.backend.name}: Retrieved {len(messages)} messages\")\n        return messages\n\n    def get_all_messages_as_string(self, format: str = \"plain\") -&gt; str:\n        \"\"\"Retrieves all messages as a formatted string.\"\"\"\n        messages = self.get_all()\n        return self._format_messages_as_string(messages, format)\n\n    def search(self, query: str = None, filters: dict = None) -&gt; list[Message]:\n        \"\"\"Searches for messages relevant to the query or filters.\"\"\"\n        search_results = self.backend.search(\n            query=query, limit=self.search_limit, filters=filters or self.search_filters\n        )\n        logger.debug(\n            f\"Memory {self.backend.name}: Found {len(search_results)} search results for query: {query}, filters: {filters}\"  # noqa: E501\n        )\n        return search_results\n\n    def get_search_results_as_string(self, query: str, filters: dict = None, format: str = \"plain\") -&gt; str:\n        \"\"\"Searches for messages relevant to the query and returns them as a string.\"\"\"\n        messages = self.search(query, filters)\n        return self._format_messages_as_string(messages, format)\n\n    def _format_messages_as_string(self, messages: list[Message], format: str = \"plain\") -&gt; str:\n        \"\"\"Converts a list of messages to a formatted string.\"\"\"\n        if format == \"plain\":\n            return \"\\n\".join([f\"{msg.role.value}: {msg.content}\" for msg in messages])\n        elif format == \"markdown\":\n            return \"\\n\".join([f\"**{msg.role.value}:** {msg.content}\" for msg in messages])\n        elif format == \"xml\":\n            xml_string = \"&lt;messages&gt;\\n\"\n            for msg in messages:\n                xml_string += \"  &lt;message&gt;\\n\"\n                xml_string += f\"    &lt;role&gt;{msg.role.value}&lt;/role&gt;\\n\"\n                xml_string += f\"    &lt;content&gt;{msg.content}&lt;/content&gt;\\n\"\n                xml_string += \"  &lt;/message&gt;\\n\"\n            xml_string += \"&lt;/messages&gt;\"\n            return xml_string\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the memory is empty.\"\"\"\n        return self.backend.is_empty()\n\n    def clear(self):\n        \"\"\"Clears the memory.\"\"\"\n        try:\n            self.backend.clear()\n            logger.debug(f\"Memory {self.backend.name}: Cleared memory\")\n        except Exception as e:\n            logger.error(f\"Error clearing memory: {e}\")\n            raise e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.__init__","title":"<code>__init__(search_limit=5, search_filters=None, backend=InMemory())</code>","text":"<p>Initializes the Memory with the given search parameters and backend.</p> <p>If no backend is provided, an InMemory backend is used by default.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def __init__(self, search_limit: int = 5, search_filters: dict = None, backend: MemoryBackend = InMemory()):\n    \"\"\"Initializes the Memory with the given search parameters and backend.\n\n    If no backend is provided, an InMemory backend is used by default.\n    \"\"\"\n    if not isinstance(backend, MemoryBackend):\n        raise TypeError(\"backend must be an instance of Backend\")\n\n    self.search_limit = search_limit\n    self.search_filters = search_filters or {}\n    self.backend = backend\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.add","title":"<code>add(role, content, metadata=None)</code>","text":"<p>Adds a message to the memory.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def add(self, role: MessageRole, content: str, metadata: dict | None = None):\n    \"\"\"Adds a message to the memory.\"\"\"\n    try:\n        metadata = metadata or {}\n        metadata[\"timestamp\"] = datetime.utcnow().timestamp()\n        message = Message(role=role, content=content, metadata=metadata)\n        self.backend.add(message)\n        logger.debug(\n            f\"Memory {self.backend.name}: Added message: {message.role}: {message.content[:min(20, len(message.content))]}...\"  # noqa: E501\n        )\n    except Exception as e:\n        logger.error(f\"Error adding message: {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.clear","title":"<code>clear()</code>","text":"<p>Clears the memory.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def clear(self):\n    \"\"\"Clears the memory.\"\"\"\n    try:\n        self.backend.clear()\n        logger.debug(f\"Memory {self.backend.name}: Cleared memory\")\n    except Exception as e:\n        logger.error(f\"Error clearing memory: {e}\")\n        raise e\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_all","title":"<code>get_all()</code>","text":"<p>Retrieves all messages from the memory.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_all(self) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from the memory.\"\"\"\n    messages = self.backend.get_all()\n    logger.debug(f\"Memory {self.backend.name}: Retrieved {len(messages)} messages\")\n    return messages\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_all_messages_as_string","title":"<code>get_all_messages_as_string(format='plain')</code>","text":"<p>Retrieves all messages as a formatted string.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_all_messages_as_string(self, format: str = \"plain\") -&gt; str:\n    \"\"\"Retrieves all messages as a formatted string.\"\"\"\n    messages = self.get_all()\n    return self._format_messages_as_string(messages, format)\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.get_search_results_as_string","title":"<code>get_search_results_as_string(query, filters=None, format='plain')</code>","text":"<p>Searches for messages relevant to the query and returns them as a string.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def get_search_results_as_string(self, query: str, filters: dict = None, format: str = \"plain\") -&gt; str:\n    \"\"\"Searches for messages relevant to the query and returns them as a string.\"\"\"\n    messages = self.search(query, filters)\n    return self._format_messages_as_string(messages, format)\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the memory is empty.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the memory is empty.\"\"\"\n    return self.backend.is_empty()\n</code></pre>"},{"location":"dynamiq/memory/memory/#dynamiq.memory.memory.Memory.search","title":"<code>search(query=None, filters=None)</code>","text":"<p>Searches for messages relevant to the query or filters.</p> Source code in <code>dynamiq/memory/memory.py</code> <pre><code>def search(self, query: str = None, filters: dict = None) -&gt; list[Message]:\n    \"\"\"Searches for messages relevant to the query or filters.\"\"\"\n    search_results = self.backend.search(\n        query=query, limit=self.search_limit, filters=filters or self.search_filters\n    )\n    logger.debug(\n        f\"Memory {self.backend.name}: Found {len(search_results)} search results for query: {query}, filters: {filters}\"  # noqa: E501\n    )\n    return search_results\n</code></pre>"},{"location":"dynamiq/memory/backend/base/","title":"Base","text":""},{"location":"dynamiq/memory/backend/base/#dynamiq.memory.backend.base.MemoryBackend","title":"<code>MemoryBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for memory storage backends.</p> Source code in <code>dynamiq/memory/backend/base.py</code> <pre><code>class MemoryBackend(ABC):\n    \"\"\"Abstract base class for memory storage backends.\"\"\"\n\n    name = \"MemoryBackend\"\n\n    @abstractmethod\n    def add(self, message: Message):\n        \"\"\"Adds a message to the memory storage.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_all(self) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from the memory storage.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def search(self, query: str, limit: int) -&gt; list[Message]:\n        \"\"\"Searches for messages relevant to the query.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the memory storage is empty.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def clear(self):\n        \"\"\"Clears the memory storage.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backend/base/#dynamiq.memory.backend.base.MemoryBackend.add","title":"<code>add(message)</code>  <code>abstractmethod</code>","text":"<p>Adds a message to the memory storage.</p> Source code in <code>dynamiq/memory/backend/base.py</code> <pre><code>@abstractmethod\ndef add(self, message: Message):\n    \"\"\"Adds a message to the memory storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backend/base/#dynamiq.memory.backend.base.MemoryBackend.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Clears the memory storage.</p> Source code in <code>dynamiq/memory/backend/base.py</code> <pre><code>@abstractmethod\ndef clear(self):\n    \"\"\"Clears the memory storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backend/base/#dynamiq.memory.backend.base.MemoryBackend.get_all","title":"<code>get_all()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all messages from the memory storage.</p> Source code in <code>dynamiq/memory/backend/base.py</code> <pre><code>@abstractmethod\ndef get_all(self) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from the memory storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backend/base/#dynamiq.memory.backend.base.MemoryBackend.is_empty","title":"<code>is_empty()</code>  <code>abstractmethod</code>","text":"<p>Checks if the memory storage is empty.</p> Source code in <code>dynamiq/memory/backend/base.py</code> <pre><code>@abstractmethod\ndef is_empty(self) -&gt; bool:\n    \"\"\"Checks if the memory storage is empty.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backend/base/#dynamiq.memory.backend.base.MemoryBackend.search","title":"<code>search(query, limit)</code>  <code>abstractmethod</code>","text":"<p>Searches for messages relevant to the query.</p> Source code in <code>dynamiq/memory/backend/base.py</code> <pre><code>@abstractmethod\ndef search(self, query: str, limit: int) -&gt; list[Message]:\n    \"\"\"Searches for messages relevant to the query.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/","title":"In memory","text":""},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.BM25DocumentRanker","title":"<code>BM25DocumentRanker</code>","text":"<p>BM25 implementation for scoring documents.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>class BM25DocumentRanker:\n    \"\"\"BM25 implementation for scoring documents.\"\"\"\n\n    def __init__(self, documents: list[str], k1: float = 1.5, b: float = 0.75):\n        \"\"\"Initialize with a list of documents and parameters for BM25.\"\"\"\n        self.documents = documents\n        self.k1 = k1\n        self.b = b\n        self.avg_dl = self._calculate_avg_dl()\n\n    def _calculate_avg_dl(self):\n        \"\"\"Calculates the average document length (number of terms per document).\"\"\"\n        total_length = sum(len(doc.lower().split()) for doc in self.documents)\n        return total_length / len(self.documents) if self.documents else 0\n\n    def _idf(self, term: str, N: int, df: int) -&gt; float:\n        \"\"\"Calculates the IDF (inverse document frequency) of a term.\"\"\"\n        return math.log((N - df + 0.5) / (df + 0.5) + 1)\n\n    def score(self, query_terms: list[str], document: str) -&gt; float:\n        \"\"\"Calculates the BM25 score for a document.\"\"\"\n        doc_terms = document.lower().split()\n        doc_len = len(doc_terms)\n        doc_term_freqs = Counter(doc_terms)\n        N = len(self.documents)\n        score = 0.0\n\n        for term in query_terms:\n            term_freq = doc_term_freqs.get(term, 0)\n            if term_freq == 0:\n                continue\n            df = sum(1 for doc in self.documents if term in doc.lower().split())\n            idf = self._idf(term, N, df)\n            numerator = term_freq * (self.k1 + 1)\n            denominator = term_freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_dl))\n            score += idf * (numerator / denominator)\n\n        return score\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.BM25DocumentRanker.__init__","title":"<code>__init__(documents, k1=1.5, b=0.75)</code>","text":"<p>Initialize with a list of documents and parameters for BM25.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def __init__(self, documents: list[str], k1: float = 1.5, b: float = 0.75):\n    \"\"\"Initialize with a list of documents and parameters for BM25.\"\"\"\n    self.documents = documents\n    self.k1 = k1\n    self.b = b\n    self.avg_dl = self._calculate_avg_dl()\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.BM25DocumentRanker.score","title":"<code>score(query_terms, document)</code>","text":"<p>Calculates the BM25 score for a document.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def score(self, query_terms: list[str], document: str) -&gt; float:\n    \"\"\"Calculates the BM25 score for a document.\"\"\"\n    doc_terms = document.lower().split()\n    doc_len = len(doc_terms)\n    doc_term_freqs = Counter(doc_terms)\n    N = len(self.documents)\n    score = 0.0\n\n    for term in query_terms:\n        term_freq = doc_term_freqs.get(term, 0)\n        if term_freq == 0:\n            continue\n        df = sum(1 for doc in self.documents if term in doc.lower().split())\n        idf = self._idf(term, N, df)\n        numerator = term_freq * (self.k1 + 1)\n        denominator = term_freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_dl))\n        score += idf * (numerator / denominator)\n\n    return score\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory","title":"<code>InMemory</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>In-memory implementation of the memory storage backend.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>class InMemory(MemoryBackend):\n    \"\"\"In-memory implementation of the memory storage backend.\"\"\"\n\n    name = \"InMemory\"\n\n    def __init__(self):\n        \"\"\"Initializes the in-memory storage.\"\"\"\n        self.messages: list[Message] = []\n\n    def add(self, message: Message):\n        \"\"\"Adds a message to the in-memory list.\"\"\"\n        try:\n            self.messages.append(message)\n        except Exception as e:\n            raise InMemoryError(f\"Error adding message to InMemory backend: {e}\") from e\n\n    def get_all(self) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from the in-memory list.\"\"\"\n        return sorted(self.messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))  # Sort by timestamp\n\n    def _apply_filters(self, messages: list[Message], filters: dict) -&gt; list[Message]:\n        \"\"\"Applies metadata filters to the list of messages.\"\"\"\n        if not filters:\n            return messages\n        filtered_messages = messages\n        for key, value in filters.items():\n            if isinstance(value, list):\n                filtered_messages = [msg for msg in filtered_messages if any(v == msg.metadata.get(key) for v in value)]\n            else:\n                filtered_messages = [msg for msg in filtered_messages if value == msg.metadata.get(key)]\n        return filtered_messages\n\n    def search(self, query: str = None, limit: int = 10, filters: dict = None) -&gt; list[Message]:\n        \"\"\"Searches for messages using BM25 scoring, with optional filters.\"\"\"\n        if not query and not filters:\n            return self.get_all()[:limit]\n\n        filtered_messages = self._apply_filters(self.messages, filters)\n        if not query:\n            return filtered_messages[:limit]\n        query_terms = query.lower().split()\n        document_texts = [msg.content for msg in filtered_messages]\n        bm25 = BM25DocumentRanker(document_texts)\n        scored_messages = [(msg, bm25.score(query_terms, msg.content)) for msg in filtered_messages]\n        scored_messages = [(msg, score) for msg, score in scored_messages if score &gt; 0]\n        scored_messages.sort(key=lambda x: x[1], reverse=True)\n        return [msg for msg, _ in scored_messages][:limit]\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the in-memory list is empty.\"\"\"\n        return len(self.messages) == 0\n\n    def clear(self):\n        \"\"\"Clears the in-memory list.\"\"\"\n        self.messages = []\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the in-memory storage.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the in-memory storage.\"\"\"\n    self.messages: list[Message] = []\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory.add","title":"<code>add(message)</code>","text":"<p>Adds a message to the in-memory list.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def add(self, message: Message):\n    \"\"\"Adds a message to the in-memory list.\"\"\"\n    try:\n        self.messages.append(message)\n    except Exception as e:\n        raise InMemoryError(f\"Error adding message to InMemory backend: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory.clear","title":"<code>clear()</code>","text":"<p>Clears the in-memory list.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def clear(self):\n    \"\"\"Clears the in-memory list.\"\"\"\n    self.messages = []\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory.get_all","title":"<code>get_all()</code>","text":"<p>Retrieves all messages from the in-memory list.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def get_all(self) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from the in-memory list.\"\"\"\n    return sorted(self.messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))  # Sort by timestamp\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the in-memory list is empty.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the in-memory list is empty.\"\"\"\n    return len(self.messages) == 0\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemory.search","title":"<code>search(query=None, limit=10, filters=None)</code>","text":"<p>Searches for messages using BM25 scoring, with optional filters.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>def search(self, query: str = None, limit: int = 10, filters: dict = None) -&gt; list[Message]:\n    \"\"\"Searches for messages using BM25 scoring, with optional filters.\"\"\"\n    if not query and not filters:\n        return self.get_all()[:limit]\n\n    filtered_messages = self._apply_filters(self.messages, filters)\n    if not query:\n        return filtered_messages[:limit]\n    query_terms = query.lower().split()\n    document_texts = [msg.content for msg in filtered_messages]\n    bm25 = BM25DocumentRanker(document_texts)\n    scored_messages = [(msg, bm25.score(query_terms, msg.content)) for msg in filtered_messages]\n    scored_messages = [(msg, score) for msg, score in scored_messages if score &gt; 0]\n    scored_messages.sort(key=lambda x: x[1], reverse=True)\n    return [msg for msg, _ in scored_messages][:limit]\n</code></pre>"},{"location":"dynamiq/memory/backend/in_memory/#dynamiq.memory.backend.in_memory.InMemoryError","title":"<code>InMemoryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for InMemory backend errors.</p> Source code in <code>dynamiq/memory/backend/in_memory.py</code> <pre><code>class InMemoryError(Exception):\n    \"\"\"Base exception class for InMemory backend errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone","title":"<code>Pinecone</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>class Pinecone(MemoryBackend):\n    name = \"Pinecone\"\n\n    def __init__(\n        self,\n        connection: PineconeConnection,\n        embedder: BaseEmbedder,\n        index_type: PineconeIndexType,\n        index_name: str = \"conversations\",\n        namespace: str = \"default\",\n        cloud: str | None = None,\n        region: str | None = None,\n        environment: str | None = None,\n        pod_type: str | None = None,\n        pods: int = 1,\n    ):\n        \"\"\"Initializes the Pinecone memory storage.\"\"\"\n        self.connection = connection\n        self.index_name = index_name\n        self.embedder = embedder\n        self.namespace = namespace\n\n        try:\n            self.vector_store = PineconeVectorStore(\n                connection=connection,\n                index_name=index_name,\n                namespace=namespace,\n                dimension=embedder.dimensions,\n                create_if_not_exist=True,\n                index_type=index_type,\n                cloud=cloud,\n                region=region,\n                environment=environment,\n                pod_type=pod_type,\n                pods=pods,\n            )\n            # Verify connection\n            if not self.vector_store._index:\n                raise PineconeError(\"Failed to initialize Pinecone index\")\n\n        except Exception as e:\n            raise PineconeError(f\"Failed to initialize Pinecone vector store: {e}\")\n\n    def _message_to_document(self, message: Message) -&gt; Document:\n        \"\"\"Converts a Message object to a Document object.\"\"\"\n        return Document(\n            id=str(uuid.uuid4()),\n            content=message.content,\n            metadata={\"role\": message.role.value, **(message.metadata or {})},\n            embedding=None,  # Will be populated during write\n        )\n\n    def _document_to_message(self, document: Document) -&gt; Message:\n        \"\"\"Converts a Document object to a Message object.\"\"\"\n        metadata = dict(document.metadata)\n        role = metadata.pop(\"role\")\n        return Message(content=document.content, role=role, metadata=metadata, score=document.score)\n\n    def add(self, message: Message):\n        \"\"\"Stores a message in Pinecone.\"\"\"\n        try:\n            document = self._message_to_document(message)\n            embedding_result = self.embedder.embed_text(document.content)\n            document.embedding = embedding_result[\"embedding\"]\n\n            self.vector_store.write_documents([document])\n\n        except Exception as e:\n            raise PineconeError(f\"Error adding message to Pinecone: {e}\") from e\n\n    def get_all(self, limit: int = 10000) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from Pinecone.\"\"\"\n        try:\n            documents = self.vector_store.list_documents(include_embeddings=False)\n            messages = [self._document_to_message(doc) for doc in documents]\n            return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n        except Exception as e:\n            raise PineconeError(f\"Error retrieving messages from Pinecone: {e}\") from e\n\n    def _prepare_filters(self, filters: dict = None) -&gt; dict:\n        \"\"\"Convert simple filters to Pinecone filter format.\"\"\"\n        if not filters:\n            return None\n\n        if all(isinstance(v, (str, int, float, bool)) for v in filters.values()):\n            conditions = []\n            for key, value in filters.items():\n                conditions.append({\"field\": key, \"operator\": \"==\", \"value\": value})\n            return {\"operator\": \"AND\", \"conditions\": conditions}\n        return filters\n\n    def search(self, query: str = None, filters: dict = None, limit: int = 10) -&gt; list[Message]:\n        \"\"\"Searches for messages in Pinecone based on the query and/or filters.\"\"\"\n        try:\n            normalized_filters = self._prepare_filters(filters)\n\n            if query:\n                embedding_result = self.embedder.embed_text(query)\n                documents = self.vector_store._embedding_retrieval(\n                    query_embedding=embedding_result[\"embedding\"],\n                    namespace=self.namespace,\n                    filters=normalized_filters,\n                    top_k=limit,\n                    exclude_document_embeddings=True,\n                )\n            elif normalized_filters:\n                dummy_vector = [0.0] * self.embedder.dimensions\n                documents = self.vector_store._embedding_retrieval(\n                    query_embedding=dummy_vector,\n                    namespace=self.namespace,\n                    filters=normalized_filters,\n                    top_k=limit,\n                    exclude_document_embeddings=True,\n                )\n            else:\n                return []\n\n            return [self._document_to_message(doc) for doc in documents]\n        except Exception as e:\n            raise PineconeError(f\"Error searching in Pinecone: {e}\") from e\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the Pinecone index is empty.\"\"\"\n        try:\n            return self.vector_store.count_documents() == 0\n        except Exception as e:\n            raise PineconeError(f\"Error checking if Pinecone index is empty: {e}\") from e\n\n    def clear(self):\n        \"\"\"Clears the Pinecone index.\"\"\"\n        try:\n            self.vector_store.delete_documents(delete_all=True)\n        except Exception as e:\n            raise PineconeError(f\"Error clearing Pinecone index: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone.__init__","title":"<code>__init__(connection, embedder, index_type, index_name='conversations', namespace='default', cloud=None, region=None, environment=None, pod_type=None, pods=1)</code>","text":"<p>Initializes the Pinecone memory storage.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>def __init__(\n    self,\n    connection: PineconeConnection,\n    embedder: BaseEmbedder,\n    index_type: PineconeIndexType,\n    index_name: str = \"conversations\",\n    namespace: str = \"default\",\n    cloud: str | None = None,\n    region: str | None = None,\n    environment: str | None = None,\n    pod_type: str | None = None,\n    pods: int = 1,\n):\n    \"\"\"Initializes the Pinecone memory storage.\"\"\"\n    self.connection = connection\n    self.index_name = index_name\n    self.embedder = embedder\n    self.namespace = namespace\n\n    try:\n        self.vector_store = PineconeVectorStore(\n            connection=connection,\n            index_name=index_name,\n            namespace=namespace,\n            dimension=embedder.dimensions,\n            create_if_not_exist=True,\n            index_type=index_type,\n            cloud=cloud,\n            region=region,\n            environment=environment,\n            pod_type=pod_type,\n            pods=pods,\n        )\n        # Verify connection\n        if not self.vector_store._index:\n            raise PineconeError(\"Failed to initialize Pinecone index\")\n\n    except Exception as e:\n        raise PineconeError(f\"Failed to initialize Pinecone vector store: {e}\")\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone.add","title":"<code>add(message)</code>","text":"<p>Stores a message in Pinecone.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>def add(self, message: Message):\n    \"\"\"Stores a message in Pinecone.\"\"\"\n    try:\n        document = self._message_to_document(message)\n        embedding_result = self.embedder.embed_text(document.content)\n        document.embedding = embedding_result[\"embedding\"]\n\n        self.vector_store.write_documents([document])\n\n    except Exception as e:\n        raise PineconeError(f\"Error adding message to Pinecone: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone.clear","title":"<code>clear()</code>","text":"<p>Clears the Pinecone index.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>def clear(self):\n    \"\"\"Clears the Pinecone index.\"\"\"\n    try:\n        self.vector_store.delete_documents(delete_all=True)\n    except Exception as e:\n        raise PineconeError(f\"Error clearing Pinecone index: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone.get_all","title":"<code>get_all(limit=10000)</code>","text":"<p>Retrieves all messages from Pinecone.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>def get_all(self, limit: int = 10000) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from Pinecone.\"\"\"\n    try:\n        documents = self.vector_store.list_documents(include_embeddings=False)\n        messages = [self._document_to_message(doc) for doc in documents]\n        return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n    except Exception as e:\n        raise PineconeError(f\"Error retrieving messages from Pinecone: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the Pinecone index is empty.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the Pinecone index is empty.\"\"\"\n    try:\n        return self.vector_store.count_documents() == 0\n    except Exception as e:\n        raise PineconeError(f\"Error checking if Pinecone index is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.Pinecone.search","title":"<code>search(query=None, filters=None, limit=10)</code>","text":"<p>Searches for messages in Pinecone based on the query and/or filters.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>def search(self, query: str = None, filters: dict = None, limit: int = 10) -&gt; list[Message]:\n    \"\"\"Searches for messages in Pinecone based on the query and/or filters.\"\"\"\n    try:\n        normalized_filters = self._prepare_filters(filters)\n\n        if query:\n            embedding_result = self.embedder.embed_text(query)\n            documents = self.vector_store._embedding_retrieval(\n                query_embedding=embedding_result[\"embedding\"],\n                namespace=self.namespace,\n                filters=normalized_filters,\n                top_k=limit,\n                exclude_document_embeddings=True,\n            )\n        elif normalized_filters:\n            dummy_vector = [0.0] * self.embedder.dimensions\n            documents = self.vector_store._embedding_retrieval(\n                query_embedding=dummy_vector,\n                namespace=self.namespace,\n                filters=normalized_filters,\n                top_k=limit,\n                exclude_document_embeddings=True,\n            )\n        else:\n            return []\n\n        return [self._document_to_message(doc) for doc in documents]\n    except Exception as e:\n        raise PineconeError(f\"Error searching in Pinecone: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/pinecone/#dynamiq.memory.backend.pinecone.PineconeError","title":"<code>PineconeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for Pinecone-related errors.</p> Source code in <code>dynamiq/memory/backend/pinecone.py</code> <pre><code>class PineconeError(Exception):\n    \"\"\"Base exception class for Pinecone-related errors.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant","title":"<code>Qdrant</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>Qdrant implementation of the memory storage backend.</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>class Qdrant(MemoryBackend):\n    \"\"\"Qdrant implementation of the memory storage backend.\"\"\"\n\n    name = \"Qdrant\"\n\n    def __init__(\n        self,\n        connection: QdrantConnection,\n        embedder: BaseEmbedder,\n        index_name: str = \"conversations\",\n        metric: str = \"cosine\",\n        on_disk: bool = False,\n        create_if_not_exist: bool = True,\n    ):\n        \"\"\"Initializes the Qdrant memory storage.\n\n        Args:\n            connection: QdrantConnection instance for connecting to Qdrant\n            embedder: Embedder instance for creating embeddings\n            index_name: Name of the collection to store messages\n        \"\"\"\n        self.connection = connection\n        self.index_name = index_name\n        self.embedder = embedder\n\n        try:\n            self.vector_store = QdrantVectorStore(\n                connection=connection,\n                index_name=index_name,\n                dimension=embedder.dimensions,\n                create_if_not_exist=create_if_not_exist,\n                metric=metric,\n                on_disk=on_disk,\n                recreate_index=False,\n            )\n            self.client = self.vector_store._client\n            if not self.client:\n                raise QdrantError(\"Failed to initialize Qdrant client\")\n        except Exception as e:\n            raise QdrantError(f\"Failed to connect to Qdrant: {e}\") from e\n\n    def _message_to_document(self, message: Message) -&gt; Document:\n        \"\"\"Converts a Message object to a Document object.\"\"\"\n        return Document(\n            id=str(uuid.uuid4()),\n            content=message.content,\n            metadata={\"role\": message.role.value, **(message.metadata or {})},\n            embedding=None,  # Will be populated during write\n        )\n\n    def _document_to_message(self, document: Document) -&gt; Message:\n        \"\"\"Converts a Document object to a Message object.\"\"\"\n        metadata = dict(document.metadata)\n        role = metadata.pop(\"role\")\n        return Message(content=document.content, role=role, metadata=metadata, score=document.score)\n\n    def add(self, message: Message):\n        \"\"\"Stores a message in Qdrant.\n\n        Args:\n            message: Message to store\n\n        Raises:\n            QdrantError: If failed to add message\n        \"\"\"\n        try:\n            document = self._message_to_document(message)\n            embedding_result = self.embedder.embed_text(document.content)\n            document.embedding = embedding_result[\"embedding\"]\n\n            self.vector_store.write_documents(\n                documents=[document], policy=DuplicatePolicy.SKIP  # Changed from OVERWRITE to prevent recreation\n            )\n        except Exception as e:\n            raise QdrantError(f\"Failed to add message to Qdrant: {e}\") from e\n\n    def get_all(self, limit: int = None) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from Qdrant.\n\n        Args:\n            limit: Maximum number of messages to retrieve\n\n        Returns:\n            List of messages sorted by timestamp\n        \"\"\"\n        try:\n            documents = self.vector_store.list_documents(include_embeddings=False)\n            messages = [self._document_to_message(doc) for doc in documents]\n            return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n        except Exception as e:\n            raise QdrantError(f\"Failed to retrieve messages from Qdrant: {e}\") from e\n\n    def search(self, query: str = None, limit: int = 10, filters: dict = None) -&gt; list[Message]:\n        \"\"\"Searches for messages in Qdrant.\n\n        Args:\n            query: Text query to search for\n            limit: Maximum number of results to return\n            filters: Metadata filters to apply\n\n        Returns:\n            List of matching messages\n        \"\"\"\n        try:\n            qdrant_filters = self._prepare_filters(filters)\n            if query:\n                embedding_result = self.embedder.embed_text(query)\n                documents = self.vector_store._query_by_embedding(\n                    query_embedding=embedding_result[\"embedding\"],\n                    filters=qdrant_filters,\n                    top_k=limit,\n                    return_embedding=False,\n                )\n            elif filters:\n                documents = self.vector_store.filter_documents(filters=qdrant_filters)\n                if limit:\n                    documents = documents[:limit]\n            else:\n                return []\n\n            return [self._document_to_message(doc) for doc in documents]\n        except Exception as e:\n            raise QdrantError(f\"Error searching in Qdrant: {e}\") from e\n\n    def _prepare_filters(self, filters: dict | None = None) -&gt; dict | None:\n        \"\"\"Prepares simple filters for Qdrant vector store format.\"\"\"\n        if not filters:\n            return None\n\n        conditions = []\n        for key, value in filters.items():\n            if isinstance(value, (str, int, float, bool)):\n                condition = {\"operator\": \"==\", \"field\": key, \"value\": value}\n            elif isinstance(value, list):\n                condition = {\"operator\": \"in\", \"field\": key, \"value\": value}\n            elif isinstance(value, dict) and any(k in value for k in [\"gte\", \"lte\", \"gt\", \"lt\"]):\n                condition = {\"operator\": \"range\", \"field\": key, **value}\n            else:\n                raise QdrantError(f\"Unsupported filter value type for key '{key}': {type(value)}\")\n\n            conditions.append(condition)\n        return {\"operator\": \"AND\", \"conditions\": conditions} if conditions else None\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the Qdrant collection is empty.\"\"\"\n        try:\n            return self.vector_store.count_documents() == 0\n        except UnexpectedResponse as e:\n            if e.status_code == 404:  # Collection doesn't exist\n                return True\n            raise QdrantError(f\"Failed to check if Qdrant collection is empty: {e}\") from e\n\n    def clear(self):\n        \"\"\"Clears the Qdrant collection.\"\"\"\n        try:\n            self.vector_store.delete_documents(delete_all=True)\n        except Exception as e:\n            raise QdrantError(f\"Failed to clear Qdrant collection: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant.__init__","title":"<code>__init__(connection, embedder, index_name='conversations', metric='cosine', on_disk=False, create_if_not_exist=True)</code>","text":"<p>Initializes the Qdrant memory storage.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Qdrant</code> <p>QdrantConnection instance for connecting to Qdrant</p> required <code>embedder</code> <code>BaseEmbedder</code> <p>Embedder instance for creating embeddings</p> required <code>index_name</code> <code>str</code> <p>Name of the collection to store messages</p> <code>'conversations'</code> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>def __init__(\n    self,\n    connection: QdrantConnection,\n    embedder: BaseEmbedder,\n    index_name: str = \"conversations\",\n    metric: str = \"cosine\",\n    on_disk: bool = False,\n    create_if_not_exist: bool = True,\n):\n    \"\"\"Initializes the Qdrant memory storage.\n\n    Args:\n        connection: QdrantConnection instance for connecting to Qdrant\n        embedder: Embedder instance for creating embeddings\n        index_name: Name of the collection to store messages\n    \"\"\"\n    self.connection = connection\n    self.index_name = index_name\n    self.embedder = embedder\n\n    try:\n        self.vector_store = QdrantVectorStore(\n            connection=connection,\n            index_name=index_name,\n            dimension=embedder.dimensions,\n            create_if_not_exist=create_if_not_exist,\n            metric=metric,\n            on_disk=on_disk,\n            recreate_index=False,\n        )\n        self.client = self.vector_store._client\n        if not self.client:\n            raise QdrantError(\"Failed to initialize Qdrant client\")\n    except Exception as e:\n        raise QdrantError(f\"Failed to connect to Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant.add","title":"<code>add(message)</code>","text":"<p>Stores a message in Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to store</p> required <p>Raises:</p> Type Description <code>QdrantError</code> <p>If failed to add message</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>def add(self, message: Message):\n    \"\"\"Stores a message in Qdrant.\n\n    Args:\n        message: Message to store\n\n    Raises:\n        QdrantError: If failed to add message\n    \"\"\"\n    try:\n        document = self._message_to_document(message)\n        embedding_result = self.embedder.embed_text(document.content)\n        document.embedding = embedding_result[\"embedding\"]\n\n        self.vector_store.write_documents(\n            documents=[document], policy=DuplicatePolicy.SKIP  # Changed from OVERWRITE to prevent recreation\n        )\n    except Exception as e:\n        raise QdrantError(f\"Failed to add message to Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant.clear","title":"<code>clear()</code>","text":"<p>Clears the Qdrant collection.</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>def clear(self):\n    \"\"\"Clears the Qdrant collection.\"\"\"\n    try:\n        self.vector_store.delete_documents(delete_all=True)\n    except Exception as e:\n        raise QdrantError(f\"Failed to clear Qdrant collection: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant.get_all","title":"<code>get_all(limit=None)</code>","text":"<p>Retrieves all messages from Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of messages to retrieve</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of messages sorted by timestamp</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>def get_all(self, limit: int = None) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from Qdrant.\n\n    Args:\n        limit: Maximum number of messages to retrieve\n\n    Returns:\n        List of messages sorted by timestamp\n    \"\"\"\n    try:\n        documents = self.vector_store.list_documents(include_embeddings=False)\n        messages = [self._document_to_message(doc) for doc in documents]\n        return sorted(messages, key=lambda msg: msg.metadata.get(\"timestamp\", 0))\n    except Exception as e:\n        raise QdrantError(f\"Failed to retrieve messages from Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the Qdrant collection is empty.</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the Qdrant collection is empty.\"\"\"\n    try:\n        return self.vector_store.count_documents() == 0\n    except UnexpectedResponse as e:\n        if e.status_code == 404:  # Collection doesn't exist\n            return True\n        raise QdrantError(f\"Failed to check if Qdrant collection is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.Qdrant.search","title":"<code>search(query=None, limit=10, filters=None)</code>","text":"<p>Searches for messages in Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Text query to search for</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return</p> <code>10</code> <code>filters</code> <code>dict</code> <p>Metadata filters to apply</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>List of matching messages</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>def search(self, query: str = None, limit: int = 10, filters: dict = None) -&gt; list[Message]:\n    \"\"\"Searches for messages in Qdrant.\n\n    Args:\n        query: Text query to search for\n        limit: Maximum number of results to return\n        filters: Metadata filters to apply\n\n    Returns:\n        List of matching messages\n    \"\"\"\n    try:\n        qdrant_filters = self._prepare_filters(filters)\n        if query:\n            embedding_result = self.embedder.embed_text(query)\n            documents = self.vector_store._query_by_embedding(\n                query_embedding=embedding_result[\"embedding\"],\n                filters=qdrant_filters,\n                top_k=limit,\n                return_embedding=False,\n            )\n        elif filters:\n            documents = self.vector_store.filter_documents(filters=qdrant_filters)\n            if limit:\n                documents = documents[:limit]\n        else:\n            return []\n\n        return [self._document_to_message(doc) for doc in documents]\n    except Exception as e:\n        raise QdrantError(f\"Error searching in Qdrant: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/qdrant/#dynamiq.memory.backend.qdrant.QdrantError","title":"<code>QdrantError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for Qdrant-related errors.</p> Source code in <code>dynamiq/memory/backend/qdrant.py</code> <pre><code>class QdrantError(Exception):\n    \"\"\"Base exception for Qdrant-related errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/","title":"Sqlite","text":""},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite","title":"<code>SQLite</code>","text":"<p>               Bases: <code>MemoryBackend</code></p> <p>SQLite implementation of the memory storage backend.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>class SQLite(MemoryBackend):\n    \"\"\"SQLite implementation of the memory storage backend.\"\"\"\n\n    name = \"SQLite\"\n\n    # Define constants for the SQL queries\n    CREATE_TABLE_QUERY = \"\"\"\n        CREATE TABLE IF NOT EXISTS {index_name} (\n            id TEXT PRIMARY KEY,\n            role TEXT NOT NULL,\n            content TEXT NOT NULL,\n            metadata TEXT,\n            timestamp REAL\n        )\n    \"\"\"  # nosec B608\n\n    VALIDATE_TABLE_QUERY = \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\"\n    INSERT_MESSAGE_QUERY = \"\"\"\n        INSERT INTO {index_name} (id, role, content, metadata, timestamp)\n        VALUES (?, ?, ?, ?, ?)\n    \"\"\"  # nosec B608\n    SELECT_ALL_MESSAGES_QUERY = \"\"\"\n        SELECT id, role, content, metadata, timestamp\n        FROM {index_name}\n        ORDER BY timestamp ASC\n    \"\"\"  # nosec B608\n    CHECK_IF_EMPTY_QUERY = \"SELECT COUNT(*) FROM {index_name}\"  # nosec B608\n    CLEAR_TABLE_QUERY = \"DELETE FROM {index_name}\"  # nosec B608\n    SEARCH_MESSAGES_QUERY = \"\"\"\n        SELECT id, role, content, metadata\n        FROM {index_name}\n    \"\"\"  # nosec B608\n\n    def __init__(self, db_path: str = \"conversations.db\", index_name: str = \"conversations\"):\n        \"\"\"Initializes the SQLite memory storage.\"\"\"\n        self.db_path = db_path\n        self.index_name = index_name\n\n        try:\n            self._validate_table_name(create_if_not_exists=True)\n        except Exception as e:\n            raise SQLiteError(f\"Error initializing SQLite backend: {e}\") from e\n\n    def _validate_table_name(self, create_if_not_exists: bool = False):\n        \"\"\"Validates the table name to prevent SQL injection and optionally creates it.\"\"\"\n        if not re.match(r\"^[A-Za-z0-9_]+$\", self.index_name):\n            raise SQLiteError(f\"Invalid table name: '{self.index_name}'\")\n\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(self.VALIDATE_TABLE_QUERY, (self.index_name,))  # nosec B608\n                result = cursor.fetchone()\n\n                if result is None:\n                    if create_if_not_exists:\n                        self._create_table()\n                    else:\n                        raise SQLiteError(f\"Table '{self.index_name}' does not exist in the database.\")\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error validating or creating table: {e}\") from e\n\n    def _create_table(self):\n        \"\"\"Creates the messages table.\"\"\"\n        query = self.CREATE_TABLE_QUERY.format(index_name=self.index_name)\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)  # nosec B608\n                conn.commit()\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error creating table: {e}\") from e\n\n    def add(self, message: Message):\n        \"\"\"Stores a message in the SQLite database.\"\"\"\n        try:\n            query = self.INSERT_MESSAGE_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                message_id = str(uuid.uuid4())\n                cursor.execute(\n                    query,\n                    (\n                        message_id,\n                        message.role.value,\n                        message.content,\n                        json.dumps(message.metadata),\n                        message.metadata.get(\"timestamp\", 0),\n                    ),\n                )  # nosec B608\n                conn.commit()\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error adding message to database: {e}\") from e\n\n    def get_all(self) -&gt; list[Message]:\n        \"\"\"Retrieves all messages from the SQLite database.\"\"\"\n        try:\n            query = self.SELECT_ALL_MESSAGES_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)  # nosec B608\n                rows = cursor.fetchall()\n            return [\n                Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\"), timestamp=row[4])\n                for row in rows\n            ]\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error retrieving messages from database: {e}\") from e\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the SQLite database is empty.\"\"\"\n        try:\n            query = self.CHECK_IF_EMPTY_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)  # nosec B608\n                count = cursor.fetchone()[0]\n            return count == 0\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error checking if database is empty: {e}\") from e\n\n    def clear(self):\n        \"\"\"Clears the SQLite database by deleting all rows in the table.\"\"\"\n        try:\n            query = self.CLEAR_TABLE_QUERY.format(index_name=self.index_name)\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)  # nosec B608\n                conn.commit()\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error clearing database: {e}\") from e\n\n    def search(self, query: str = None, limit: int = 10, filters: dict = None) -&gt; list[Message]:\n        \"\"\"Searches for messages in SQLite based on the query and/or filters.\"\"\"\n        limit = limit or self.config.search_limit  # Use default if not provided\n        try:\n            where_clauses = []\n            params = []\n\n            if query:\n                where_clauses.append(\"content LIKE ?\")\n                params.append(f\"%{query}%\")\n\n            if filters:\n                for key, value in filters.items():\n                    if isinstance(value, list):\n                        # Use IN for list-based filters\n                        placeholders = \",\".join(\"?\" for _ in value)\n                        where_clauses.append(f\"json_extract(metadata, '$.{key}') IN ({placeholders})\")\n                        params.extend(value)\n                    else:\n                        # Use LIKE for string matching, otherwise use = for exact match\n                        if isinstance(value, str) and \"%\" in value:\n                            where_clauses.append(f\"json_extract(metadata, '$.{key}') LIKE ?\")\n                            params.append(value)\n                        else:\n                            where_clauses.append(f\"json_extract(metadata, '$.{key}') = ?\")\n                            params.append(value)\n\n            query_str = self.SEARCH_MESSAGES_QUERY.format(index_name=self.index_name)\n            if where_clauses:\n                query_str += f\" WHERE {' AND '.join(where_clauses)}\"\n            query_str += \" ORDER BY id DESC LIMIT ?\"\n            params.append(limit)\n\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.cursor()\n                cursor.execute(query_str, params)\n                rows = cursor.fetchall()\n\n            return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\")) for row in rows]\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error searching in database: {e}\") from e\n\n        except sqlite3.Error as e:\n            raise SQLiteError(f\"Error searching in database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite.__init__","title":"<code>__init__(db_path='conversations.db', index_name='conversations')</code>","text":"<p>Initializes the SQLite memory storage.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>def __init__(self, db_path: str = \"conversations.db\", index_name: str = \"conversations\"):\n    \"\"\"Initializes the SQLite memory storage.\"\"\"\n    self.db_path = db_path\n    self.index_name = index_name\n\n    try:\n        self._validate_table_name(create_if_not_exists=True)\n    except Exception as e:\n        raise SQLiteError(f\"Error initializing SQLite backend: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite.add","title":"<code>add(message)</code>","text":"<p>Stores a message in the SQLite database.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>def add(self, message: Message):\n    \"\"\"Stores a message in the SQLite database.\"\"\"\n    try:\n        query = self.INSERT_MESSAGE_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            message_id = str(uuid.uuid4())\n            cursor.execute(\n                query,\n                (\n                    message_id,\n                    message.role.value,\n                    message.content,\n                    json.dumps(message.metadata),\n                    message.metadata.get(\"timestamp\", 0),\n                ),\n            )  # nosec B608\n            conn.commit()\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error adding message to database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite.clear","title":"<code>clear()</code>","text":"<p>Clears the SQLite database by deleting all rows in the table.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>def clear(self):\n    \"\"\"Clears the SQLite database by deleting all rows in the table.\"\"\"\n    try:\n        query = self.CLEAR_TABLE_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)  # nosec B608\n            conn.commit()\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error clearing database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite.get_all","title":"<code>get_all()</code>","text":"<p>Retrieves all messages from the SQLite database.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>def get_all(self) -&gt; list[Message]:\n    \"\"\"Retrieves all messages from the SQLite database.\"\"\"\n    try:\n        query = self.SELECT_ALL_MESSAGES_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)  # nosec B608\n            rows = cursor.fetchall()\n        return [\n            Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\"), timestamp=row[4])\n            for row in rows\n        ]\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error retrieving messages from database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite.is_empty","title":"<code>is_empty()</code>","text":"<p>Checks if the SQLite database is empty.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Checks if the SQLite database is empty.\"\"\"\n    try:\n        query = self.CHECK_IF_EMPTY_QUERY.format(index_name=self.index_name)\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)  # nosec B608\n            count = cursor.fetchone()[0]\n        return count == 0\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error checking if database is empty: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLite.search","title":"<code>search(query=None, limit=10, filters=None)</code>","text":"<p>Searches for messages in SQLite based on the query and/or filters.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>def search(self, query: str = None, limit: int = 10, filters: dict = None) -&gt; list[Message]:\n    \"\"\"Searches for messages in SQLite based on the query and/or filters.\"\"\"\n    limit = limit or self.config.search_limit  # Use default if not provided\n    try:\n        where_clauses = []\n        params = []\n\n        if query:\n            where_clauses.append(\"content LIKE ?\")\n            params.append(f\"%{query}%\")\n\n        if filters:\n            for key, value in filters.items():\n                if isinstance(value, list):\n                    # Use IN for list-based filters\n                    placeholders = \",\".join(\"?\" for _ in value)\n                    where_clauses.append(f\"json_extract(metadata, '$.{key}') IN ({placeholders})\")\n                    params.extend(value)\n                else:\n                    # Use LIKE for string matching, otherwise use = for exact match\n                    if isinstance(value, str) and \"%\" in value:\n                        where_clauses.append(f\"json_extract(metadata, '$.{key}') LIKE ?\")\n                        params.append(value)\n                    else:\n                        where_clauses.append(f\"json_extract(metadata, '$.{key}') = ?\")\n                        params.append(value)\n\n        query_str = self.SEARCH_MESSAGES_QUERY.format(index_name=self.index_name)\n        if where_clauses:\n            query_str += f\" WHERE {' AND '.join(where_clauses)}\"\n        query_str += \" ORDER BY id DESC LIMIT ?\"\n        params.append(limit)\n\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query_str, params)\n            rows = cursor.fetchall()\n\n        return [Message(role=row[1], content=row[2], metadata=json.loads(row[3] or \"{}\")) for row in rows]\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error searching in database: {e}\") from e\n\n    except sqlite3.Error as e:\n        raise SQLiteError(f\"Error searching in database: {e}\") from e\n</code></pre>"},{"location":"dynamiq/memory/backend/sqlite/#dynamiq.memory.backend.sqlite.SQLiteError","title":"<code>SQLiteError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for SQLite-related errors in the memory backend.</p> Source code in <code>dynamiq/memory/backend/sqlite.py</code> <pre><code>class SQLiteError(Exception):\n    \"\"\"Base exception class for SQLite-related errors in the memory backend.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/","title":"Exceptions","text":""},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeConditionFailedException","title":"<code>NodeConditionFailedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node's condition fails to be met.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeConditionFailedException(NodeException):\n    \"\"\"\n    Exception raised when a node's condition fails to be met.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeConditionSkippedException","title":"<code>NodeConditionSkippedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node's condition skipped.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeConditionSkippedException(NodeException):\n    \"\"\"\n    Exception raised when a node's condition skipped.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeException","title":"<code>NodeException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for node-related errors.</p> <p>Parameters:</p> Name Type Description Default <code>failed_depend</code> <code>NodeDependency</code> <p>The dependency that caused the exception. Defaults to None.</p> <code>None</code> <code>message</code> <code>str</code> <p>Additional error message. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>failed_depend</code> <code>NodeDependency</code> <p>The dependency that caused the exception.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeException(Exception):\n    \"\"\"\n    Base exception class for node-related errors.\n\n    Args:\n        failed_depend (NodeDependency, optional): The dependency that caused the exception. Defaults to None.\n        message (str, optional): Additional error message. Defaults to None.\n\n    Attributes:\n        failed_depend (NodeDependency): The dependency that caused the exception.\n    \"\"\"\n\n    def __init__(self, failed_depend: Optional[\"NodeDependency\"] = None, message=None):\n        super().__init__(message)\n        self.failed_depend = failed_depend\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeFailedException","title":"<code>NodeFailedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node fails to execute.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeFailedException(NodeException):\n    \"\"\"\n    Exception raised when a node fails to execute.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/exceptions/#dynamiq.nodes.exceptions.NodeSkippedException","title":"<code>NodeSkippedException</code>","text":"<p>               Bases: <code>NodeException</code></p> <p>Exception raised when a node is skipped during execution.</p> <p>This exception is a subclass of NodeException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/exceptions.py</code> <pre><code>class NodeSkippedException(NodeException):\n    \"\"\"\n    Exception raised when a node is skipped during execution.\n\n    This exception is a subclass of NodeException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/managers/","title":"Managers","text":""},{"location":"dynamiq/nodes/managers/#dynamiq.nodes.managers.NodeManager","title":"<code>NodeManager</code>","text":"<p>A class for managing and retrieving node types.</p> Source code in <code>dynamiq/nodes/managers.py</code> <pre><code>class NodeManager:\n    \"\"\"A class for managing and retrieving node types.\"\"\"\n\n    @staticmethod\n    def get_node_by_type(node_type: str) -&gt; type[Node]:\n        \"\"\"\n        Retrieves a node class based on the given node type.\n\n        Args:\n            node_type (str): The type of node to retrieve.\n\n        Returns:\n            type[Node]: The node class corresponding to the given type.\n\n        Raises:\n            ValueError: If the node type is not found.\n\n        Example:\n            &gt;&gt;&gt; node_class = NodeManager.get_node_by_type(\"LLM_OPENAI\")\n            &gt;&gt;&gt; isinstance(node_class, type(Node))\n            True\n        \"\"\"\n        try:\n            entity_module, entity_name = node_type.rsplit(\".\", 1)\n            imported_module = importlib.import_module(entity_module)\n            if entity := getattr(imported_module, entity_name, None):\n                return entity\n        except (ModuleNotFoundError, ImportError):\n            raise ValueError(f\"Node type {node_type} not found\")\n</code></pre>"},{"location":"dynamiq/nodes/managers/#dynamiq.nodes.managers.NodeManager.get_node_by_type","title":"<code>get_node_by_type(node_type)</code>  <code>staticmethod</code>","text":"<p>Retrieves a node class based on the given node type.</p> <p>Parameters:</p> Name Type Description Default <code>node_type</code> <code>str</code> <p>The type of node to retrieve.</p> required <p>Returns:</p> Type Description <code>type[Node]</code> <p>type[Node]: The node class corresponding to the given type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the node type is not found.</p> Example <p>node_class = NodeManager.get_node_by_type(\"LLM_OPENAI\") isinstance(node_class, type(Node)) True</p> Source code in <code>dynamiq/nodes/managers.py</code> <pre><code>@staticmethod\ndef get_node_by_type(node_type: str) -&gt; type[Node]:\n    \"\"\"\n    Retrieves a node class based on the given node type.\n\n    Args:\n        node_type (str): The type of node to retrieve.\n\n    Returns:\n        type[Node]: The node class corresponding to the given type.\n\n    Raises:\n        ValueError: If the node type is not found.\n\n    Example:\n        &gt;&gt;&gt; node_class = NodeManager.get_node_by_type(\"LLM_OPENAI\")\n        &gt;&gt;&gt; isinstance(node_class, type(Node))\n        True\n    \"\"\"\n    try:\n        entity_module, entity_name = node_type.rsplit(\".\", 1)\n        imported_module = importlib.import_module(entity_module)\n        if entity := getattr(imported_module, entity_name, None):\n            return entity\n    except (ModuleNotFoundError, ImportError):\n        raise ValueError(f\"Node type {node_type} not found\")\n</code></pre>"},{"location":"dynamiq/nodes/node/","title":"Node","text":""},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.CachingConfig","title":"<code>CachingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for node caching.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether caching is enabled for the node.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class CachingConfig(BaseModel):\n    \"\"\"\n    Configuration for node caching.\n\n    Attributes:\n        enabled (bool): Whether caching is enabled for the node.\n    \"\"\"\n    enabled: bool = False\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode","title":"<code>ConnectionNode</code>","text":"<p>               Bases: <code>Node</code>, <code>ABC</code></p> <p>Abstract base class for nodes that require a connection.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>BaseConnection | None</code> <p>The connection to use.</p> <code>client</code> <code>Any | None</code> <p>The client instance.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class ConnectionNode(Node, ABC):\n    \"\"\"\n    Abstract base class for nodes that require a connection.\n\n    Attributes:\n        connection (BaseConnection | None): The connection to use.\n        client (Any | None): The client instance.\n    \"\"\"\n\n    connection: BaseConnection | None = None\n    client: Any | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_connection_client(self):\n        \"\"\"Validate that either connection or client is specified.\"\"\"\n        if not self.client and not self.connection:\n            raise ValueError(\"'connection' or 'client' should be specified\")\n        return self\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize components for the node.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.client is None:\n            self.client = connection_manager.get_connection_client(\n                connection=self.connection\n            )\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize components for the node.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize components for the node.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.client is None:\n        self.client = connection_manager.get_connection_client(\n            connection=self.connection\n        )\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ConnectionNode.validate_connection_client","title":"<code>validate_connection_client()</code>","text":"<p>Validate that either connection or client is specified.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_connection_client(self):\n    \"\"\"Validate that either connection or client is specified.\"\"\"\n    if not self.client and not self.connection:\n        raise ValueError(\"'connection' or 'client' should be specified\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ErrorHandling","title":"<code>ErrorHandling</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for error handling in nodes.</p> <p>Attributes:</p> Name Type Description <code>timeout_seconds</code> <code>float | None</code> <p>Timeout in seconds for node execution.</p> <code>retry_interval_seconds</code> <code>float</code> <p>Interval between retries in seconds.</p> <code>max_retries</code> <code>int</code> <p>Maximum number of retries.</p> <code>backoff_rate</code> <code>float</code> <p>Rate of increase for retry intervals.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class ErrorHandling(BaseModel):\n    \"\"\"\n    Configuration for error handling in nodes.\n\n    Attributes:\n        timeout_seconds (float | None): Timeout in seconds for node execution.\n        retry_interval_seconds (float): Interval between retries in seconds.\n        max_retries (int): Maximum number of retries.\n        backoff_rate (float): Rate of increase for retry intervals.\n    \"\"\"\n    timeout_seconds: float | None = None\n    retry_interval_seconds: float = 1\n    max_retries: int = 0\n    backoff_rate: float = 1\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.InputTransformer","title":"<code>InputTransformer</code>","text":"<p>               Bases: <code>Transformer</code></p> <p>Input transformer for nodes.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class InputTransformer(Transformer):\n    \"\"\"Input transformer for nodes.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Runnable</code>, <code>ABC</code></p> <p>Abstract base class for all nodes in the workflow.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the node.</p> <code>name</code> <code>str | None</code> <p>Optional name for the node.</p> <code>group</code> <code>NodeGroup</code> <p>Group the node belongs to.</p> <code>description</code> <code>str | None</code> <p>Optional description for the node.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>input_transformer</code> <code>InputTransformer</code> <p>Input data transformer.</p> <code>output_transformer</code> <code>OutputTransformer</code> <p>Output data transformer.</p> <code>caching</code> <code>CachingConfig</code> <p>Caching configuration.</p> <code>depends</code> <code>list[NodeDependency]</code> <p>List of node dependencies.</p> <code>metadata</code> <code>NodeMetadata | None</code> <p>Optional metadata for the node.</p> <code>is_postponed_component_init</code> <code>bool</code> <p>Whether component initialization is postponed.</p> <code>is_optimized_for_agents</code> <code>bool</code> <p>Whether to optimize output for agents. By default is set to False.</p> <code>supports_files</code> <code>bool</code> <p>Whether the node has access to files. By default is set to False.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class Node(BaseModel, Runnable, ABC):\n    \"\"\"\n    Abstract base class for all nodes in the workflow.\n\n    Attributes:\n        id (str): Unique identifier for the node.\n        name (str | None): Optional name for the node.\n        group (NodeGroup): Group the node belongs to.\n        description (str | None): Optional description for the node.\n        error_handling (ErrorHandling): Error handling configuration.\n        input_transformer (InputTransformer): Input data transformer.\n        output_transformer (OutputTransformer): Output data transformer.\n        caching (CachingConfig): Caching configuration.\n        depends (list[NodeDependency]): List of node dependencies.\n        metadata (NodeMetadata | None): Optional metadata for the node.\n        is_postponed_component_init (bool): Whether component initialization is postponed.\n        is_optimized_for_agents (bool): Whether to optimize output for agents. By default is set to False.\n        supports_files (bool): Whether the node has access to files. By default is set to False.\n    \"\"\"\n    id: str = Field(default_factory=generate_uuid)\n    name: str | None = None\n    description: str | None = None\n    group: NodeGroup\n    error_handling: ErrorHandling = ErrorHandling()\n    input_transformer: InputTransformer = InputTransformer()\n    input_mapping: dict[str, Any] = {}\n    output_transformer: OutputTransformer = OutputTransformer()\n    caching: CachingConfig = CachingConfig()\n    streaming: StreamingConfig = StreamingConfig()\n    depends: list[NodeDependency] = []\n    metadata: NodeMetadata | None = None\n\n    is_postponed_component_init: bool = False\n    is_optimized_for_agents: bool = False\n    is_files_allowed: bool = False\n\n    _output_references: NodeOutputReferences = PrivateAttr()\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[BaseModel] | None] = None\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if not self.is_postponed_component_init:\n            self.init_components()\n\n        self._output_references = NodeOutputReferences(node=self)\n\n    @computed_field\n    @cached_property\n    def type(self) -&gt; str:\n        return f\"{self.__module__.rsplit('.', 1)[0]}.{self.__class__.__name__}\"\n\n    @staticmethod\n    def _validate_dependency_status(depend: NodeDependency, depends_result: dict[str, RunnableResult]):\n        \"\"\"\n        Validate the status of a dependency.\n\n        Args:\n            depend (NodeDependency): The dependency to validate.\n            depends_result (dict[str, RunnableResult]): Results of dependent nodes.\n\n        Raises:\n            NodeException: If the dependency result is missing.\n            NodeFailedException: If the dependency failed.\n            NodeSkippedException: If the dependency was skipped.\n        \"\"\"\n        if not (dep_result := depends_result.get(depend.node.id)):\n            raise NodeException(\n                failed_depend=depend,\n                message=f\"Dependency {depend.node.id}: result missed\",\n            )\n\n        if dep_result.status == RunnableStatus.FAILURE:\n            raise NodeFailedException(\n                failed_depend=depend, message=f\"Dependency {depend.node.id}: failed\"\n            )\n\n        if dep_result.status == RunnableStatus.SKIP:\n            raise NodeSkippedException(failed_depend=depend, message=f\"Dependency {depend.node.id}: skipped\")\n\n    @staticmethod\n    def _validate_dependency_condition(depend: NodeDependency, depends_result: dict[str, RunnableResult]):\n        \"\"\"\n        Validate the condition of a dependency.\n\n        Args:\n            depend (NodeDependency): The dependency to validate.\n            depends_result (dict[str, RunnableResult]): Results of dependent nodes.\n\n        Raises:\n            NodeConditionFailedException: If the dependency condition is not met.\n            NodeConditionSkippedException: If the dependency condition is skipped.\n        \"\"\"\n        if (\n            (dep_output_data := depends_result.get(depend.node.id))\n            and (isinstance(dep_output_data.output, dict))\n            and (dep_condition_result := dep_output_data.output.get(depend.option))\n        ):\n            if dep_condition_result.status == RunnableStatus.FAILURE:\n                raise NodeConditionFailedException(\n                    failed_depend=depend,\n                    message=f\"Dependency {depend.node.id} condition {depend.option}: result is false\",\n                )\n            if dep_condition_result.status == RunnableStatus.SKIP:\n                raise NodeConditionSkippedException(\n                    failed_depend=depend,\n                    message=f\"Dependency {depend.node.id} condition {depend.option}: skipped\",\n                )\n\n    @staticmethod\n    def _validate_input_mapping_value_func(func: Callable):\n        \"\"\"\n        Validate input mapping value function.\n\n        Args:\n            func (Callable): Input mapping value function.\n\n        Raises:\n            ValueError: If the function does not accept 'inputs' and 'outputs' or **kwargs.\n        \"\"\"\n        params = inspect.signature(func).parameters\n\n        # Check if the function accepts the at least 'inputs' and 'outputs' parameters\n        if len(params) &gt;= 2:\n            return\n\n        # Check if the function accepts **kwargs\n        elif params and list(params.values())[0].kind == inspect.Parameter.VAR_KEYWORD:\n            return\n\n        raise ValueError(f\"Input function '{func.__name__}' must accept parameters 'inputs' and 'outputs' or **kwargs.\")\n\n    def validate_depends(self, depends_result):\n        \"\"\"\n        Validate all dependencies of the node.\n\n        Args:\n            depends_result (dict): Results of dependent nodes.\n\n        Raises:\n            Various exceptions based on dependency validation results.\n        \"\"\"\n        for dep in self.depends:\n            self._validate_dependency_status(depend=dep, depends_result=depends_result)\n            if dep.option:\n                self._validate_dependency_condition(\n                    depend=dep, depends_result=depends_result\n                )\n\n    def validate_input_schema(self, input_data: dict[str, Any]) -&gt; dict[str, Any] | BaseModel:\n        \"\"\"\n        Validate input data against the input schema. Returns instance of input_schema if it is is provided.\n\n        Args:\n            input_data (Any): Input data to validate.\n\n        Raises:\n            NodeException: If input data does not match the input schema.\n        \"\"\"\n        from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n        if self.input_schema:\n            try:\n                return self.input_schema(**input_data)\n            except Exception as e:\n                raise RecoverableAgentException(f\"Input data validation failed: {e}\")\n\n        return input_data\n\n    def transform_input(self, input_data: dict, depends_result: dict[Any, RunnableResult]) -&gt; dict:\n        \"\"\"\n        Transform input data for the node.\n\n        Args:\n            input_data (dict): Input data for the node.\n            depends_result (dict): Results of dependent nodes.\n\n        Raises:\n            NodeException: If a dependency result is missing or input mapping fails.\n\n        Returns:\n            dict: Transformed input data.\n        \"\"\"\n        # Apply input transformer\n        if self.input_transformer.path or self.input_transformer.selector:\n            depends_result_as_dict = {k: result.to_depend_dict() for k, result in depends_result.items()}\n            inputs = self.transform(input_data | depends_result_as_dict, self.input_transformer, self.id)\n        else:\n            inputs = input_data | {k: result.to_tracing_depend_dict() for k, result in depends_result.items()}\n\n        # Apply input bindings\n        for key, value in self.input_mapping.items():\n            if isinstance(value, NodeOutputReference):\n                depend_result = depends_result.get(value.node.id)\n                if not depend_result:\n                    raise NodeException(message=f\"Dependency {value.node.id}: result not found.\")\n                if value.output_key not in depend_result.output:\n                    raise NodeException(message=f\"Dependency {value.node.id} output {value.output_key}: not found.\")\n\n                inputs[key] = depend_result.output[value.output_key]\n\n            elif callable(value):\n                try:\n                    inputs[key] = value(inputs, {d_id: result.output for d_id, result in depends_result.items()})\n                except Exception:\n                    raise NodeException(message=f\"Input mapping {key}: failed.\")\n            else:\n                inputs[key] = value\n\n        inputs = self.validate_input_schema(inputs)\n\n        return inputs\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize node components.\n\n        Args:\n            connection_manager (ConnectionManager, optional): Connection manager instance.\n                Defaults to ConnectionManager().\n        \"\"\"\n        self.is_postponed_component_init = False\n\n    @staticmethod\n    def transform(data: Any, transformer: Transformer, node_id: str) -&gt; Any:\n        \"\"\"\n        Apply transformation to data.\n\n        Args:\n            data (Any): Input data to transform.\n            transformer (Transformer): Transformer to apply.\n            node_id (str): ID of the node performing the transformation.\n\n        Returns:\n            Any: Transformed data.\n        \"\"\"\n        output = jsonpath_filter(data, transformer.path, node_id)\n        output = jsonpath_mapper(output, transformer.selector, node_id)\n        return output\n\n    def transform_output(self, output_data: Any) -&gt; Any:\n        \"\"\"\n        Transform output data from the node.\n\n        Args:\n            output_data (Any): Output data to transform.\n\n        Returns:\n            Any: Transformed output data.\n        \"\"\"\n        return self.transform(output_data, self.output_transformer, self.id)\n\n    @property\n    def to_dict_exclude_params(self):\n        return {\n            \"client\": True,\n            \"vector_store\": True,\n            \"connection\": {\"api_key\": True},\n            \"depends\": True,\n            \"input_mapping\": True,\n        }\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = self.model_dump(\n            exclude=kwargs.pop(\"exclude\", self.to_dict_exclude_params),\n            serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n            **kwargs,\n        )\n        data[\"depends\"] = [depend.to_dict(**kwargs) for depend in self.depends]\n        data[\"input_mapping\"] = format_value(self.input_mapping)\n        return data\n\n    def run(\n        self,\n        input_data: Any,\n        config: RunnableConfig = None,\n        depends_result: dict = None,\n        **kwargs,\n    ) -&gt; RunnableResult:\n        \"\"\"\n        Run the node with given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            depends_result (dict, optional): Results of dependent nodes. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the node execution.\n        \"\"\"\n        from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n\n        logger.info(f\"Node {self.name} - {self.id}: execution started.\")\n        time_start = datetime.now()\n\n        config = ensure_config(config)\n        run_id = uuid4()\n        merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"parent_run_id\": kwargs.get(\"parent_run_id\", run_id)})\n        if depends_result is None:\n            depends_result = {}\n\n        try:\n            self.validate_depends(depends_result)\n        except NodeException as e:\n            transformed_input = input_data | {\n                k: result.to_tracing_depend_dict() for k, result in depends_result.items()\n            }\n            skip_data = {\"failed_dependency\": e.failed_depend.to_dict()}\n            self.run_on_node_skip(\n                callbacks=config.callbacks,\n                skip_data=skip_data,\n                input_data=transformed_input,\n                **merged_kwargs,\n            )\n            logger.info(f\"Node {self.name} - {self.id}: execution skipped.\")\n            return RunnableResult(\n                status=RunnableStatus.SKIP,\n                input=transformed_input,\n                output=format_value(e),\n            )\n\n        try:\n            transformed_input = self.transform_input(input_data=input_data, depends_result=depends_result)\n\n            self.run_on_node_start(config.callbacks, transformed_input, **merged_kwargs)\n\n            cache = cache_wf_entity(\n                entity_id=self.id,\n                cache_enabled=self.caching.enabled,\n                cache_config=config.cache,\n            )\n\n            output, from_cache = cache(self.execute_with_retry)(\n                transformed_input, config, **merged_kwargs\n            )\n\n            merged_kwargs[\"is_output_from_cache\"] = from_cache\n            transformed_output = self.transform_output(output)\n            self.run_on_node_end(config.callbacks, transformed_output, **merged_kwargs)\n\n            logger.info(\n                f\"Node {self.name} - {self.id}: execution succeeded in \"\n                f\"{format_duration(time_start, datetime.now())}.\"\n            )\n            return RunnableResult(\n                status=RunnableStatus.SUCCESS,\n                input=transformed_input,\n                output=transformed_output,\n            )\n        except Exception as e:\n            self.run_on_node_error(config.callbacks, e, **merged_kwargs)\n            logger.error(\n                f\"Node {self.name} - {self.id}: execution failed in \"\n                f\"{format_duration(time_start, datetime.now())}.\"\n            )\n\n            recoverable = isinstance(e, RecoverableAgentException)\n            return RunnableResult(\n                status=RunnableStatus.FAILURE,\n                input=input_data,\n                output=format_value(e, recoverable=recoverable),\n            )\n\n    def execute_with_retry(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the node with retry logic.\n\n        Args:\n            input_data (dict[str, Any]): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Any: Result of the node execution.\n\n        Raises:\n            Exception: If all retry attempts fail.\n        \"\"\"\n        config = ensure_config(config)\n\n        error = None\n        n_attempt = self.error_handling.max_retries + 1\n        for attempt in range(n_attempt):\n            merged_kwargs = merge(kwargs, {\"execution_run_id\": uuid4()})\n\n            self.run_on_node_execute_start(config.callbacks, input_data, **merged_kwargs)\n\n            try:\n                output = self.execute_with_timeout(\n                    self.error_handling.timeout_seconds,\n                    input_data,\n                    config,\n                    **merged_kwargs,\n                )\n\n                self.run_on_node_execute_end(config.callbacks, output, **merged_kwargs)\n                return output\n            except TimeoutError as e:\n                error = e\n                self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n                logger.warning(f\"Node {self.name} - {self.id}: timeout.\")\n            except Exception as e:\n                error = e\n                self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n                logger.error(f\"Node {self.name} - {self.id}: execution error: {e}\")\n\n            # do not sleep after the last attempt\n            if attempt &lt; n_attempt - 1:\n                time_to_sleep = self.error_handling.retry_interval_seconds * (\n                    self.error_handling.backoff_rate**attempt\n                )\n                logger.info(\n                    f\"Node {self.name} - {self.id}: retrying in {time_to_sleep} seconds.\"\n                )\n                time.sleep(time_to_sleep)\n\n        logger.error(\n            f\"Node {self.name} - {self.id}: execution failed after {n_attempt} attempts.\"\n        )\n        raise error\n\n    def execute_with_timeout(\n        self,\n        timeout: float | None,\n        input_data: dict[str, Any] | BaseModel,\n        config: RunnableConfig = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Execute the node with a timeout.\n\n        Args:\n            timeout (float | None): Timeout duration in seconds.\n            input_data (dict[str, Any]): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Any: Result of the execution.\n\n        Raises:\n            Exception: If execution fails or times out.\n        \"\"\"\n        with ThreadPoolExecutor() as executor:\n            future = executor.submit(self.execute, input_data, config=config, **kwargs)\n\n            try:\n                result = future.result(timeout=timeout)\n            except Exception as e:\n                raise e\n\n            return result\n\n    def get_input_streaming_event(\n        self,\n        event_msg_type: \"type[StreamingEventMessage]\" = StreamingEventMessage,\n        event: str | None = None,\n        config: RunnableConfig = None,\n    ) -&gt; StreamingEventMessage:\n        \"\"\"\n        Get the input streaming event from the input streaming.\n\n        Args:\n            event_msg_type (Type[StreamingEventMessage], optional): The event message type to use.\n            event (str, optional): The event to use for the message.\n            config (RunnableConfig, optional): Configuration for the runnable.\n\n        \"\"\"\n        # Use runnable streaming configuration. If not found use node streaming configuration\n        streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n        if streaming.input_streaming_enabled:\n            while not streaming.input_queue_done_event or not streaming.input_queue_done_event.is_set():\n                try:\n                    data = streaming.input_queue.get(timeout=streaming.timeout)\n                except Empty:\n                    raise ValueError(f\"Input streaming timeout: {streaming.timeout} exceeded.\")\n\n                try:\n                    event_msg = event_msg_type.model_validate_json(data)\n                    if event and event_msg.event != event:\n                        raise ValueError()\n                except ValueError:\n                    logger.error(\n                        f\"Invalid streaming event data: {data}. \"\n                        f\"Allowed event: {event}, event_msg_type: {event_msg_type}\"\n                    )\n                    continue\n\n                return event_msg\n\n        raise ValueError(\"Input streaming is not enabled.\")\n\n    def run_on_node_start(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        input_data: dict[str, Any] | BaseModel,\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node start.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n\n        if isinstance(input_data, BaseModel):\n            input_data = dict(input_data)\n\n        for callback in callbacks:\n            callback.on_node_start(self.to_dict(), input_data, **kwargs)\n\n    def run_on_node_end(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        output_data: dict[str, Any],\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node end.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_end(self.model_dump(), output_data, **kwargs)\n\n    def run_on_node_error(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        error: BaseException,\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node error.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            error (BaseException): The error that occurred.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_error(self.to_dict(), error, **kwargs)\n\n    def run_on_node_skip(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        skip_data: dict[str, Any],\n        input_data: dict[str, Any],\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node skip.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            skip_data (dict[str, Any]): Data related to the skip.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_skip(self.to_dict(), skip_data, input_data, **kwargs)\n\n    def run_on_node_execute_start(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        input_data: dict[str, Any] | BaseModel,\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node execute start.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            input_data (dict[str, Any]): Input data for the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if isinstance(input_data, BaseModel):\n            input_data = dict(input_data)\n\n        for callback in callbacks:\n            callback.on_node_execute_start(self.to_dict(), input_data, **kwargs)\n\n    def run_on_node_execute_end(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        output_data: dict[str, Any],\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node execute end.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            output_data (dict[str, Any]): Output data from the node.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_execute_end(self.to_dict(), output_data, **kwargs)\n\n    def run_on_node_execute_error(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        error: BaseException,\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node execute error.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            error (BaseException): The error that occurred.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_execute_error(self.model_dump(), error, **kwargs)\n\n    def run_on_node_execute_run(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node execute run.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_execute_run(self.to_dict(), **kwargs)\n\n    def run_on_node_execute_stream(\n        self,\n        callbacks: list[BaseCallbackHandler],\n        chunk: dict[str, Any] | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Run callbacks on node execute stream.\n\n        Args:\n            callbacks (list[BaseCallbackHandler]): List of callback handlers.\n            chunk (dict[str, Any]): Chunk of streaming data.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        for callback in callbacks:\n            callback.on_node_execute_stream(self.to_dict(), chunk, **kwargs)\n\n    @abstractmethod\n    def execute(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs) -&gt; Any:\n        \"\"\"\n        Execute the node with the given input.\n\n        Args:\n            input_data (dict[str, Any]): Input data for the node.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Any: Result of the execution.\n        \"\"\"\n        pass\n\n    def depends_on(self, nodes: Union[\"Node\", list[\"Node\"]]):\n        \"\"\"\n        Add dependencies for this node. Accepts either a single node or a list of nodes.\n\n        Args:\n            nodes (Node or list[Node]): A single node or list of nodes this node depends on.\n\n        Raises:\n            TypeError: If the input is neither a Node nor a list of Node instances.\n            ValueError: If an empty list is provided.\n\n        Returns:\n            self: Enables method chaining.\n        \"\"\"\n\n        if nodes is None:\n            raise ValueError(\"Nodes cannot be None.\")\n\n        # If a single node is provided, convert it to a list\n        if isinstance(nodes, Node):\n            nodes = [nodes]\n\n        # Ensure the input is a list of Node instances\n        if not isinstance(nodes, list) or not all(isinstance(node, Node) for node in nodes):\n            raise TypeError(f\"Expected a Node or a list of Node instances, but got {type(nodes).__name__}.\")\n\n        if not nodes:\n            raise ValueError(\"Cannot add an empty list of dependencies.\")\n\n        # Add each node as a dependency\n        for node in nodes:\n            self.depends.append(NodeDependency(node))\n\n        return self  # enable chaining\n\n    def enable_streaming(self, event: str = STREAMING_EVENT):\n        \"\"\"\n        Enable streaming for the node and optionally set the event name.\n\n        Args:\n            event (str): The event name for streaming. Defaults to 'streaming'.\n\n        Returns:\n            self: Enables method chaining.\n        \"\"\"\n        self.streaming.enabled = True\n        self.streaming.event = event\n        return self\n\n    @property\n    def outputs(self):\n        \"\"\"\n        Provide the output references for the node.\n        \"\"\"\n        return self._output_references\n\n    def inputs(self, **kwargs):\n        \"\"\"\n        Add input mappings for the node.\n\n        Returns:\n            self: Enables method chaining.\n\n        Examples:\n            from dynamiq.nodes.llms import OpenAI\n\n            openai_1_node = OpenAI(...)\n            openai_2_node = OpenAI(...)\n            openai_3_node = OpenAI(...)\n\n            def merge_and_short_content(inputs: dict, outputs: dict[str, dict]):\n                return (\n                    f\"- {outputs[openai_1_node.id]['content'][:200]} \\n - {outputs[openai_2_node.id]['content'][:200]}\"\n                )\n\n            openai_4_node = (\n                OpenAI(\n                    ...\n                    prompt=prompts.Prompt(\n                        messages=[\n                            prompts.Message(\n                                role=\"user\",\n                                content=(\n                                    \"Please simplify that information for {{purpose}}:\\n\"\n                                    \"{{extra_instructions}}\\n\"\n                                    \"{{content}}\\n\"\n                                    \"{{extra_content}}\"\n                                ),\n                            )\n                        ],\n                    ),\n                )\n                .inputs(\n                    purpose=\"10 years old kids\",\n                    extra_instructions=\"Please return information in readable format.\",\n                    content=merge_and_short_content,\n                    extra_content=openai_3_node.outputs.content,\n                )\n                .depends_on([openai_1_node, openai_2_node, openai_3_node])\n            )\n        \"\"\"\n        for key, value in kwargs.items():\n            if callable(value):\n                self._validate_input_mapping_value_func(value)\n\n            self.input_mapping[key] = value\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.outputs","title":"<code>outputs</code>  <code>property</code>","text":"<p>Provide the output references for the node.</p>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.depends_on","title":"<code>depends_on(nodes)</code>","text":"<p>Add dependencies for this node. Accepts either a single node or a list of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Node or list[Node]</code> <p>A single node or list of nodes this node depends on.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is neither a Node nor a list of Node instances.</p> <code>ValueError</code> <p>If an empty list is provided.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>Enables method chaining.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def depends_on(self, nodes: Union[\"Node\", list[\"Node\"]]):\n    \"\"\"\n    Add dependencies for this node. Accepts either a single node or a list of nodes.\n\n    Args:\n        nodes (Node or list[Node]): A single node or list of nodes this node depends on.\n\n    Raises:\n        TypeError: If the input is neither a Node nor a list of Node instances.\n        ValueError: If an empty list is provided.\n\n    Returns:\n        self: Enables method chaining.\n    \"\"\"\n\n    if nodes is None:\n        raise ValueError(\"Nodes cannot be None.\")\n\n    # If a single node is provided, convert it to a list\n    if isinstance(nodes, Node):\n        nodes = [nodes]\n\n    # Ensure the input is a list of Node instances\n    if not isinstance(nodes, list) or not all(isinstance(node, Node) for node in nodes):\n        raise TypeError(f\"Expected a Node or a list of Node instances, but got {type(nodes).__name__}.\")\n\n    if not nodes:\n        raise ValueError(\"Cannot add an empty list of dependencies.\")\n\n    # Add each node as a dependency\n    for node in nodes:\n        self.depends.append(NodeDependency(node))\n\n    return self  # enable chaining\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.enable_streaming","title":"<code>enable_streaming(event=STREAMING_EVENT)</code>","text":"<p>Enable streaming for the node and optionally set the event name.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>str</code> <p>The event name for streaming. Defaults to 'streaming'.</p> <code>STREAMING_EVENT</code> <p>Returns:</p> Name Type Description <code>self</code> <p>Enables method chaining.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def enable_streaming(self, event: str = STREAMING_EVENT):\n    \"\"\"\n    Enable streaming for the node and optionally set the event name.\n\n    Args:\n        event (str): The event name for streaming. Defaults to 'streaming'.\n\n    Returns:\n        self: Enables method chaining.\n    \"\"\"\n    self.streaming.enabled = True\n    self.streaming.event = event\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute the node with the given input.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of the execution.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>@abstractmethod\ndef execute(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs) -&gt; Any:\n    \"\"\"\n    Execute the node with the given input.\n\n    Args:\n        input_data (dict[str, Any]): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Result of the execution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.execute_with_retry","title":"<code>execute_with_retry(input_data, config=None, **kwargs)</code>","text":"<p>Execute the node with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>Result of the node execution.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If all retry attempts fail.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def execute_with_retry(self, input_data: dict[str, Any] | BaseModel, config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the node with retry logic.\n\n    Args:\n        input_data (dict[str, Any]): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Result of the node execution.\n\n    Raises:\n        Exception: If all retry attempts fail.\n    \"\"\"\n    config = ensure_config(config)\n\n    error = None\n    n_attempt = self.error_handling.max_retries + 1\n    for attempt in range(n_attempt):\n        merged_kwargs = merge(kwargs, {\"execution_run_id\": uuid4()})\n\n        self.run_on_node_execute_start(config.callbacks, input_data, **merged_kwargs)\n\n        try:\n            output = self.execute_with_timeout(\n                self.error_handling.timeout_seconds,\n                input_data,\n                config,\n                **merged_kwargs,\n            )\n\n            self.run_on_node_execute_end(config.callbacks, output, **merged_kwargs)\n            return output\n        except TimeoutError as e:\n            error = e\n            self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n            logger.warning(f\"Node {self.name} - {self.id}: timeout.\")\n        except Exception as e:\n            error = e\n            self.run_on_node_execute_error(config.callbacks, error, **merged_kwargs)\n            logger.error(f\"Node {self.name} - {self.id}: execution error: {e}\")\n\n        # do not sleep after the last attempt\n        if attempt &lt; n_attempt - 1:\n            time_to_sleep = self.error_handling.retry_interval_seconds * (\n                self.error_handling.backoff_rate**attempt\n            )\n            logger.info(\n                f\"Node {self.name} - {self.id}: retrying in {time_to_sleep} seconds.\"\n            )\n            time.sleep(time_to_sleep)\n\n    logger.error(\n        f\"Node {self.name} - {self.id}: execution failed after {n_attempt} attempts.\"\n    )\n    raise error\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.execute_with_timeout","title":"<code>execute_with_timeout(timeout, input_data, config=None, **kwargs)</code>","text":"<p>Execute the node with a timeout.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float | None</code> <p>Timeout duration in seconds.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>Result of the execution.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If execution fails or times out.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def execute_with_timeout(\n    self,\n    timeout: float | None,\n    input_data: dict[str, Any] | BaseModel,\n    config: RunnableConfig = None,\n    **kwargs,\n):\n    \"\"\"\n    Execute the node with a timeout.\n\n    Args:\n        timeout (float | None): Timeout duration in seconds.\n        input_data (dict[str, Any]): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Result of the execution.\n\n    Raises:\n        Exception: If execution fails or times out.\n    \"\"\"\n    with ThreadPoolExecutor() as executor:\n        future = executor.submit(self.execute, input_data, config=config, **kwargs)\n\n        try:\n            result = future.result(timeout=timeout)\n        except Exception as e:\n            raise e\n\n        return result\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.get_input_streaming_event","title":"<code>get_input_streaming_event(event_msg_type=StreamingEventMessage, event=None, config=None)</code>","text":"<p>Get the input streaming event from the input streaming.</p> <p>Parameters:</p> Name Type Description Default <code>event_msg_type</code> <code>Type[StreamingEventMessage]</code> <p>The event message type to use.</p> <code>StreamingEventMessage</code> <code>event</code> <code>str</code> <p>The event to use for the message.</p> <code>None</code> <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def get_input_streaming_event(\n    self,\n    event_msg_type: \"type[StreamingEventMessage]\" = StreamingEventMessage,\n    event: str | None = None,\n    config: RunnableConfig = None,\n) -&gt; StreamingEventMessage:\n    \"\"\"\n    Get the input streaming event from the input streaming.\n\n    Args:\n        event_msg_type (Type[StreamingEventMessage], optional): The event message type to use.\n        event (str, optional): The event to use for the message.\n        config (RunnableConfig, optional): Configuration for the runnable.\n\n    \"\"\"\n    # Use runnable streaming configuration. If not found use node streaming configuration\n    streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n    if streaming.input_streaming_enabled:\n        while not streaming.input_queue_done_event or not streaming.input_queue_done_event.is_set():\n            try:\n                data = streaming.input_queue.get(timeout=streaming.timeout)\n            except Empty:\n                raise ValueError(f\"Input streaming timeout: {streaming.timeout} exceeded.\")\n\n            try:\n                event_msg = event_msg_type.model_validate_json(data)\n                if event and event_msg.event != event:\n                    raise ValueError()\n            except ValueError:\n                logger.error(\n                    f\"Invalid streaming event data: {data}. \"\n                    f\"Allowed event: {event}, event_msg_type: {event_msg_type}\"\n                )\n                continue\n\n            return event_msg\n\n    raise ValueError(\"Input streaming is not enabled.\")\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize node components.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>Connection manager instance. Defaults to ConnectionManager().</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize node components.\n\n    Args:\n        connection_manager (ConnectionManager, optional): Connection manager instance.\n            Defaults to ConnectionManager().\n    \"\"\"\n    self.is_postponed_component_init = False\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.inputs","title":"<code>inputs(**kwargs)</code>","text":"<pre><code>    Add input mappings for the node.\n\n    Returns:\n        self: Enables method chaining.\n\n    Examples:\n        from dynamiq.nodes.llms import OpenAI\n\n        openai_1_node = OpenAI(...)\n        openai_2_node = OpenAI(...)\n        openai_3_node = OpenAI(...)\n\n        def merge_and_short_content(inputs: dict, outputs: dict[str, dict]):\n            return (\n                f\"- {outputs[openai_1_node.id]['content'][:200]}\n</code></pre> <ul> <li>{outputsopenai_2_node.id[:200]}\"                 )<pre><code>    openai_4_node = (\n        OpenAI(\n            ...\n            prompt=prompts.Prompt(\n                messages=[\n                    prompts.Message(\n                        role=\"user\",\n                        content=(\n                            \"Please simplify that information for {{purpose}}:\n</code></pre> <p>\"                                 \"{{extra_instructions}} \"                                 \"{{content}} \"                                 \"{{extra_content}}\"                             ),                         )                     ],                 ),             )             .inputs(                 purpose=\"10 years old kids\",                 extra_instructions=\"Please return information in readable format.\",                 content=merge_and_short_content,                 extra_content=openai_3_node.outputs.content,             )             .depends_on([openai_1_node, openai_2_node, openai_3_node])         )</p> </li> </ul> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def inputs(self, **kwargs):\n    \"\"\"\n    Add input mappings for the node.\n\n    Returns:\n        self: Enables method chaining.\n\n    Examples:\n        from dynamiq.nodes.llms import OpenAI\n\n        openai_1_node = OpenAI(...)\n        openai_2_node = OpenAI(...)\n        openai_3_node = OpenAI(...)\n\n        def merge_and_short_content(inputs: dict, outputs: dict[str, dict]):\n            return (\n                f\"- {outputs[openai_1_node.id]['content'][:200]} \\n - {outputs[openai_2_node.id]['content'][:200]}\"\n            )\n\n        openai_4_node = (\n            OpenAI(\n                ...\n                prompt=prompts.Prompt(\n                    messages=[\n                        prompts.Message(\n                            role=\"user\",\n                            content=(\n                                \"Please simplify that information for {{purpose}}:\\n\"\n                                \"{{extra_instructions}}\\n\"\n                                \"{{content}}\\n\"\n                                \"{{extra_content}}\"\n                            ),\n                        )\n                    ],\n                ),\n            )\n            .inputs(\n                purpose=\"10 years old kids\",\n                extra_instructions=\"Please return information in readable format.\",\n                content=merge_and_short_content,\n                extra_content=openai_3_node.outputs.content,\n            )\n            .depends_on([openai_1_node, openai_2_node, openai_3_node])\n        )\n    \"\"\"\n    for key, value in kwargs.items():\n        if callable(value):\n            self._validate_input_mapping_value_func(value)\n\n        self.input_mapping[key] = value\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run","title":"<code>run(input_data, config=None, depends_result=None, **kwargs)</code>","text":"<p>Run the node with given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the node execution.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run(\n    self,\n    input_data: Any,\n    config: RunnableConfig = None,\n    depends_result: dict = None,\n    **kwargs,\n) -&gt; RunnableResult:\n    \"\"\"\n    Run the node with given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the node.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        depends_result (dict, optional): Results of dependent nodes. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the node execution.\n    \"\"\"\n    from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n\n    logger.info(f\"Node {self.name} - {self.id}: execution started.\")\n    time_start = datetime.now()\n\n    config = ensure_config(config)\n    run_id = uuid4()\n    merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"parent_run_id\": kwargs.get(\"parent_run_id\", run_id)})\n    if depends_result is None:\n        depends_result = {}\n\n    try:\n        self.validate_depends(depends_result)\n    except NodeException as e:\n        transformed_input = input_data | {\n            k: result.to_tracing_depend_dict() for k, result in depends_result.items()\n        }\n        skip_data = {\"failed_dependency\": e.failed_depend.to_dict()}\n        self.run_on_node_skip(\n            callbacks=config.callbacks,\n            skip_data=skip_data,\n            input_data=transformed_input,\n            **merged_kwargs,\n        )\n        logger.info(f\"Node {self.name} - {self.id}: execution skipped.\")\n        return RunnableResult(\n            status=RunnableStatus.SKIP,\n            input=transformed_input,\n            output=format_value(e),\n        )\n\n    try:\n        transformed_input = self.transform_input(input_data=input_data, depends_result=depends_result)\n\n        self.run_on_node_start(config.callbacks, transformed_input, **merged_kwargs)\n\n        cache = cache_wf_entity(\n            entity_id=self.id,\n            cache_enabled=self.caching.enabled,\n            cache_config=config.cache,\n        )\n\n        output, from_cache = cache(self.execute_with_retry)(\n            transformed_input, config, **merged_kwargs\n        )\n\n        merged_kwargs[\"is_output_from_cache\"] = from_cache\n        transformed_output = self.transform_output(output)\n        self.run_on_node_end(config.callbacks, transformed_output, **merged_kwargs)\n\n        logger.info(\n            f\"Node {self.name} - {self.id}: execution succeeded in \"\n            f\"{format_duration(time_start, datetime.now())}.\"\n        )\n        return RunnableResult(\n            status=RunnableStatus.SUCCESS,\n            input=transformed_input,\n            output=transformed_output,\n        )\n    except Exception as e:\n        self.run_on_node_error(config.callbacks, e, **merged_kwargs)\n        logger.error(\n            f\"Node {self.name} - {self.id}: execution failed in \"\n            f\"{format_duration(time_start, datetime.now())}.\"\n        )\n\n        recoverable = isinstance(e, RecoverableAgentException)\n        return RunnableResult(\n            status=RunnableStatus.FAILURE,\n            input=input_data,\n            output=format_value(e, recoverable=recoverable),\n        )\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_end","title":"<code>run_on_node_end(callbacks, output_data, **kwargs)</code>","text":"<p>Run callbacks on node end.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_end(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    output_data: dict[str, Any],\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node end.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_end(self.model_dump(), output_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_error","title":"<code>run_on_node_error(callbacks, error, **kwargs)</code>","text":"<p>Run callbacks on node error.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_error(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    error: BaseException,\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node error.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        error (BaseException): The error that occurred.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_error(self.to_dict(), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_end","title":"<code>run_on_node_execute_end(callbacks, output_data, **kwargs)</code>","text":"<p>Run callbacks on node execute end.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>output_data</code> <code>dict[str, Any]</code> <p>Output data from the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_end(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    output_data: dict[str, Any],\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node execute end.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        output_data (dict[str, Any]): Output data from the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_execute_end(self.to_dict(), output_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_error","title":"<code>run_on_node_execute_error(callbacks, error, **kwargs)</code>","text":"<p>Run callbacks on node execute error.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_error(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    error: BaseException,\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node execute error.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        error (BaseException): The error that occurred.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_execute_error(self.model_dump(), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_run","title":"<code>run_on_node_execute_run(callbacks, **kwargs)</code>","text":"<p>Run callbacks on node execute run.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_run(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node execute run.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_execute_run(self.to_dict(), **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_start","title":"<code>run_on_node_execute_start(callbacks, input_data, **kwargs)</code>","text":"<p>Run callbacks on node execute start.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_start(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    input_data: dict[str, Any] | BaseModel,\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node execute start.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if isinstance(input_data, BaseModel):\n        input_data = dict(input_data)\n\n    for callback in callbacks:\n        callback.on_node_execute_start(self.to_dict(), input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_execute_stream","title":"<code>run_on_node_execute_stream(callbacks, chunk=None, **kwargs)</code>","text":"<p>Run callbacks on node execute stream.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>chunk</code> <code>dict[str, Any]</code> <p>Chunk of streaming data.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_execute_stream(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    chunk: dict[str, Any] | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node execute stream.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        chunk (dict[str, Any]): Chunk of streaming data.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_execute_stream(self.to_dict(), chunk, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_skip","title":"<code>run_on_node_skip(callbacks, skip_data, input_data, **kwargs)</code>","text":"<p>Run callbacks on node skip.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>skip_data</code> <code>dict[str, Any]</code> <p>Data related to the skip.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_skip(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    skip_data: dict[str, Any],\n    input_data: dict[str, Any],\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node skip.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        skip_data (dict[str, Any]): Data related to the skip.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    for callback in callbacks:\n        callback.on_node_skip(self.to_dict(), skip_data, input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.run_on_node_start","title":"<code>run_on_node_start(callbacks, input_data, **kwargs)</code>","text":"<p>Run callbacks on node start.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> required <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the node.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def run_on_node_start(\n    self,\n    callbacks: list[BaseCallbackHandler],\n    input_data: dict[str, Any] | BaseModel,\n    **kwargs,\n):\n    \"\"\"\n    Run callbacks on node start.\n\n    Args:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        input_data (dict[str, Any]): Input data for the node.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    if isinstance(input_data, BaseModel):\n        input_data = dict(input_data)\n\n    for callback in callbacks:\n        callback.on_node_start(self.to_dict(), input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = self.model_dump(\n        exclude=kwargs.pop(\"exclude\", self.to_dict_exclude_params),\n        serialize_as_any=kwargs.pop(\"serialize_as_any\", True),\n        **kwargs,\n    )\n    data[\"depends\"] = [depend.to_dict(**kwargs) for depend in self.depends]\n    data[\"input_mapping\"] = format_value(self.input_mapping)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.transform","title":"<code>transform(data, transformer, node_id)</code>  <code>staticmethod</code>","text":"<p>Apply transformation to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data to transform.</p> required <code>transformer</code> <code>Transformer</code> <p>Transformer to apply.</p> required <code>node_id</code> <code>str</code> <p>ID of the node performing the transformation.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Transformed data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>@staticmethod\ndef transform(data: Any, transformer: Transformer, node_id: str) -&gt; Any:\n    \"\"\"\n    Apply transformation to data.\n\n    Args:\n        data (Any): Input data to transform.\n        transformer (Transformer): Transformer to apply.\n        node_id (str): ID of the node performing the transformation.\n\n    Returns:\n        Any: Transformed data.\n    \"\"\"\n    output = jsonpath_filter(data, transformer.path, node_id)\n    output = jsonpath_mapper(output, transformer.selector, node_id)\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.transform_input","title":"<code>transform_input(input_data, depends_result)</code>","text":"<p>Transform input data for the node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input data for the node.</p> required <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes.</p> required <p>Raises:</p> Type Description <code>NodeException</code> <p>If a dependency result is missing or input mapping fails.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Transformed input data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def transform_input(self, input_data: dict, depends_result: dict[Any, RunnableResult]) -&gt; dict:\n    \"\"\"\n    Transform input data for the node.\n\n    Args:\n        input_data (dict): Input data for the node.\n        depends_result (dict): Results of dependent nodes.\n\n    Raises:\n        NodeException: If a dependency result is missing or input mapping fails.\n\n    Returns:\n        dict: Transformed input data.\n    \"\"\"\n    # Apply input transformer\n    if self.input_transformer.path or self.input_transformer.selector:\n        depends_result_as_dict = {k: result.to_depend_dict() for k, result in depends_result.items()}\n        inputs = self.transform(input_data | depends_result_as_dict, self.input_transformer, self.id)\n    else:\n        inputs = input_data | {k: result.to_tracing_depend_dict() for k, result in depends_result.items()}\n\n    # Apply input bindings\n    for key, value in self.input_mapping.items():\n        if isinstance(value, NodeOutputReference):\n            depend_result = depends_result.get(value.node.id)\n            if not depend_result:\n                raise NodeException(message=f\"Dependency {value.node.id}: result not found.\")\n            if value.output_key not in depend_result.output:\n                raise NodeException(message=f\"Dependency {value.node.id} output {value.output_key}: not found.\")\n\n            inputs[key] = depend_result.output[value.output_key]\n\n        elif callable(value):\n            try:\n                inputs[key] = value(inputs, {d_id: result.output for d_id, result in depends_result.items()})\n            except Exception:\n                raise NodeException(message=f\"Input mapping {key}: failed.\")\n        else:\n            inputs[key] = value\n\n    inputs = self.validate_input_schema(inputs)\n\n    return inputs\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.transform_output","title":"<code>transform_output(output_data)</code>","text":"<p>Transform output data from the node.</p> <p>Parameters:</p> Name Type Description Default <code>output_data</code> <code>Any</code> <p>Output data to transform.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Transformed output data.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def transform_output(self, output_data: Any) -&gt; Any:\n    \"\"\"\n    Transform output data from the node.\n\n    Args:\n        output_data (Any): Output data to transform.\n\n    Returns:\n        Any: Transformed output data.\n    \"\"\"\n    return self.transform(output_data, self.output_transformer, self.id)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.validate_depends","title":"<code>validate_depends(depends_result)</code>","text":"<p>Validate all dependencies of the node.</p> <p>Parameters:</p> Name Type Description Default <code>depends_result</code> <code>dict</code> <p>Results of dependent nodes.</p> required Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def validate_depends(self, depends_result):\n    \"\"\"\n    Validate all dependencies of the node.\n\n    Args:\n        depends_result (dict): Results of dependent nodes.\n\n    Raises:\n        Various exceptions based on dependency validation results.\n    \"\"\"\n    for dep in self.depends:\n        self._validate_dependency_status(depend=dep, depends_result=depends_result)\n        if dep.option:\n            self._validate_dependency_condition(\n                depend=dep, depends_result=depends_result\n            )\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Node.validate_input_schema","title":"<code>validate_input_schema(input_data)</code>","text":"<p>Validate input data against the input schema. Returns instance of input_schema if it is is provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data to validate.</p> required <p>Raises:</p> Type Description <code>NodeException</code> <p>If input data does not match the input schema.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def validate_input_schema(self, input_data: dict[str, Any]) -&gt; dict[str, Any] | BaseModel:\n    \"\"\"\n    Validate input data against the input schema. Returns instance of input_schema if it is is provided.\n\n    Args:\n        input_data (Any): Input data to validate.\n\n    Raises:\n        NodeException: If input data does not match the input schema.\n    \"\"\"\n    from dynamiq.nodes.agents.exceptions import RecoverableAgentException\n    if self.input_schema:\n        try:\n            return self.input_schema(**input_data)\n        except Exception as e:\n            raise RecoverableAgentException(f\"Input data validation failed: {e}\")\n\n    return input_data\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeDependency","title":"<code>NodeDependency</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a dependency between nodes.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The dependent node.</p> <code>option</code> <code>str | None</code> <p>Optional condition for the dependency.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeDependency(BaseModel):\n    \"\"\"\n    Represents a dependency between nodes.\n\n    Attributes:\n        node (Node): The dependent node.\n        option (str | None): Optional condition for the dependency.\n    \"\"\"\n    node: \"Node\"\n    option: str | None = None\n\n    def __init__(self, node: \"Node\", option: str | None = None):\n        super().__init__(node=node, option=option)\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        return {\n            \"node\": self.node.to_dict(**kwargs),\n            \"option\": self.option,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeDependency.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    return {\n        \"node\": self.node.to_dict(**kwargs),\n        \"option\": self.option,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeMetadata","title":"<code>NodeMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a node.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str | None</code> <p>Optional label for the node.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeMetadata(BaseModel):\n    \"\"\"\n    Metadata for a node.\n\n    Attributes:\n        label (str | None): Optional label for the node.\n    \"\"\"\n    label: str | None = None\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeOutputReference","title":"<code>NodeOutputReference</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a reference to a node output.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The node to reference.</p> <code>output_key</code> <code>str</code> <p>Key for the output.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeOutputReference(BaseModel):\n    \"\"\"\n    Represents a reference to a node output.\n\n    Attributes:\n        node (Node): The node to reference.\n        output_key (str): Key for the output.\n    \"\"\"\n\n    node: \"Node\"\n    output_key: str\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeOutputReferences","title":"<code>NodeOutputReferences</code>","text":"<p>Provides output references for a node.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The node to provide output references for.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeOutputReferences:\n    \"\"\"\n    Provides output references for a node.\n\n    Attributes:\n        node (Node): The node to provide output references for.\n    \"\"\"\n\n    def __init__(self, node: \"Node\"):\n        self.node = node\n\n    def __getattr__(self, key: Any):\n        return NodeOutputReference(node=self.node, output_key=key)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.NodeReadyToRun","title":"<code>NodeReadyToRun</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a node ready to run with its input data and dependencies.</p> <p>Attributes:</p> Name Type Description <code>node</code> <code>Node</code> <p>The node to be run.</p> <code>is_ready</code> <code>bool</code> <p>Whether the node is ready to run.</p> <code>input_data</code> <code>Any</code> <p>Input data for the node.</p> <code>depends_result</code> <code>dict[str, Any]</code> <p>Results of dependent nodes.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class NodeReadyToRun(BaseModel):\n    \"\"\"\n    Represents a node ready to run with its input data and dependencies.\n\n    Attributes:\n        node (Node): The node to be run.\n        is_ready (bool): Whether the node is ready to run.\n        input_data (Any): Input data for the node.\n        depends_result (dict[str, Any]): Results of dependent nodes.\n    \"\"\"\n    node: \"Node\"\n    is_ready: bool\n    input_data: Any = None\n    depends_result: dict[str, Any] = {}\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.OutputTransformer","title":"<code>OutputTransformer</code>","text":"<p>               Bases: <code>InputTransformer</code></p> <p>Output transformer for nodes.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class OutputTransformer(InputTransformer):\n    \"\"\"Output transformer for nodes.\"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for input and output transformers.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str | None</code> <p>JSONPath for data selection.</p> <code>selector</code> <code>dict[str, str] | None</code> <p>Mapping for data transformation.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class Transformer(BaseModel):\n    \"\"\"\n    Base class for input and output transformers.\n\n    Attributes:\n        path (str | None): JSONPath for data selection.\n        selector (dict[str, str] | None): Mapping for data transformation.\n    \"\"\"\n    path: str | None = None\n    selector: dict[str, str] | None = None\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.VectorStoreNode","title":"<code>VectorStoreNode</code>","text":"<p>               Bases: <code>ConnectionNode</code>, <code>BaseVectorStoreParams</code>, <code>ABC</code></p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>class VectorStoreNode(ConnectionNode, BaseVectorStoreParams, ABC):\n    vector_store: Any | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_connection_client(self):\n        if not self.vector_store and not self.connection:\n            raise ValueError(\"'connection' or 'vector_store' should be specified\")\n        return self\n\n    @property\n    @abstractmethod\n    def vector_store_cls(self):\n        raise NotImplementedError\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(BaseVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def connect_to_vector_store(self):\n        vector_store_params = self.vector_store_params\n        vector_store = self.vector_store_cls(**vector_store_params)\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: connected to {self.vector_store_cls.__name__} vector store with\"\n            f\" {vector_store_params}\"\n        )\n\n        return vector_store\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize components for the node.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n        \"\"\"\n        # Use vector_store client if it is already initialized\n        if self.vector_store:\n            self.client = self.vector_store.client\n\n        super().init_components(connection_manager)\n\n        if self.vector_store is None:\n            self.vector_store = self.connect_to_vector_store()\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.VectorStoreNode.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize components for the node.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize components for the node.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n    \"\"\"\n    # Use vector_store client if it is already initialized\n    if self.vector_store:\n        self.client = self.vector_store.client\n\n    super().init_components(connection_manager)\n\n    if self.vector_store is None:\n        self.vector_store = self.connect_to_vector_store()\n</code></pre>"},{"location":"dynamiq/nodes/node/#dynamiq.nodes.node.ensure_config","title":"<code>ensure_config(config=None)</code>","text":"<p>Ensure that a valid RunnableConfig is provided.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RunnableConfig</code> <p>The input configuration. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RunnableConfig</code> <code>RunnableConfig</code> <p>A valid RunnableConfig object.</p> Source code in <code>dynamiq/nodes/node.py</code> <pre><code>def ensure_config(config: RunnableConfig = None) -&gt; RunnableConfig:\n    \"\"\"\n    Ensure that a valid RunnableConfig is provided.\n\n    Args:\n        config (RunnableConfig, optional): The input configuration. Defaults to None.\n\n    Returns:\n        RunnableConfig: A valid RunnableConfig object.\n    \"\"\"\n    if config is None:\n        return RunnableConfig(callbacks=[])\n\n    return config\n</code></pre>"},{"location":"dynamiq/nodes/types/","title":"Types","text":""},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.InferenceMode","title":"<code>InferenceMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of inference types.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class InferenceMode(Enum):\n    \"\"\"\n    Enumeration of inference types.\n    \"\"\"\n\n    DEFAULT = \"DEFAULT\"\n    XML = \"XML\"\n    FUNCTION_CALLING = \"FUNCTION_CALLING\"\n    STRUCTURED_OUTPUT = \"STRUCTURED_OUTPUT\"\n</code></pre>"},{"location":"dynamiq/nodes/types/#dynamiq.nodes.types.NodeGroup","title":"<code>NodeGroup</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of node groups that categorize different types of nodes.</p> <p>Each group represents a collection of related node types, providing a higher-level classification of the system's components.</p> Source code in <code>dynamiq/nodes/types.py</code> <pre><code>class NodeGroup(Enum):\n    \"\"\"\n    Enumeration of node groups that categorize different types of nodes.\n\n    Each group represents a collection of related node types, providing a higher-level\n    classification of the system's components.\n    \"\"\"\n\n    LLMS = \"llms\"\n    OPERATORS = \"operators\"\n    EMBEDDERS = \"embedders\"\n    RANKERS = \"rankers\"\n    CONVERTERS = \"converters\"\n    RETRIEVERS = \"retrievers\"\n    SPLITTERS = \"splitters\"\n    WRITERS = \"writers\"\n    UTILS = \"utils\"\n    TOOLS = \"tools\"\n    AGENTS = \"agents\"\n    AUDIO = \"audio\"\n    VALIDATORS = \"validators\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/","title":"Base","text":""},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>Node</code></p> <p>Base class for an AI Agent that interacts with a Language Model and tools.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class Agent(Node):\n    \"\"\"Base class for an AI Agent that interacts with a Language Model and tools.\"\"\"\n\n    DEFAULT_INTRODUCTION: ClassVar[str] = (\n        \"You are a helpful AI assistant designed to assist users with various tasks and queries.\"\n        \"Your goal is to provide accurate, helpful, and friendly responses to the best of your abilities.\"\n    )\n    DEFAULT_DATE: ClassVar[str] = datetime.now().strftime(\"%d %B %Y\")\n\n    llm: Node = Field(..., description=\"LLM used by the agent.\")\n    group: NodeGroup = NodeGroup.AGENTS\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    tools: list[Node] = []\n    files: list[io.BytesIO | bytes] | None = None\n    name: str = \"AI Agent\"\n    role: str | None = None\n    max_loops: int = 1\n    memory: Memory | None = Field(None, description=\"Memory node for the agent.\")\n    memory_retrieval_strategy: str = \"all\"  # all, relevant, both\n\n    _prompt_blocks: dict[str, str] = PrivateAttr(default_factory=dict)\n    _prompt_variables: dict[str, Any] = PrivateAttr(default_factory=dict)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._intermediate_steps: dict[int, dict] = {}\n        self._run_depends: list[dict] = []\n        self._init_prompt_blocks()\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"llm\": True, \"tools\": True, \"memory\": True, \"files\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        data[\"tools\"] = [tool.to_dict(**kwargs) for tool in self.tools]\n        if self.files:\n            data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n        return data\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"Initialize components for the manager and agents.\"\"\"\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n        for tool in self.tools:\n            if tool.is_postponed_component_init:\n                tool.init_components(connection_manager)\n            tool.is_optimized_for_agents = True\n\n    def sanitize_tool_name(self, s: str):\n        \"\"\"Sanitize tool name to follow [^a-zA-Z0-9_-].\"\"\"\n        s = s.replace(\" \", \"-\")\n        sanitized = re.sub(r\"[^a-zA-Z0-9_-]\", \"\", s)\n        return sanitized\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initializes default prompt blocks and variables.\"\"\"\n        self._prompt_blocks = {\n            \"introduction\": self.DEFAULT_INTRODUCTION,\n            \"role\": self.role or \"\",\n            \"date\": self.DEFAULT_DATE,\n            \"tools\": \"{tool_description}\",\n            \"files\": \"{file_description}\",\n            \"instructions\": \"\",\n            \"output_format\": \"\",\n            \"relevant_information\": \"{relevant_memory}\",\n            \"conversation_history\": \"{context}\",\n            \"request\": \"User request: {input}\",\n        }\n        self._prompt_variables = {\n            \"tool_description\": self.tool_description,\n            \"file_description\": self.file_description,\n            \"user_input\": \"\",\n            \"context\": \"\",\n            \"relevant_memory\": \"\",\n        }\n\n    def add_block(self, block_name: str, content: str):\n        \"\"\"Adds or updates a prompt block.\"\"\"\n        self._prompt_blocks[block_name] = content\n\n    def set_prompt_variable(self, variable_name: str, value: Any):\n        \"\"\"Sets or updates a prompt variable.\"\"\"\n        self._prompt_variables[variable_name] = value\n\n    def _retrieve_chat_history(self, messages: list[Message]) -&gt; str:\n        \"\"\"Converts a list of messages to a formatted string.\"\"\"\n        return \"\\n\".join([f\"**{msg.role.value}:** {msg.content}\" for msg in messages])\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the agent with the given input data.\n        \"\"\"\n        logger.debug(f\"Agent {self.name} - {self.id}: started with input {input_data}\")\n        self.reset_run_state()\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        user_id = input_data.get(\"user_id\", None)\n        session_id = input_data.get(\"session_id\", None)\n        custom_metadata = input_data.get(\"metadata\", {}).copy()\n        custom_metadata.update({k: v for k, v in input_data.items() if k not in [\"user_id\", \"session_id\", \"input\"]})\n        metadata = {**custom_metadata, \"user_id\": user_id, \"session_id\": session_id}\n        chat_history = input_data.get(\"chat_history\", None)\n\n        if chat_history:\n            try:\n                logger.debug(f\"Agent {self.name} - {self.id}: Chat history provided\")\n                chat_history = TypeAdapter(list[Message]).validate_python(chat_history)\n                chat_history = self._retrieve_chat_history(chat_history)\n                logger.debug(f\"Agent {self.name} - {self.id}: Chat history: {len(chat_history)}\")\n                self._prompt_variables[\"context\"] = chat_history\n\n            except ValidationError as e:\n                raise TypeError(f\"Invalid chat history: {e}\")\n\n        if self.memory:\n            self.memory.add(role=MessageRole.USER, content=input_data.get(\"input\"), metadata=metadata)\n            self._retrieve_memory(input_data)\n\n        files = input_data.get(\"files\", [])\n        if files:\n            self.files = files\n            self._prompt_variables[\"file_description\"] = self.file_description\n\n        self._prompt_variables.update(input_data)\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n\n        result = self._run_agent(config=config, **kwargs)\n        if self.memory:\n            self.memory.add(role=MessageRole.ASSISTANT, content=result, metadata=metadata)\n\n        execution_result = {\n            \"content\": result,\n            \"intermediate_steps\": self._intermediate_steps,\n        }\n\n        logger.debug(f\"Agent {self.name} - {self.id}: finished with result {result}\")\n        return execution_result\n\n    def _retrieve_memory(self, input_data):\n        \"\"\"\n        Retrieves memory based on the selected strategy: 'relevant', 'all', or 'both'.\n        \"\"\"\n        user_id = input_data.get(\"user_id\", None)\n        filters = {\"user_id\": user_id} if user_id else None\n\n        if self.memory_retrieval_strategy == \"relevant\":\n            relevant_memory = self.memory.get_search_results_as_string(query=input_data.get(\"input\"), filters=filters)\n            self._prompt_variables[\"relevant_memory\"] = relevant_memory\n\n        elif self.memory_retrieval_strategy == \"all\":\n            context = self.memory.get_all_messages_as_string()\n            self._prompt_variables[\"context\"] = context\n\n        elif self.memory_retrieval_strategy == \"both\":\n            relevant_memory = self.memory.get_search_results_as_string(query=input_data.get(\"input\"), filters=filters)\n            context = self.memory.get_all_messages_as_string()\n            self._prompt_variables[\"relevant_memory\"] = relevant_memory\n            self._prompt_variables[\"context\"] = context\n\n    def _run_llm(self, prompt: str, config: RunnableConfig | None = None, **kwargs) -&gt; str:\n        \"\"\"Runs the LLM with a given prompt and handles streaming or full responses.\"\"\"\n        logger.debug(f\"Agent {self.name} - {self.id}: Running LLM with prompt:\\n{prompt}\")\n        try:\n            llm_result = self.llm.run(\n                input_data={},\n                config=config,\n                prompt=Prompt(messages=[Message(role=\"user\", content=prompt)]),\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n            logger.debug(f\"Agent {self.name} - {self.id}: RAW LLM result:\\n{llm_result.output['content']}\")\n            if llm_result.status != RunnableStatus.SUCCESS:\n                raise ValueError(\"LLM execution failed\")\n\n            return llm_result.output[\"content\"]\n\n        except Exception as e:\n            logger.error(f\"Agent {self.name} - {self.id}: LLM execution failed: {str(e)}\")\n            raise\n\n    def stream_content(self, content: str, source: str, step: str, config: RunnableConfig | None = None, **kwargs):\n        if self.streaming.by_tokens:\n            return self.stream_by_tokens(content=content, source=source, step=step, config=config, **kwargs)\n        return self.stream_response(content=content, source=source, step=step, config=config, **kwargs)\n\n    def stream_by_tokens(self, content: str, source: str, step: str, config: RunnableConfig | None = None, **kwargs):\n        \"\"\"Streams the input content to the callbacks.\"\"\"\n        tokens = content.split(\" \")\n        final_response = []\n        for token in tokens:\n            final_response.append(token)\n            token_with_prefix = \" \" + token\n            token_for_stream = StreamChunk(\n                choices=[\n                    StreamChunkChoice(delta=StreamChunkChoiceDelta(content=token_with_prefix, source=source, step=step))\n                ]\n            )\n            logger.debug(f\"Agent {self.name} - {self.id}: Streaming token: {token_for_stream}\")\n            self.run_on_node_execute_stream(\n                callbacks=config.callbacks,\n                chunk=token_for_stream.model_dump(),\n                **kwargs,\n            )\n        return \" \".join(final_response)\n\n    def stream_response(self, content: str, source: str, step: str, config: RunnableConfig | None = None, **kwargs):\n        response_for_stream = StreamChunk(\n            choices=[StreamChunkChoice(delta=StreamChunkChoiceDelta(content=content, source=source, step=step))]\n        )\n        logger.debug(f\"Agent {self.name} - {self.id}: Streaming response: {response_for_stream}\")\n\n        self.run_on_node_execute_stream(\n            callbacks=config.callbacks,\n            chunk=response_for_stream.model_dump(),\n            **kwargs,\n        )\n        return content\n\n    def _run_agent(self, config: RunnableConfig | None = None, **kwargs) -&gt; str:\n        \"\"\"Runs the agent with the generated prompt and handles exceptions.\"\"\"\n        formatted_prompt = self.generate_prompt()\n        try:\n            logger.info(f\"Streaming config  {self.streaming}\")\n            llm_result = self._run_llm(formatted_prompt, config=config, **kwargs)\n            if self.streaming.enabled:\n                return self.stream_content(\n                    content=llm_result,\n                    source=self.name,\n                    step=\"answer\",\n                    config=config,\n                    **kwargs,\n                )\n            return llm_result\n\n        except Exception as e:\n            logger.error(f\"Agent {self.name} - {self.id}: failed with error: {str(e)}\")\n            raise e\n\n    def _parse_action(self, output: str) -&gt; tuple[str | None, str | None]:\n        \"\"\"Parses the action and its input from the output string.\"\"\"\n        try:\n            action_match = re.search(\n                r\"Action:\\s*(.*?)\\nAction Input:\\s*(({\\n)?.*?)(?:[^}]*$)\",\n                output,\n                re.DOTALL,\n            )\n            if action_match:\n                action = action_match.group(1).strip()\n                action_input = action_match.group(2).strip()\n                if \"```json\" in action_input:\n                    action_input = action_input.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n\n                action_input = json.loads(action_input)\n                return action, action_input\n            else:\n                raise ActionParsingException()\n        except Exception as e:\n            logger.error(f\"Error parsing action: {e}\")\n            raise ActionParsingException(\n                (\n                    \"Error: Unable to parse action and action input.\"\n                    \"Please rewrite using the correct Action/Action Input format\"\n                    \"with action input as a valid dictionary.\"\n                    \"Ensure all quotes are included.\"\n                ),\n                recoverable=True,\n            )\n\n    def _extract_final_answer(self, output: str) -&gt; str:\n        \"\"\"Extracts the final answer from the output string.\"\"\"\n        match = re.search(r\"Answer:\\s*(.*)\", output, re.DOTALL)\n        return match.group(1).strip() if match else \"\"\n\n    def _get_tool(self, action: str) -&gt; Node:\n        \"\"\"Retrieves the tool corresponding to the given action.\"\"\"\n        tool = self.tool_by_names.get(action)\n        if not tool:\n            raise AgentUnknownToolException(\n                f\"Unknown tool: {action}.\"\n                \"Use only available tools and provide only the tool's name in the action field. \"\n                \"Do not include any additional reasoning. \"\n                \"Please correct the action field or state that you cannot answer the question.\"\n            )\n        return tool\n\n    def _run_tool(self, tool: Node, tool_input: str, config, **kwargs) -&gt; Any:\n        \"\"\"Runs a specific tool with the given input.\"\"\"\n        logger.debug(f\"Agent {self.name} - {self.id}: Running tool '{tool.name}'\")\n        if self.files:\n            if tool.is_files_allowed is True:\n                tool_input[\"files\"] = self.files\n\n        tool_result = tool.run(\n            input_data=tool_input,\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=tool).to_dict()]\n        if tool_result.status != RunnableStatus.SUCCESS:\n            logger.error({tool_result.output[\"content\"]})\n            if tool_result.output[\"recoverable\"]:\n                raise ToolExecutionException({tool_result.output[\"content\"]})\n            else:\n                raise ValueError({tool_result.output[\"content\"]})\n        return tool_result.output[\"content\"]\n\n    @property\n    def tool_description(self) -&gt; str:\n        \"\"\"Returns a description of the tools available to the agent.\"\"\"\n        return (\n            \"\\n\".join(\n                [f\"{tool.name}: {tool.description.strip()}\" for tool in self.tools]\n            )\n            if self.tools\n            else \"\"\n        )\n\n    @property\n    def file_description(self) -&gt; str:\n        \"\"\"Returns a description of the files available to the agent.\"\"\"\n        if self.files:\n            file_description = \"You can work with the following files:\\n\"\n            for file in self.files:\n                name = getattr(file, \"name\", \"Unnamed file\")\n                description = getattr(file, \"description\", \"No description\")\n                file_description += f\"&lt;file&gt;: {name} - {description} &lt;\\\\file&gt;\\n\"\n            return file_description\n        return \"\"\n\n    @property\n    def tool_names(self) -&gt; str:\n        \"\"\"Returns a comma-separated list of tool names available to the agent.\"\"\"\n        return \",\".join([self.sanitize_tool_name(tool.name) for tool in self.tools])\n\n    @property\n    def tool_by_names(self) -&gt; dict[str, Node]:\n        \"\"\"Returns a dictionary mapping tool names to their corresponding Node objects.\"\"\"\n        return {self.sanitize_tool_name(tool.name): tool for tool in self.tools}\n\n    def reset_run_state(self):\n        \"\"\"Resets the agent's run state.\"\"\"\n        self._intermediate_steps = {}\n        self._run_depends = []\n\n    def generate_prompt(self, block_names: list[str] | None = None, **kwargs) -&gt; str:\n        \"\"\"Generates the prompt using specified blocks and variables.\"\"\"\n        temp_variables = self._prompt_variables.copy()\n        temp_variables.update(kwargs)\n        prompt = \"\"\n        for block, content in self._prompt_blocks.items():\n            if block_names is None or block in block_names:\n                if content:\n                    formatted_content = content.format(**temp_variables)\n                    prompt += f\"{block.upper()}:\\n{formatted_content}\\n\\n\"\n\n        prompt = textwrap.dedent(prompt)\n        lines = prompt.splitlines()\n        stripped_lines = [line.strip() for line in lines if line.strip()]\n        prompt = \"\\n\".join(stripped_lines)\n        prompt = \"\\n\".join(\" \".join(line.split()) for line in prompt.split(\"\\n\"))\n        return prompt\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.file_description","title":"<code>file_description: str</code>  <code>property</code>","text":"<p>Returns a description of the files available to the agent.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.tool_by_names","title":"<code>tool_by_names: dict[str, Node]</code>  <code>property</code>","text":"<p>Returns a dictionary mapping tool names to their corresponding Node objects.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.tool_description","title":"<code>tool_description: str</code>  <code>property</code>","text":"<p>Returns a description of the tools available to the agent.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.tool_names","title":"<code>tool_names: str</code>  <code>property</code>","text":"<p>Returns a comma-separated list of tool names available to the agent.</p>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.add_block","title":"<code>add_block(block_name, content)</code>","text":"<p>Adds or updates a prompt block.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def add_block(self, block_name: str, content: str):\n    \"\"\"Adds or updates a prompt block.\"\"\"\n    self._prompt_blocks[block_name] = content\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the agent with the given input data.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the agent with the given input data.\n    \"\"\"\n    logger.debug(f\"Agent {self.name} - {self.id}: started with input {input_data}\")\n    self.reset_run_state()\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    user_id = input_data.get(\"user_id\", None)\n    session_id = input_data.get(\"session_id\", None)\n    custom_metadata = input_data.get(\"metadata\", {}).copy()\n    custom_metadata.update({k: v for k, v in input_data.items() if k not in [\"user_id\", \"session_id\", \"input\"]})\n    metadata = {**custom_metadata, \"user_id\": user_id, \"session_id\": session_id}\n    chat_history = input_data.get(\"chat_history\", None)\n\n    if chat_history:\n        try:\n            logger.debug(f\"Agent {self.name} - {self.id}: Chat history provided\")\n            chat_history = TypeAdapter(list[Message]).validate_python(chat_history)\n            chat_history = self._retrieve_chat_history(chat_history)\n            logger.debug(f\"Agent {self.name} - {self.id}: Chat history: {len(chat_history)}\")\n            self._prompt_variables[\"context\"] = chat_history\n\n        except ValidationError as e:\n            raise TypeError(f\"Invalid chat history: {e}\")\n\n    if self.memory:\n        self.memory.add(role=MessageRole.USER, content=input_data.get(\"input\"), metadata=metadata)\n        self._retrieve_memory(input_data)\n\n    files = input_data.get(\"files\", [])\n    if files:\n        self.files = files\n        self._prompt_variables[\"file_description\"] = self.file_description\n\n    self._prompt_variables.update(input_data)\n    kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    kwargs.pop(\"run_depends\", None)\n\n    result = self._run_agent(config=config, **kwargs)\n    if self.memory:\n        self.memory.add(role=MessageRole.ASSISTANT, content=result, metadata=metadata)\n\n    execution_result = {\n        \"content\": result,\n        \"intermediate_steps\": self._intermediate_steps,\n    }\n\n    logger.debug(f\"Agent {self.name} - {self.id}: finished with result {result}\")\n    return execution_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.generate_prompt","title":"<code>generate_prompt(block_names=None, **kwargs)</code>","text":"<p>Generates the prompt using specified blocks and variables.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def generate_prompt(self, block_names: list[str] | None = None, **kwargs) -&gt; str:\n    \"\"\"Generates the prompt using specified blocks and variables.\"\"\"\n    temp_variables = self._prompt_variables.copy()\n    temp_variables.update(kwargs)\n    prompt = \"\"\n    for block, content in self._prompt_blocks.items():\n        if block_names is None or block in block_names:\n            if content:\n                formatted_content = content.format(**temp_variables)\n                prompt += f\"{block.upper()}:\\n{formatted_content}\\n\\n\"\n\n    prompt = textwrap.dedent(prompt)\n    lines = prompt.splitlines()\n    stripped_lines = [line.strip() for line in lines if line.strip()]\n    prompt = \"\\n\".join(stripped_lines)\n    prompt = \"\\n\".join(\" \".join(line.split()) for line in prompt.split(\"\\n\"))\n    return prompt\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize components for the manager and agents.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"Initialize components for the manager and agents.\"\"\"\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n\n    for tool in self.tools:\n        if tool.is_postponed_component_init:\n            tool.init_components(connection_manager)\n        tool.is_optimized_for_agents = True\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Resets the agent's run state.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"Resets the agent's run state.\"\"\"\n    self._intermediate_steps = {}\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.sanitize_tool_name","title":"<code>sanitize_tool_name(s)</code>","text":"<p>Sanitize tool name to follow [^a-zA-Z0-9_-].</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def sanitize_tool_name(self, s: str):\n    \"\"\"Sanitize tool name to follow [^a-zA-Z0-9_-].\"\"\"\n    s = s.replace(\" \", \"-\")\n    sanitized = re.sub(r\"[^a-zA-Z0-9_-]\", \"\", s)\n    return sanitized\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.set_prompt_variable","title":"<code>set_prompt_variable(variable_name, value)</code>","text":"<p>Sets or updates a prompt variable.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def set_prompt_variable(self, variable_name: str, value: Any):\n    \"\"\"Sets or updates a prompt variable.\"\"\"\n    self._prompt_variables[variable_name] = value\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.stream_by_tokens","title":"<code>stream_by_tokens(content, source, step, config=None, **kwargs)</code>","text":"<p>Streams the input content to the callbacks.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def stream_by_tokens(self, content: str, source: str, step: str, config: RunnableConfig | None = None, **kwargs):\n    \"\"\"Streams the input content to the callbacks.\"\"\"\n    tokens = content.split(\" \")\n    final_response = []\n    for token in tokens:\n        final_response.append(token)\n        token_with_prefix = \" \" + token\n        token_for_stream = StreamChunk(\n            choices=[\n                StreamChunkChoice(delta=StreamChunkChoiceDelta(content=token_with_prefix, source=source, step=step))\n            ]\n        )\n        logger.debug(f\"Agent {self.name} - {self.id}: Streaming token: {token_for_stream}\")\n        self.run_on_node_execute_stream(\n            callbacks=config.callbacks,\n            chunk=token_for_stream.model_dump(),\n            **kwargs,\n        )\n    return \" \".join(final_response)\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.Agent.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    data[\"tools\"] = [tool.to_dict(**kwargs) for tool in self.tools]\n    if self.files:\n        data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager","title":"<code>AgentManager</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Manager class that extends the Agent class to include specific actions.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class AgentManager(Agent):\n    \"\"\"Manager class that extends the Agent class to include specific actions.\"\"\"\n\n    _actions: dict[str, Callable] = PrivateAttr(default_factory=dict)\n    name: str = \"Manager Agent\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._init_actions()\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"_actions\"] = {\n            k: getattr(action, \"__name__\", str(action))\n            for k, action in self._actions.items()\n        }\n        return data\n\n    def _init_actions(self):\n        \"\"\"Initializes the default actions for the manager.\"\"\"\n        self._actions = {\"plan\": self._plan, \"assign\": self._assign, \"final\": self._final}\n\n    def add_action(self, name: str, action: Callable):\n        \"\"\"Adds a custom action to the manager.\"\"\"\n        self._actions[name] = action\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Executes the manager agent with the given input data and action.\"\"\"\n        self.reset_run_state()\n        config = config or RunnableConfig()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        logger.info(\n            f\"AgentManager {self.name} - {self.id}: started with input {input_data}\"\n        )\n\n        action = input_data.get(\"action\")\n        if not action or action not in self._actions:\n            raise InvalidActionException(\n                f\"Invalid or missing action: {action}. Please select an action from {self._actions}.\"  # nosec: B608\n            )\n\n        self._prompt_variables.update(input_data)\n\n        kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        kwargs.pop(\"run_depends\", None)\n\n        _result_llm = self._actions[action](config=config, **kwargs)\n        result = {\"action\": action, \"result\": _result_llm}\n\n        execution_result = {\n            \"content\": result,\n            \"intermediate_steps\": self._intermediate_steps,\n        }\n\n        logger.debug(\n            f\"AgentManager {self.name} - {self.id}: finished with result {result}\"\n        )\n        return execution_result\n\n    def _plan(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'plan' action.\"\"\"\n        prompt = self.generate_prompt(block_names=[\"plan\"])\n        llm_result = self._run_llm(prompt, config, **kwargs)\n        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n            return self.stream_content(content=llm_result, step=\"reasoning\", source=self.name, config=config, **kwargs)\n        return llm_result\n\n    def _assign(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'assign' action.\"\"\"\n        prompt = self.generate_prompt(block_names=[\"assign\"])\n        llm_result = self._run_llm(prompt, config, **kwargs)\n        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n            return self.stream_content(content=llm_result, step=\"reasoning\", source=self.name, config=config, **kwargs)\n        return llm_result\n\n    def _final(self, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"Executes the 'final' action.\"\"\"\n        prompt = self.generate_prompt(block_names=[\"final\"])\n        llm_result = self._run_llm(prompt, config, **kwargs)\n        if self.streaming.enabled:\n            return self.stream_content(content=llm_result, step=\"answer\", source=self.name, config=config, **kwargs)\n        return llm_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.add_action","title":"<code>add_action(name, action)</code>","text":"<p>Adds a custom action to the manager.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def add_action(self, name: str, action: Callable):\n    \"\"\"Adds a custom action to the manager.\"\"\"\n    self._actions[name] = action\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the manager agent with the given input data and action.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Executes the manager agent with the given input data and action.\"\"\"\n    self.reset_run_state()\n    config = config or RunnableConfig()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    logger.info(\n        f\"AgentManager {self.name} - {self.id}: started with input {input_data}\"\n    )\n\n    action = input_data.get(\"action\")\n    if not action or action not in self._actions:\n        raise InvalidActionException(\n            f\"Invalid or missing action: {action}. Please select an action from {self._actions}.\"  # nosec: B608\n        )\n\n    self._prompt_variables.update(input_data)\n\n    kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    kwargs.pop(\"run_depends\", None)\n\n    _result_llm = self._actions[action](config=config, **kwargs)\n    result = {\"action\": action, \"result\": _result_llm}\n\n    execution_result = {\n        \"content\": result,\n        \"intermediate_steps\": self._intermediate_steps,\n    }\n\n    logger.debug(\n        f\"AgentManager {self.name} - {self.id}: finished with result {result}\"\n    )\n    return execution_result\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentManager.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"_actions\"] = {\n        k: getattr(action, \"__name__\", str(action))\n        for k, action in self._actions.items()\n    }\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.AgentStatus","title":"<code>AgentStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the status of an agent's execution.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class AgentStatus(Enum):\n    \"\"\"Represents the status of an agent's execution.\"\"\"\n\n    SUCCESS = \"success\"\n    FAIL = \"fail\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunk","title":"<code>StreamChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for streaming chunks with choices containing delta updates.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class StreamChunk(BaseModel):\n    \"\"\"Model for streaming chunks with choices containing delta updates.\"\"\"\n\n    choices: list[StreamChunkChoice]\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunkChoice","title":"<code>StreamChunkChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Stream chunk choice model.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class StreamChunkChoice(BaseModel):\n    \"\"\"Stream chunk choice model.\"\"\"\n\n    delta: StreamChunkChoiceDelta\n</code></pre>"},{"location":"dynamiq/nodes/agents/base/#dynamiq.nodes.agents.base.StreamChunkChoiceDelta","title":"<code>StreamChunkChoiceDelta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Delta model for content chunks.</p> Source code in <code>dynamiq/nodes/agents/base.py</code> <pre><code>class StreamChunkChoiceDelta(BaseModel):\n    \"\"\"Delta model for content chunks.\"\"\"\n    content: str | dict\n    source: str\n    step: str\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/","title":"Exceptions","text":""},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.ActionParsingException","title":"<code>ActionParsingException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when an action cannot be parsed. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class ActionParsingException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when an action cannot be parsed. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.AgentUnknownToolException","title":"<code>AgentUnknownToolException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when a unknown tool is requested. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class AgentUnknownToolException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when a unknown tool is requested. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.InvalidActionException","title":"<code>InvalidActionException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when invalid action is chosen. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class InvalidActionException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when invalid action is chosen. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.MaxLoopsExceededException","title":"<code>MaxLoopsExceededException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when the agent exceeds the maximum number of allowed loops.</p> <p>This exception is recoverable, meaning the agent can continue after catching this exception.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class MaxLoopsExceededException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when the agent exceeds the maximum number of allowed loops.\n\n    This exception is recoverable, meaning the agent can continue after catching this exception.\n    \"\"\"\n\n    def __init__(\n        self, message: str = \"Maximum number of loops reached without finding a final answer.\", recoverable: bool = True\n    ):\n        super().__init__(message, recoverable=recoverable)\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.RecoverableAgentException","title":"<code>RecoverableAgentException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for recoverable agent errors.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class RecoverableAgentException(Exception):\n    \"\"\"\n    Base exception class for recoverable agent errors.\n    \"\"\"\n\n    def __init__(self, *args, recoverable: bool = True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.recoverable = recoverable\n</code></pre>"},{"location":"dynamiq/nodes/agents/exceptions/#dynamiq.nodes.agents.exceptions.ToolExecutionException","title":"<code>ToolExecutionException</code>","text":"<p>               Bases: <code>RecoverableAgentException</code></p> <p>Exception raised when a tools fails to execute. Raising this exeption will allow Agent to reiterate.</p> <p>This exception is a subclass of AgentException and inherits its attributes and methods.</p> Source code in <code>dynamiq/nodes/agents/exceptions.py</code> <pre><code>class ToolExecutionException(RecoverableAgentException):\n    \"\"\"\n    Exception raised when a tools fails to execute. Raising this exeption will allow Agent to reiterate.\n\n    This exception is a subclass of AgentException and inherits its attributes and methods.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/","title":"React","text":""},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent","title":"<code>ReActAgent</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Agent that uses the ReAct strategy for processing tasks by interacting with tools in a loop.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>class ReActAgent(Agent):\n    \"\"\"Agent that uses the ReAct strategy for processing tasks by interacting with tools in a loop.\"\"\"\n\n    name: str = \"React\"\n    max_loops: int = Field(default=15, ge=2)\n    inference_mode: InferenceMode = InferenceMode.DEFAULT\n    behaviour_on_max_loops: Behavior = Field(\n        default=Behavior.RAISE,\n        description=\"Define behavior when max loops are exceeded. Options are 'raise' or 'return'.\",\n    )\n    format_schema: list = []\n\n    @model_validator(mode=\"after\")\n    def validate_inference_mode(self):\n        \"\"\"Validate whether specified model can be inferenced in provided mode.\"\"\"\n        match self.inference_mode:\n            case InferenceMode.FUNCTION_CALLING:\n                if not supports_function_calling(model=self.llm.model):\n                    raise ValueError(f\"Model {self.llm.model} does not support function calling\")\n\n            case InferenceMode.STRUCTURED_OUTPUT:\n                params = get_supported_openai_params(model=self.llm.model)\n                if \"response_format\" not in params:\n                    raise ValueError(f\"Model {self.llm.model} does not support structured output\")\n\n        return self\n\n    def parse_xml_content(self, text: str, tag: str) -&gt; str:\n        \"\"\"Extract content from XML-like tags.\"\"\"\n        match = re.search(f\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\", text, re.DOTALL)\n        return match.group(1).strip() if match else \"\"\n\n    def parse_xml_and_extract_info(self, text: str) -&gt; dict[str, Any]:\n        \"\"\"Parse XML-like structure and extract action and action_input.\"\"\"\n        output_content = self.parse_xml_content(text, \"output\")\n        action = self.parse_xml_content(output_content, \"action\")\n        action_input_text = self.parse_xml_content(output_content, \"action_input\")\n\n        try:\n            action_input = json.loads(action_input_text)\n        except json.JSONDecodeError:\n            raise ActionParsingException(\n                (\n                    \"Error: Unable to parse action and action input. \"\n                    \"Please rewrite in the correct XML format with action_input as a valid dictionary.\"\n                ),\n                recoverable=True,\n            )\n\n        return action, action_input\n\n    def extract_output_and_answer_xml(self, text: str) -&gt; dict[str, str]:\n        \"\"\"Extract output and answer from XML-like structure.\"\"\"\n        output = self.parse_xml_content(text, \"output\")\n        answer = self.parse_xml_content(text, \"answer\")\n\n        logger.debug(f\"Extracted output: {output}\")\n        logger.debug(f\"Extracted answer: {answer}\")\n\n        return {\"output\": output, \"answer\": answer}\n\n    def tracing_final(self, loop_num, final_answer, config, kwargs):\n        self._intermediate_steps[loop_num][\"final_answer\"] = final_answer\n\n    def tracing_intermediate(self, loop_num, formatted_prompt, llm_generated_output):\n        self._intermediate_steps[loop_num] = AgentIntermediateStep(\n            input_data={\"prompt\": formatted_prompt},\n            model_observation=AgentIntermediateStepModelObservation(\n                initial=llm_generated_output,\n            ),\n        ).model_dump()\n\n    def convert_string_to_dict(string: str) -&gt; dict:\n        string = re.sub(r'\\\\+', r'\\\\', string)\n        return json.loads(string)\n\n    def _run_agent(self, config: RunnableConfig | None = None, **kwargs) -&gt; str:\n        \"\"\"\n        Executes the ReAct strategy by iterating through thought, action, and observation cycles.\n        Args:\n            config (RunnableConfig | None): Configuration for the agent run.\n            **kwargs: Additional parameters for running the agent.\n        Returns:\n            str: Final answer provided by the agent.\n        Raises:\n            RuntimeError: If the maximum number of loops is reached without finding a final answer.\n            Exception: If an error occurs during execution.\n        \"\"\"\n\n        logger.info(f\"Agent {self.name} - {self.id}: Running ReAct strategy\")\n        previous_responses = []\n\n        for loop_num in range(self.max_loops):\n            formatted_prompt = self.generate_prompt(\n                user_request=kwargs.get(\"input\", \"\"),\n                tools_desc=self.tool_description,\n                tools_name=self.tool_names,\n                context=\"\\n\".join(previous_responses),\n                input_formats=self.generate_input_formats(self.tools)\n            )\n            logger.info(f\"Agent {self.name} - {self.id}: Loop {loop_num + 1} started.\")\n\n            logger.debug(f\"Agent {self.name} - {self.id}: Loop {loop_num + 1}. Prompt:\\n{formatted_prompt}\")\n\n            try:\n\n                llm_result = self.llm.run(\n                    input_data={},\n                    config=config,\n                    prompt=Prompt(messages=[Message(role=\"user\", content=formatted_prompt)]),\n                    run_depends=self._run_depends,\n                    schema=self.format_schema,\n                    inference_mode=self.inference_mode,\n                    **kwargs,\n                )\n                self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n\n                if llm_result.status != RunnableStatus.SUCCESS:\n                    logger.error(\n                        f\"Agent {self.name} - {self.id}: Loop {loop_num + 1} LLM execution failed. \"\n                        f\"Error output: {llm_result.output}\"\n                    )\n                    previous_responses.append(llm_result.output[\"content\"])\n                    continue\n\n                action, action_input = None, None\n                llm_generated_output = \"\"\n                match self.inference_mode:\n                    case InferenceMode.DEFAULT:\n                        llm_generated_output = llm_result.output[\"content\"]\n                        logger.debug(\n                            f\"Agent {self.name} - {self.id}:Loop {loop_num + 1}. \"\n                            f\"RAW LLM output:n{llm_generated_output}\"\n                        )\n                        self.tracing_intermediate(loop_num, formatted_prompt, llm_generated_output)\n                        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n\n                            self.stream_content(\n                                content=llm_generated_output,\n                                source=self.name,\n                                step=f\"reasoning_{loop_num + 1}\",\n                                config=config,\n                                **kwargs,\n                            )\n                        if \"Answer:\" in llm_generated_output:\n                            final_answer = self._extract_final_answer(llm_generated_output)\n                            self.tracing_final(loop_num, final_answer, config, kwargs)\n                            if self.streaming.enabled:\n                                self.stream_content(\n                                    content=final_answer,\n                                    source=self.name,\n                                    step=\"answer\",\n                                    config=config,\n                                    **kwargs,\n                                )\n                            return final_answer\n\n                        action, action_input = self._parse_action(llm_generated_output)\n\n                    case InferenceMode.FUNCTION_CALLING:\n                        action = llm_result.output[\"tool_calls\"][0][\"function\"][\"name\"].strip()\n                        llm_generated_output_json = json.loads(\n                            llm_result.output[\"tool_calls\"][0][\"function\"][\"arguments\"]\n                        )\n                        llm_generated_output = json.dumps(llm_generated_output_json)\n                        self.tracing_intermediate(loop_num, formatted_prompt, llm_generated_output)\n                        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n                            self.stream_content(\n                                content=llm_generated_output,\n                                source=self.name,\n                                step=f\"reasoning_{loop_num + 1}\",\n                                config=config,\n                                **kwargs,\n                            )\n\n                        if action == \"provide_final_answer\":\n                            final_answer = llm_generated_output_json[\"answer\"]\n                            self.tracing_final(loop_num, final_answer, config, kwargs)\n                            if self.streaming.enabled:\n                                self.stream_content(\n                                    content=final_answer,\n                                    source=self.name,\n                                    step=\"answer\",\n                                    config=config,\n                                    **kwargs,\n                                )\n                            return final_answer\n\n                        action_input = llm_generated_output_json[\"action_input\"]\n                        logger.debug(\n                            f\"Agent {self.name} - {self.id}:Loop {loop_num + 1}. \"\n                            f\"RAW LLM output:n{llm_generated_output}\"\n                        )\n                    case InferenceMode.STRUCTURED_OUTPUT:\n                        llm_generated_output_json = json.loads(llm_result.output[\"content\"])\n                        action = llm_generated_output_json[\"action\"]\n\n                        self.tracing_intermediate(loop_num, formatted_prompt, llm_generated_output)\n                        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n                            self.stream_content(\n                                content=llm_generated_output,\n                                source=self.name,\n                                step=f\"reasoning_{loop_num + 1}\",\n                                config=config,\n                                **kwargs,\n                            )\n\n                        if action == \"finish\":\n                            final_answer = llm_generated_output_json[\"action_input\"]\n                            self.tracing_final(loop_num, final_answer, config, kwargs)\n                            if self.streaming.enabled:\n                                self.stream_content(\n                                    content=final_answer,\n                                    source=self.name,\n                                    step=\"answer\",\n                                    config=config,\n                                    **kwargs,\n                                )\n                            return final_answer\n\n                        action_input = json.loads(llm_generated_output_json[\"action_input\"])\n                        llm_generated_output = json.dumps(llm_generated_output_json)\n\n                    case InferenceMode.XML:\n                        llm_generated_output = llm_result.output[\"content\"]\n                        self.tracing_intermediate(loop_num, formatted_prompt, llm_generated_output)\n                        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n                            self.stream_content(\n                                content=llm_generated_output,\n                                source=self.name,\n                                step=f\"reasoning_{loop_num + 1}\",\n                                config=config,\n                                **kwargs,\n                            )\n                        if \"&lt;answer&gt;\" in llm_generated_output:\n                            final_answer = self._extract_final_answer_xml(llm_generated_output)\n                            self.tracing_final(loop_num, final_answer, config, kwargs)\n                            if self.streaming.enabled:\n                                self.stream_content(\n                                    content=final_answer,\n                                    source=self.name,\n                                    step=\"answer\",\n                                    config=config,\n                                    **kwargs,\n                                )\n                            return final_answer\n                        action, action_input = self.parse_xml_and_extract_info(llm_generated_output)\n\n                if action:\n                    logger.debug(f\"Agent {self.name} - {self.id}:Loop {loop_num + 1}. Action:\\n{action}\")\n                    logger.debug(f\"Agent {self.name} - {self.id}:Loop {loop_num + 1}. Action Input:\\n{action_input}\")\n\n                    if self.tools:\n                        try:\n                            tool = self._get_tool(action)\n                            tool_result = self._run_tool(tool, action_input, config, **kwargs)\n\n                            logger.debug(\n                                f\"Agent {self.name} - {self.id}:Loop {loop_num + 1}. Tool Result:\\n{tool_result}\"\n                            )\n\n                        except RecoverableAgentException as e:\n                            tool_result = f\"{type(e).__name__}: {e}\"\n\n                        observation = f\"\\nObservation: {tool_result}\\n\"\n                        llm_generated_output += observation\n                        if self.streaming.enabled and self.streaming.mode == StreamingMode.ALL:\n                            self.stream_content(\n                                content=observation,\n                                source=tool.name,\n                                step=f\"tool_{loop_num}\",\n                                config=config,\n                                **kwargs,\n                            )\n\n                        self._intermediate_steps[loop_num][\"model_observation\"].update(\n                            AgentIntermediateStepModelObservation(\n                                tool_using=action,\n                                tool_input=action_input,\n                                tool_output=tool_result,\n                                updated=llm_generated_output,\n                            ).model_dump()\n                        )\n\n                previous_responses.append(llm_generated_output)\n\n            except ActionParsingException as e:\n                logger.error(f\"Agent {self.name} - {self.id}:Loop {loop_num + 1}. failed with error: {str(e)}\")\n                previous_responses.append(f\"{type(e).__name__}: {e}\")\n                continue\n        logger.warning(f\"Agent {self.name} - {self.id}: Maximum number of loops reached.\")\n        if self.behaviour_on_max_loops == Behavior.RAISE:\n            error_message = (\n                f\"Agent {self.name} (ID: {self.id}) has reached the maximum loop limit of {self.max_loops} without finding a final answer. \"  # noqa: E501\n                f\"Consider increasing the maximum number of loops or reviewing the task complexity to ensure completion.\"  # noqa: E501\n            )\n            raise MaxLoopsExceededException(message=error_message)\n        else:\n            max_loop_final_answer = self._handle_max_loops_exceeded(previous_responses, config, **kwargs)\n            if self.streaming.enabled:\n                self.stream_content(\n                    content=max_loop_final_answer,\n                    source=self.name,\n                    step=\"answer\",\n                    config=config,\n                    **kwargs,\n                )\n            return max_loop_final_answer\n\n    def _handle_max_loops_exceeded(\n        self, previous_responses: list, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; str:\n        \"\"\"\n        Handle the case where max loops are exceeded by crafting a thoughtful response.\n        \"\"\"\n        final_attempt_prompt = REACT_MAX_LOOPS_PROMPT.format(context=\"\\n\".join(previous_responses))\n        llm_final_attempt = self._run_llm(final_attempt_prompt, config=config, **kwargs)\n        self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n        final_answer = self.parse_xml_content(llm_final_attempt, \"answer\")\n\n        return f\"{final_answer}\"\n\n    def _extract_final_answer_xml(self, llm_output: str) -&gt; str:\n        \"\"\"Extract the final answer from the LLM output.\"\"\"\n        final_answer = self.extract_output_and_answer_xml(llm_output)\n        logger.info(f\"Agent {self.name} - {self.id}: Final answer found: {final_answer['answer']}\")\n        return final_answer[\"answer\"]\n\n    def generate_input_formats(self, tools: list[Node]) -&gt; str:\n        \"\"\"Generate formatted input descriptions for each tool.\"\"\"\n        input_formats = []\n        for tool in tools:\n            params = []\n            for name, field in tool.input_schema.model_fields.items():\n                if not field.json_schema_extra or field.json_schema_extra.get(\"is_accessible_to_agent\", True):\n                    # Handle Union types\n                    if get_origin(field.annotation) in (Union, types.UnionType):\n                        type_str = str(field.annotation)\n                    else:\n                        type_str = getattr(field.annotation, \"__name__\", str(field.annotation))\n\n                    description = field.description or \"No description\"\n                    params.append(f\"{name} ({type_str}): {description}\")\n\n            input_formats.append(f\" - {self.sanitize_tool_name(tool.name)}\\n \\t* \" + \"\\n\\t* \".join(params))\n        return \"\\n\".join(input_formats)\n\n    def generate_structured_output_schemas(self):\n        tool_names = [self.sanitize_tool_name(tool.name) for tool in self.tools]\n\n        schema = {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"plan_next_action\",\n                \"strict\": True,\n                \"schema\": {\n                    \"type\": \"object\",\n                    \"required\": [\"thought\", \"action\", \"action_input\"],\n                    \"properties\": {\n                        \"thought\": {\n                            \"type\": \"string\",\n                            \"description\": \"Your reasoning about the next step.\",\n                        },\n                        \"action\": {\n                            \"type\": \"string\",\n                            \"description\": f\"Next action to make (choose from [{tool_names}, finish]).\",\n                        },\n                        \"action_input\": {\n                            \"type\": \"string\",\n                            \"description\": \"Input for chosen action.\",\n                        },\n                    },\n                    \"additionalProperties\": False,\n                },\n            },\n        }\n\n        self.format_schema = schema\n\n    @staticmethod\n    def filter_format_type(param_type: str | type) -&gt; str:\n        \"\"\"Filters proper type for a function calling schema.\"\"\"\n        type_mapping = {\n            int: \"integer\",\n            float: \"float\",\n            bool: \"boolean\",\n            str: \"string\",\n        }\n\n        if isinstance(param_type, str):\n            match param_type:\n                case \"bool\":\n                    return \"boolean\"\n                case \"int\":\n                    return \"integer\"\n                case \"float\":\n                    return \"float\"\n                case _:\n                    return \"string\"\n        elif get_origin(param_type) is Union:\n            first_type = next((arg for arg in get_args(param_type) if arg is not type(None)), None)\n            if first_type is None:\n                return \"string\"\n            return type_mapping.get(first_type, getattr(first_type, \"__name__\", \"string\"))\n        else:\n            return type_mapping.get(param_type, getattr(param_type, \"__name__\", \"string\"))\n\n    def generate_function_calling_schemas(self):\n        \"\"\"Generate schemas for function calling.\"\"\"\n        self.format_schema.append(final_answer_function_schema)\n        for tool in self.tools:\n            properties = {}\n            for name, field in tool.input_schema.model_fields.items():\n                if not field.json_schema_extra or field.json_schema_extra.get(\"is_accessible_to_agent\", True):\n                    param_type = self.filter_format_type(field.annotation)\n                    description = field.description or \"No description\"\n                    properties[name] = {\"type\": param_type, \"description\": description}\n\n            schema = {\n                \"type\": \"function\",\n                \"strict\": True,\n                \"function\": {\n                    \"name\": self.sanitize_tool_name(tool.name),\n                    \"description\": tool.description[:1024],\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"thought\": {\n                                \"type\": \"string\",\n                                \"description\": \"Your reasoning about why you can answer original question.\",\n                            },\n                            \"action_input\": {\n                                \"type\": \"object\",\n                                \"description\": \"Input for chosen action.\",\n                                \"properties\": properties,\n                            },\n                        },\n                        \"required\": [\"thought\", \"action_input\"],\n                    },\n                },\n            }\n\n            self.format_schema.append(schema)\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initialize the prompt blocks required for the ReAct strategy.\"\"\"\n        super()._init_prompt_blocks()\n\n        prompt_blocks = {\n            \"tools\": REACT_BLOCK_TOOLS if self.tools else REACT_BLOCK_NO_TOOLS,\n            \"instructions\": REACT_BLOCK_INSTRUCTIONS if self.tools else REACT_BLOCK_INSTRUCTIONS_NO_TOOLS,\n            \"output_format\": REACT_BLOCK_OUTPUT_FORMAT,\n            \"context\": REACT_BLOCK_CONTEXT,\n            \"request\": REACT_BLOCK_REQUEST,\n        }\n\n        match self.inference_mode:\n            case InferenceMode.FUNCTION_CALLING:\n                self.generate_function_calling_schemas()\n                prompt_blocks[\"instructions\"] = REACT_BLOCK_INSTRUCTIONS_FUNCTION_CALLING\n            case InferenceMode.STRUCTURED_OUTPUT:\n                self.generate_structured_output_schemas()\n                prompt_blocks[\"instructions\"] = REACT_BLOCK_INSTRUCTIONS_STRUCTURED_OUTPUT\n            case InferenceMode.DEFAULT:\n                if not self.tools:\n                    prompt_blocks[\"tools\"] = REACT_BLOCK_NO_TOOLS\n                    prompt_blocks[\"instructions\"] = REACT_BLOCK_INSTRUCTIONS_NO_TOOLS\n            case InferenceMode.XML:\n                prompt_blocks[\"instructions\"] = REACT_BLOCK_XML_INSTRUCTIONS\n\n        self._prompt_blocks.update(prompt_blocks)\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.extract_output_and_answer_xml","title":"<code>extract_output_and_answer_xml(text)</code>","text":"<p>Extract output and answer from XML-like structure.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>def extract_output_and_answer_xml(self, text: str) -&gt; dict[str, str]:\n    \"\"\"Extract output and answer from XML-like structure.\"\"\"\n    output = self.parse_xml_content(text, \"output\")\n    answer = self.parse_xml_content(text, \"answer\")\n\n    logger.debug(f\"Extracted output: {output}\")\n    logger.debug(f\"Extracted answer: {answer}\")\n\n    return {\"output\": output, \"answer\": answer}\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.filter_format_type","title":"<code>filter_format_type(param_type)</code>  <code>staticmethod</code>","text":"<p>Filters proper type for a function calling schema.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>@staticmethod\ndef filter_format_type(param_type: str | type) -&gt; str:\n    \"\"\"Filters proper type for a function calling schema.\"\"\"\n    type_mapping = {\n        int: \"integer\",\n        float: \"float\",\n        bool: \"boolean\",\n        str: \"string\",\n    }\n\n    if isinstance(param_type, str):\n        match param_type:\n            case \"bool\":\n                return \"boolean\"\n            case \"int\":\n                return \"integer\"\n            case \"float\":\n                return \"float\"\n            case _:\n                return \"string\"\n    elif get_origin(param_type) is Union:\n        first_type = next((arg for arg in get_args(param_type) if arg is not type(None)), None)\n        if first_type is None:\n            return \"string\"\n        return type_mapping.get(first_type, getattr(first_type, \"__name__\", \"string\"))\n    else:\n        return type_mapping.get(param_type, getattr(param_type, \"__name__\", \"string\"))\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.generate_function_calling_schemas","title":"<code>generate_function_calling_schemas()</code>","text":"<p>Generate schemas for function calling.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>def generate_function_calling_schemas(self):\n    \"\"\"Generate schemas for function calling.\"\"\"\n    self.format_schema.append(final_answer_function_schema)\n    for tool in self.tools:\n        properties = {}\n        for name, field in tool.input_schema.model_fields.items():\n            if not field.json_schema_extra or field.json_schema_extra.get(\"is_accessible_to_agent\", True):\n                param_type = self.filter_format_type(field.annotation)\n                description = field.description or \"No description\"\n                properties[name] = {\"type\": param_type, \"description\": description}\n\n        schema = {\n            \"type\": \"function\",\n            \"strict\": True,\n            \"function\": {\n                \"name\": self.sanitize_tool_name(tool.name),\n                \"description\": tool.description[:1024],\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"thought\": {\n                            \"type\": \"string\",\n                            \"description\": \"Your reasoning about why you can answer original question.\",\n                        },\n                        \"action_input\": {\n                            \"type\": \"object\",\n                            \"description\": \"Input for chosen action.\",\n                            \"properties\": properties,\n                        },\n                    },\n                    \"required\": [\"thought\", \"action_input\"],\n                },\n            },\n        }\n\n        self.format_schema.append(schema)\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.generate_input_formats","title":"<code>generate_input_formats(tools)</code>","text":"<p>Generate formatted input descriptions for each tool.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>def generate_input_formats(self, tools: list[Node]) -&gt; str:\n    \"\"\"Generate formatted input descriptions for each tool.\"\"\"\n    input_formats = []\n    for tool in tools:\n        params = []\n        for name, field in tool.input_schema.model_fields.items():\n            if not field.json_schema_extra or field.json_schema_extra.get(\"is_accessible_to_agent\", True):\n                # Handle Union types\n                if get_origin(field.annotation) in (Union, types.UnionType):\n                    type_str = str(field.annotation)\n                else:\n                    type_str = getattr(field.annotation, \"__name__\", str(field.annotation))\n\n                description = field.description or \"No description\"\n                params.append(f\"{name} ({type_str}): {description}\")\n\n        input_formats.append(f\" - {self.sanitize_tool_name(tool.name)}\\n \\t* \" + \"\\n\\t* \".join(params))\n    return \"\\n\".join(input_formats)\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.parse_xml_and_extract_info","title":"<code>parse_xml_and_extract_info(text)</code>","text":"<p>Parse XML-like structure and extract action and action_input.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>def parse_xml_and_extract_info(self, text: str) -&gt; dict[str, Any]:\n    \"\"\"Parse XML-like structure and extract action and action_input.\"\"\"\n    output_content = self.parse_xml_content(text, \"output\")\n    action = self.parse_xml_content(output_content, \"action\")\n    action_input_text = self.parse_xml_content(output_content, \"action_input\")\n\n    try:\n        action_input = json.loads(action_input_text)\n    except json.JSONDecodeError:\n        raise ActionParsingException(\n            (\n                \"Error: Unable to parse action and action input. \"\n                \"Please rewrite in the correct XML format with action_input as a valid dictionary.\"\n            ),\n            recoverable=True,\n        )\n\n    return action, action_input\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.parse_xml_content","title":"<code>parse_xml_content(text, tag)</code>","text":"<p>Extract content from XML-like tags.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>def parse_xml_content(self, text: str, tag: str) -&gt; str:\n    \"\"\"Extract content from XML-like tags.\"\"\"\n    match = re.search(f\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\", text, re.DOTALL)\n    return match.group(1).strip() if match else \"\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/react/#dynamiq.nodes.agents.react.ReActAgent.validate_inference_mode","title":"<code>validate_inference_mode()</code>","text":"<p>Validate whether specified model can be inferenced in provided mode.</p> Source code in <code>dynamiq/nodes/agents/react.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_inference_mode(self):\n    \"\"\"Validate whether specified model can be inferenced in provided mode.\"\"\"\n    match self.inference_mode:\n        case InferenceMode.FUNCTION_CALLING:\n            if not supports_function_calling(model=self.llm.model):\n                raise ValueError(f\"Model {self.llm.model} does not support function calling\")\n\n        case InferenceMode.STRUCTURED_OUTPUT:\n            params = get_supported_openai_params(model=self.llm.model)\n            if \"response_format\" not in params:\n                raise ValueError(f\"Model {self.llm.model} does not support structured output\")\n\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/agents/reflection/","title":"Reflection","text":""},{"location":"dynamiq/nodes/agents/reflection/#dynamiq.nodes.agents.reflection.ReflectionAgent","title":"<code>ReflectionAgent</code>","text":"<p>               Bases: <code>Agent</code></p> Source code in <code>dynamiq/nodes/agents/reflection.py</code> <pre><code>class ReflectionAgent(Agent):\n    name: str = \"Reflection Agent\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._init_prompt_blocks()\n\n    def _init_prompt_blocks(self):\n        super()._init_prompt_blocks()\n        prompt_blocks = {\n            \"instructions\": REFLECTION_REFLECT_PROMPT,\n        }\n        self._prompt_blocks.update(prompt_blocks)\n\n    @staticmethod\n    def extract_output_content(text: str) -&gt; list[str]:\n        \"\"\"\n        Extracts content from &lt;output&gt; tags in the given text.\n\n        Args:\n            text (str): The input text containing &lt;output&gt; tags.\n\n        Returns:\n            List[str]: A list of content found within &lt;output&gt; tags.\n        \"\"\"\n        logger.debug(\"Extracting content from &lt;output&gt; tags.\")\n        pattern = r\"&lt;output&gt;(.*?)&lt;/output&gt;\"\n        output_content = re.findall(pattern, text, re.DOTALL)\n\n        if not output_content:\n            logger.warning(\"No closed &lt;/output&gt; tags found. Checking for unclosed tags.\")\n            pattern = r\"&lt;output&gt;(.*)\"\n            output_content = re.findall(pattern, text, re.DOTALL)\n\n        return [content.strip() for content in output_content]\n\n    def _run_agent(self, config: RunnableConfig | None = None, **kwargs) -&gt; str:\n        try:\n            formatted_prompt = self.generate_prompt(\n                block_names=[\"introduction\", \"role\", \"date\", \"instructions\", \"request\"]\n            )\n            result = self._run_llm(formatted_prompt, config=config, **kwargs)\n            output_content = self.extract_output_content(result)\n            if self.streaming.enabled:\n                if self.streaming.mode == StreamingMode.FINAL:\n                    logger.debug(\"Streaming mode set to FINAL. Returning final output.\")\n                    if not output_content:\n                        logger.warning(\"No output content extracted.\")\n                        return \"\"\n                    return self.stream_content(\n                        content=output_content[-1],\n                        step=\"answer\",\n                        source=self.name,\n                        config=config,\n                        **kwargs,\n                    )\n                elif self.streaming.mode == StreamingMode.ALL:\n                    logger.debug(\"Streaming mode set to ALL. Returning all output.\")\n                    return self.stream_content(\n                        content=result, step=\"reasoning\", source=self.name, config=config, **kwargs\n                    )\n\n            if not output_content:\n                logger.warning(\"No output content extracted.\")\n                return \"\"\n\n            return output_content[-1]\n        except Exception as e:\n            logger.error(f\"Agent {self.name} - {self.id}: failed with error: {str(e)}\")\n            raise\n</code></pre>"},{"location":"dynamiq/nodes/agents/reflection/#dynamiq.nodes.agents.reflection.ReflectionAgent.extract_output_content","title":"<code>extract_output_content(text)</code>  <code>staticmethod</code>","text":"<p>Extracts content from  tags in the given text. <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing  tags. required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of content found within  tags. Source code in <code>dynamiq/nodes/agents/reflection.py</code> <pre><code>@staticmethod\ndef extract_output_content(text: str) -&gt; list[str]:\n    \"\"\"\n    Extracts content from &lt;output&gt; tags in the given text.\n\n    Args:\n        text (str): The input text containing &lt;output&gt; tags.\n\n    Returns:\n        List[str]: A list of content found within &lt;output&gt; tags.\n    \"\"\"\n    logger.debug(\"Extracting content from &lt;output&gt; tags.\")\n    pattern = r\"&lt;output&gt;(.*?)&lt;/output&gt;\"\n    output_content = re.findall(pattern, text, re.DOTALL)\n\n    if not output_content:\n        logger.warning(\"No closed &lt;/output&gt; tags found. Checking for unclosed tags.\")\n        pattern = r\"&lt;output&gt;(.*)\"\n        output_content = re.findall(pattern, text, re.DOTALL)\n\n    return [content.strip() for content in output_content]\n</code></pre>"},{"location":"dynamiq/nodes/agents/simple/","title":"Simple","text":""},{"location":"dynamiq/nodes/agents/simple/#dynamiq.nodes.agents.simple.SimpleAgent","title":"<code>SimpleAgent</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Agent that uses the Simple strategy for processing tasks.</p> Source code in <code>dynamiq/nodes/agents/simple.py</code> <pre><code>class SimpleAgent(Agent):\n    \"\"\"Agent that uses the Simple strategy for processing tasks.\"\"\"\n\n    name: str = \"Simple\"\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/","title":"Adaptive","text":""},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.ActionParseError","title":"<code>ActionParseError</code>","text":"<p>               Bases: <code>OrchestratorError</code></p> <p>Raised when there's an error parsing the LLM action.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>class ActionParseError(OrchestratorError):\n    \"\"\"Raised when there's an error parsing the LLM action.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator","title":"<code>AdaptiveOrchestrator</code>","text":"<p>               Bases: <code>Node</code></p> <p>Orchestrates the execution of complex tasks using multiple specialized agents.</p> <p>This class manages the breakdown of a main objective into subtasks, delegates these subtasks to appropriate agents, and synthesizes the results into a final answer.</p> <p>Attributes:</p> Name Type Description <code>manager</code> <code>ManagerAgent</code> <p>The managing agent responsible for overseeing the orchestration process.</p> <code>agents</code> <code>List[BaseAgent]</code> <p>List of specialized agents available for task execution.</p> <code>objective</code> <code>Optional[str]</code> <p>The main objective of the orchestration.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>class AdaptiveOrchestrator(Node):\n    \"\"\"\n    Orchestrates the execution of complex tasks using multiple specialized agents.\n\n    This class manages the breakdown of a main objective into subtasks,\n    delegates these subtasks to appropriate agents, and synthesizes the results\n    into a final answer.\n\n    Attributes:\n        manager (ManagerAgent): The managing agent responsible for overseeing the orchestration process.\n        agents (List[BaseAgent]): List of specialized agents available for task execution.\n        objective (Optional[str]): The main objective of the orchestration.\n    \"\"\"\n\n    name: str | None = \"AdaptiveOrchestrator\"\n    group: NodeGroup = NodeGroup.AGENTS\n    manager: AdaptiveAgentManager\n    agents: list[Agent] = []\n    objective: str | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the orchestrator with given parameters.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._chat_history = []\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"manager\": True, \"agents\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"manager\"] = self.manager.to_dict(**kwargs)\n        data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n        return data\n\n    def reset_run_state(self):\n        self._chat_history = []\n        self._run_depends = []\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ) -&gt; None:\n        \"\"\"\n        Initialize components of the orchestrator.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.manager.is_postponed_component_init:\n            self.manager.init_components(connection_manager)\n\n        for agent in self.agents:\n            if agent.is_postponed_component_init:\n                agent.init_components(connection_manager)\n\n    @property\n    def agents_descriptions(self) -&gt; str:\n        \"\"\"Get a formatted string of agent descriptions.\"\"\"\n        return (\n            \"\\n\".join([f\"{i}. {agent.name}\" for i, agent in enumerate(self.agents)])\n            if self.agents\n            else \"\"\n        )\n\n    def get_next_action(self, config: RunnableConfig = None, **kwargs) -&gt; Action:\n        \"\"\"\n        Determine the next action based on the current state and LLM output.\n\n        Returns:\n            Action: The next action to be taken.\n\n        Raises:\n            ActionParseError: If there is an error parsing the action from the LLM response.\n        \"\"\"\n        prompt = \"\\n\".join(\n            [f\"{msg['role']}: {msg['content']}\" for msg in self._chat_history]\n        )\n\n        logger.debug(f\"AdaptiveOrchestrator {self.id}: PROMPT {prompt}\")\n\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"plan\",\n                \"agents\": self.agents_descriptions,\n                \"chat_history\": prompt,\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n        logger.debug(\n            f\"AdaptiveOrchestrator {self.id}: LLM response: {manager_result.output.get('content')}\"\n        )\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            logger.error(\n                f\"AdaptiveOrchestrator {self.id}: Error getting next action from Manager\"\n            )\n            raise ActionParseError(\"Unable to retrieve the next action from Manager.\")\n\n        manager_result = (\n            manager_result.output.get(\"content\")\n            .get(\"result\")\n            .replace(\"json\", \"\")\n            .replace(\"```\", \"\")\n            .strip()\n        )\n        try:\n            return Action.model_validate_json(manager_result)\n        except ValidationError as e:\n            logger.error(\n                f\"AdaptiveOrchestrator {self.id}: Error creation Agent based on LLM output. \"\n                f\"Raw LLM output: {manager_result}. Error: {e}\"\n            )\n            raise ActionParseError(\"Unable to create Action from LLM output.\")\n\n    def run_task(self, task: str, config: RunnableConfig = None, **kwargs) -&gt; str:\n        \"\"\"\n        Process the given task using the manager agent logic.\n\n        Args:\n            task (str): The task to be processed.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            str: The final answer generated after processing the task.\n        \"\"\"\n        self._chat_history.append({\"role\": \"user\", \"content\": task})\n\n        while True:\n            action = self.get_next_action(config=config, **kwargs)\n            logger.debug(f\"AdaptiveOrchestrator {self.id}: chat history: {self._chat_history}\")\n            logger.debug(f\"AdaptiveOrchestrator {self.id}: Next action: {action.model_dump()}\")\n\n            if action.command == ActionCommand.DELEGATE:\n                self._handle_delegation(action=action, config=config, **kwargs)\n            elif action.command == ActionCommand.FINAL_ANSWER:\n                return self.get_final_result(\n                    input_task=task,\n                    preliminary_answer=action.answer,\n                    config=config,\n                    **kwargs,\n                )\n\n    def _handle_delegation(\n        self, action: Action, config: RunnableConfig = None, **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Handle task delegation to a specialized agent.\n\n        Args:\n            action (Action): The action containing the delegation command and details.\n        \"\"\"\n        agent = next((a for a in self.agents if a.name == action.agent), None)\n        if agent:\n            result = agent.run(\n                input_data={\"input\": action.task},\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=agent).to_dict()]\n            if result.status != RunnableStatus.SUCCESS:\n                logger.error(f\"AdaptiveOrchestrator {self.id}: Error executing Agent {agent.name}\")\n                raise OrchestratorError(\n                    f\"Failed to execute Agent {agent.name} with Error: {result.output.get('content')}\"\n                )\n\n            self._chat_history.append(\n                {\n                    \"role\": \"system\",\n                    \"content\": f\"Agent {action.agent} result: {result.output.get('content')}\",\n                }\n            )\n        else:\n            logger.warning(\n                f\"AdaptiveOrchestrator {self.id}: Agent {action.agent} not found. Using LLM.\"\n            )\n            result = self.manager.run(\n                input_data={\"action\": \"run\", \"simple_prompt\": action.task},\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n            if result.status != RunnableStatus.SUCCESS:\n                logger.error(f\"AdaptiveOrchestrator {self.id}: Error executing LLM: {result.output.get('content')}\")\n\n            self._chat_history.append(\n                {\n                    \"role\": \"system\",\n                    \"content\": f\"LLM result: {result.output.get('content')}\",\n                }\n            )\n\n    def get_final_result(\n        self,\n        input_task: str,\n        preliminary_answer: str,\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"\n        Generate a comprehensive final result based on the task and agent outputs.\n\n        Args:\n            input_task (str): The original task given.\n            preliminary_answer (str): The preliminary answer generated.\n            config (RunnableConfig): Configuration for the runnable.\n\n        Returns:\n            str: The final comprehensive result.\n\n        Raises:\n            OrchestratorError: If an error occurs while generating the final answer.\n        \"\"\"\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"final\",\n                \"input_task\": input_task,\n                \"chat_history\": self._chat_history,\n                \"preliminary_answer\": preliminary_answer,\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            logger.error(\n                f\"AdaptiveOrchestrator {self.id}: Error generating final answer\"\n            )\n            raise OrchestratorError(\"Failed to generate final answer\")\n\n        return manager_result.output.get(\"content\").get(\"result\")\n\n    def execute(\n        self, input_data: Any, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Execute the orchestration process with the given input data and configuration.\n\n        Args:\n            input_data (Any): The input data containing the objective or additional context.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: The result of the orchestration process.\n\n        Raises:\n            OrchestratorError: If an error occurs during the orchestration process.\n        \"\"\"\n        self.reset_run_state()\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        objective = input_data.get(\"input\") or self.objective\n        logger.debug(\n            f\"AdaptiveOrchestrator {self.id}: Starting the flow with objective:\\n```{objective}```\"\n        )\n\n        run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        run_kwargs.pop(\"run_depends\", None)\n\n        if self.streaming.enabled:\n            self.manager.streaming = self.streaming\n            for agent in self.agents:\n                agent.streaming = self.streaming\n\n        result = self.run_task(\n            task=objective,\n            config=config,\n            **run_kwargs,\n        )\n\n        logger.debug(f\"AdaptiveOrchestrator {self.id}: Output collected\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.agents_descriptions","title":"<code>agents_descriptions: str</code>  <code>property</code>","text":"<p>Get a formatted string of agent descriptions.</p>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the orchestrator with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the orchestrator with given parameters.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._chat_history = []\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the orchestration process with the given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data containing the objective or additional context.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, Any]: The result of the orchestration process.</p> <p>Raises:</p> Type Description <code>OrchestratorError</code> <p>If an error occurs during the orchestration process.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def execute(\n    self, input_data: Any, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict:\n    \"\"\"\n    Execute the orchestration process with the given input data and configuration.\n\n    Args:\n        input_data (Any): The input data containing the objective or additional context.\n        config (Optional[RunnableConfig]): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: The result of the orchestration process.\n\n    Raises:\n        OrchestratorError: If an error occurs during the orchestration process.\n    \"\"\"\n    self.reset_run_state()\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    objective = input_data.get(\"input\") or self.objective\n    logger.debug(\n        f\"AdaptiveOrchestrator {self.id}: Starting the flow with objective:\\n```{objective}```\"\n    )\n\n    run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    run_kwargs.pop(\"run_depends\", None)\n\n    if self.streaming.enabled:\n        self.manager.streaming = self.streaming\n        for agent in self.agents:\n            agent.streaming = self.streaming\n\n    result = self.run_task(\n        task=objective,\n        config=config,\n        **run_kwargs,\n    )\n\n    logger.debug(f\"AdaptiveOrchestrator {self.id}: Output collected\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.get_final_result","title":"<code>get_final_result(input_task, preliminary_answer, config=None, **kwargs)</code>","text":"<p>Generate a comprehensive final result based on the task and agent outputs.</p> <p>Parameters:</p> Name Type Description Default <code>input_task</code> <code>str</code> <p>The original task given.</p> required <code>preliminary_answer</code> <code>str</code> <p>The preliminary answer generated.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final comprehensive result.</p> <p>Raises:</p> Type Description <code>OrchestratorError</code> <p>If an error occurs while generating the final answer.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def get_final_result(\n    self,\n    input_task: str,\n    preliminary_answer: str,\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"\n    Generate a comprehensive final result based on the task and agent outputs.\n\n    Args:\n        input_task (str): The original task given.\n        preliminary_answer (str): The preliminary answer generated.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        str: The final comprehensive result.\n\n    Raises:\n        OrchestratorError: If an error occurs while generating the final answer.\n    \"\"\"\n    manager_result = self.manager.run(\n        input_data={\n            \"action\": \"final\",\n            \"input_task\": input_task,\n            \"chat_history\": self._chat_history,\n            \"preliminary_answer\": preliminary_answer,\n        },\n        config=config,\n        run_depends=self._run_depends,\n        **kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n    if manager_result.status != RunnableStatus.SUCCESS:\n        logger.error(\n            f\"AdaptiveOrchestrator {self.id}: Error generating final answer\"\n        )\n        raise OrchestratorError(\"Failed to generate final answer\")\n\n    return manager_result.output.get(\"content\").get(\"result\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.get_next_action","title":"<code>get_next_action(config=None, **kwargs)</code>","text":"<p>Determine the next action based on the current state and LLM output.</p> <p>Returns:</p> Name Type Description <code>Action</code> <code>Action</code> <p>The next action to be taken.</p> <p>Raises:</p> Type Description <code>ActionParseError</code> <p>If there is an error parsing the action from the LLM response.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def get_next_action(self, config: RunnableConfig = None, **kwargs) -&gt; Action:\n    \"\"\"\n    Determine the next action based on the current state and LLM output.\n\n    Returns:\n        Action: The next action to be taken.\n\n    Raises:\n        ActionParseError: If there is an error parsing the action from the LLM response.\n    \"\"\"\n    prompt = \"\\n\".join(\n        [f\"{msg['role']}: {msg['content']}\" for msg in self._chat_history]\n    )\n\n    logger.debug(f\"AdaptiveOrchestrator {self.id}: PROMPT {prompt}\")\n\n    manager_result = self.manager.run(\n        input_data={\n            \"action\": \"plan\",\n            \"agents\": self.agents_descriptions,\n            \"chat_history\": prompt,\n        },\n        config=config,\n        run_depends=self._run_depends,\n        **kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n    logger.debug(\n        f\"AdaptiveOrchestrator {self.id}: LLM response: {manager_result.output.get('content')}\"\n    )\n\n    if manager_result.status != RunnableStatus.SUCCESS:\n        logger.error(\n            f\"AdaptiveOrchestrator {self.id}: Error getting next action from Manager\"\n        )\n        raise ActionParseError(\"Unable to retrieve the next action from Manager.\")\n\n    manager_result = (\n        manager_result.output.get(\"content\")\n        .get(\"result\")\n        .replace(\"json\", \"\")\n        .replace(\"```\", \"\")\n        .strip()\n    )\n    try:\n        return Action.model_validate_json(manager_result)\n    except ValidationError as e:\n        logger.error(\n            f\"AdaptiveOrchestrator {self.id}: Error creation Agent based on LLM output. \"\n            f\"Raw LLM output: {manager_result}. Error: {e}\"\n        )\n        raise ActionParseError(\"Unable to create Action from LLM output.\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize components of the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager. Defaults to ConnectionManager.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n) -&gt; None:\n    \"\"\"\n    Initialize components of the orchestrator.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager. Defaults to ConnectionManager.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.manager.is_postponed_component_init:\n        self.manager.init_components(connection_manager)\n\n    for agent in self.agents:\n        if agent.is_postponed_component_init:\n            agent.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.run_task","title":"<code>run_task(task, config=None, **kwargs)</code>","text":"<p>Process the given task using the manager agent logic.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The task to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final answer generated after processing the task.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def run_task(self, task: str, config: RunnableConfig = None, **kwargs) -&gt; str:\n    \"\"\"\n    Process the given task using the manager agent logic.\n\n    Args:\n        task (str): The task to be processed.\n        config (RunnableConfig): Configuration for the runnable.\n\n    Returns:\n        str: The final answer generated after processing the task.\n    \"\"\"\n    self._chat_history.append({\"role\": \"user\", \"content\": task})\n\n    while True:\n        action = self.get_next_action(config=config, **kwargs)\n        logger.debug(f\"AdaptiveOrchestrator {self.id}: chat history: {self._chat_history}\")\n        logger.debug(f\"AdaptiveOrchestrator {self.id}: Next action: {action.model_dump()}\")\n\n        if action.command == ActionCommand.DELEGATE:\n            self._handle_delegation(action=action, config=config, **kwargs)\n        elif action.command == ActionCommand.FINAL_ANSWER:\n            return self.get_final_result(\n                input_task=task,\n                preliminary_answer=action.answer,\n                config=config,\n                **kwargs,\n            )\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AdaptiveOrchestrator.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"manager\"] = self.manager.to_dict(**kwargs)\n    data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.AgentNotFoundError","title":"<code>AgentNotFoundError</code>","text":"<p>               Bases: <code>OrchestratorError</code></p> <p>Raised when a specified agent is not found.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>class AgentNotFoundError(OrchestratorError):\n    \"\"\"Raised when a specified agent is not found.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive/#dynamiq.nodes.agents.orchestrators.adaptive.OrchestratorError","title":"<code>OrchestratorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for AdaptiveOrchestrator errors.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive.py</code> <pre><code>class OrchestratorError(Exception):\n    \"\"\"Base exception for AdaptiveOrchestrator errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive_manager/","title":"Adaptive manager","text":""},{"location":"dynamiq/nodes/agents/orchestrators/adaptive_manager/#dynamiq.nodes.agents.orchestrators.adaptive_manager.AdaptiveAgentManager","title":"<code>AdaptiveAgentManager</code>","text":"<p>               Bases: <code>AgentManager</code></p> <p>An adaptive agent manager that coordinates specialized agents to complete complex tasks.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive_manager.py</code> <pre><code>class AdaptiveAgentManager(AgentManager):\n    \"\"\"An adaptive agent manager that coordinates specialized agents to complete complex tasks.\"\"\"\n\n    name: str = \"Adaptive Manager\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the AdaptiveAgentManager and set up prompt templates.\"\"\"\n        super().__init__(**kwargs)\n        self._init_prompt_blocks()\n\n    def _init_prompt_blocks(self):\n        \"\"\"Initialize the prompt blocks with adaptive plan and final prompts.\"\"\"\n        super()._init_prompt_blocks()\n        self._prompt_blocks.update(\n            {\n                \"plan\": self._get_adaptive_plan_prompt(),\n                \"final\": self._get_adaptive_final_prompt(),\n            }\n        )\n\n    @staticmethod\n    def _get_adaptive_plan_prompt() -&gt; str:\n        \"\"\"Return the adaptive plan prompt template.\"\"\"\n        return PROMPT_TEMPLATE_AGENT_MANAGER_PLAN\n\n    @staticmethod\n    def _get_adaptive_final_prompt() -&gt; str:\n        \"\"\"Return the adaptive final answer prompt template.\"\"\"\n        return PROMPT_TEMPLATE_AGENT_MANAGER_FINAL_ANSWER\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/adaptive_manager/#dynamiq.nodes.agents.orchestrators.adaptive_manager.AdaptiveAgentManager.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AdaptiveAgentManager and set up prompt templates.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/adaptive_manager.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the AdaptiveAgentManager and set up prompt templates.\"\"\"\n    super().__init__(**kwargs)\n    self._init_prompt_blocks()\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/","title":"Linear","text":""},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator","title":"<code>LinearOrchestrator</code>","text":"<p>               Bases: <code>Node</code></p> <p>Manages the execution of tasks by coordinating multiple agents and leveraging LLM (Large Language Model).</p> <p>Attributes:</p> Name Type Description <code>manager</code> <code>ManagerAgent</code> <p>The managing agent responsible for overseeing the orchestration process.</p> <code>agents</code> <code>List[BaseAgent]</code> <p>List of specialized agents available for task execution.</p> <code>input_task</code> <code>str | None</code> <p>Initial task input.</p> <code>use_summarizer</code> <code>bool</code> <p>Indicates if a final summarizer is used.</p> <code>summarize_all_answers</code> <code>bool</code> <p>Indicates whether to summarize answers to all tasks or use only last one.              Will only be applied if use_summarizer is set to True.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>class LinearOrchestrator(Node):\n    \"\"\"\n    Manages the execution of tasks by coordinating multiple agents and leveraging LLM (Large Language Model).\n\n    Attributes:\n        manager (ManagerAgent): The managing agent responsible for overseeing the orchestration process.\n        agents (List[BaseAgent]): List of specialized agents available for task execution.\n        input_task (str | None): Initial task input.\n        use_summarizer (bool): Indicates if a final summarizer is used.\n        summarize_all_answers (bool): Indicates whether to summarize answers to all tasks or use only last one.\\\n              Will only be applied if use_summarizer is set to True.\n\n    \"\"\"\n\n    name: str | None = \"LinearOrchestrator\"\n    group: NodeGroup = NodeGroup.AGENTS\n    manager: LinearAgentManager\n    agents: list[Agent] = []\n    input_task: str | None = None\n    use_summarizer: bool = True\n    summarize_all_answers: bool = True\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._results = {}\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"manager\": True, \"agents\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"manager\"] = self.manager.to_dict(**kwargs)\n        data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n        return data\n\n    def reset_run_state(self):\n        self._results = {}\n        self._run_depends = []\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"Initialize components for the manager and agents.\"\"\"\n        super().init_components(connection_manager)\n        if self.manager.is_postponed_component_init:\n            self.manager.init_components(connection_manager)\n\n        for agent in self.agents:\n            if agent.is_postponed_component_init:\n                agent.init_components(connection_manager)\n\n    @cached_property\n    def agents_descriptions(self) -&gt; str:\n        \"\"\"Generate a string description of all agents.\"\"\"\n        return (\n            \"\\n\".join([f\"{i}. {_agent.name}\" for i, _agent in enumerate(self.agents)])\n            if self.agents\n            else \"\"\n        )\n\n    def get_tasks(self, config: RunnableConfig = None, **kwargs) -&gt; list[Task]:\n        \"\"\"Generate tasks using the manager agent.\"\"\"\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"plan\",\n                \"input_task\": self.input_task,\n                \"agents\": self.agents_descriptions,\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n        if manager_result.status != RunnableStatus.SUCCESS:\n            raise ValueError(\"Agent LLM failed to generate tasks\")\n\n        manager_result_content = manager_result.output.get(\"content\").get(\"result\")\n        logger.debug(\n            f\"LinearOrchestrator {self.id}: Manager plan result content: {manager_result_content}\"\n        )\n\n        tasks = self.parse_tasks_from_output(manager_result_content)\n        logger.debug(f\"LinearOrchestrator {self.id}: Task list JSON: {tasks}\")\n\n        return tasks\n\n    def parse_tasks_from_output(self, output: str) -&gt; list[Task]:\n        \"\"\"Parse tasks from the manager's output string.\"\"\"\n        # Remove 'output' XML tags if present\n        if \"&lt;output&gt;\" in output and \"&lt;/output&gt;\" in output:\n            output = output.split(\"&lt;output&gt;\")[1].split(\"&lt;/output&gt;\")[0]\n\n        # Remove '```' code block markers and 'json' keyword if present\n        try:\n            output = output.replace(\"```\", \"\").replace(\"json\", \"\")\n        except AttributeError as e:\n            logger.warning(\n                f\"LinearOrchestrator {self.id}: Failed to remove code block markers and 'json' keyword from output {e}\"\n            )\n\n        # Parse the JSON string\n        try:\n            task_list_json = output.strip()\n        except AttributeError as e:\n            logger.warning(\n                f\"LinearOrchestrator {self.id}: Failed to strip the output string: {e}\"\n            )\n            task_list_json = output\n        return TypeAdapter(list[Task]).validate_json(task_list_json)\n\n    def get_dependency_outputs(self, dependencies: list[int]) -&gt; str:\n        \"\"\"Format the outputs of dependent tasks.\"\"\"\n        if not dependencies:\n            return \"\"\n\n        dependencies_formatted = \"**Here is the previously collected information:**\\n\"\n        for dep in dependencies:\n            if dep in self._results:\n                task_name = self._results[dep][\"name\"]\n                task_result = str(self._results[dep][\"result\"])\n                dependencies_formatted += (\n                    f\"**Task:** {task_name}\\n**Result:** {task_result}\\n\\n\"\n                )\n\n        return dependencies_formatted.strip()\n\n    def get_final_result(self, config: RunnableConfig = None, **kwargs) -&gt; str:\n        \"\"\"Generate the final result.\"\"\"\n        final_task_output = \"\\n\\n\".join(\n            f\"**Task:** {result['name']}\\n**Result:** {result['result']}\"\n            for result in self._results.values()\n            if result\n        )\n\n        if self.use_summarizer:\n            if not self.summarize_all_answers:\n                final_task_id = max(self._results.keys(), default=None)\n                logger.debug(f\"LinearOrchestrator {self.id}: Final task id: {final_task_id}\")\n\n                if final_task_id is not None:\n                    final_task_output = self._results[final_task_id].get(\"result\", \"\")\n\n                logger.debug(f\"LinearOrchestrator {self.id}: Final task output: {final_task_output}\")\n\n            logger.debug(f\"LinearOrchestrator {self.id}: Running final summarizer\")\n            manager_result = self.manager.run(\n                input_data={\n                    \"action\": \"final\",\n                    \"input_task\": self.input_task,\n                    \"tasks_outputs\": final_task_output,\n                },\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n            if manager_result.status != RunnableStatus.SUCCESS:\n                raise ValueError(\"Agent LLM failed to generate final result\")\n\n            result = manager_result.output.get(\"content\")\n        else:\n            logger.debug(f\"LinearOrchestrator {self.id}: Returning final task output\")\n            result = final_task_output\n\n        return result\n\n    def run_tasks(\n        self, tasks: list[Task], config: RunnableConfig = None, **kwargs\n    ) -&gt; None:\n        \"\"\"Execute the tasks using appropriate agents.\"\"\"\n        logger.debug(\n            f\"LinearOrchestrator {self.id}: Assigning and executing tasks. Agents: {self.agents_descriptions}\"\n        )\n\n        for task in tasks:\n            task_per_llm = f\"**{task.description}**\\n**Required information for output**: {task.output}\"\n            logger.debug(\n                f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} prepared for LLM: {task_per_llm}\"\n            )\n\n            dependency_outputs = self.get_dependency_outputs(task.dependencies)\n            if dependency_outputs:\n                task_per_llm += f\"\\n{dependency_outputs}\"\n                logger.debug(\n                    f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} \"\n                    f\"prepared for LLM with dependencies: {task_per_llm}\"\n                )\n\n            logger.debug(\n                f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} \"\n                f\"with dependencies: {task.dependencies}\"\n            )\n            logger.debug(\n                f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} \"\n                f\"task dependencies output: ```{dependency_outputs}```\"\n            )\n\n            success_flag = False\n            for _ in range(self.manager.max_loops):\n                manager_result = self.manager.run(\n                    input_data={\n                        \"action\": \"assign\",\n                        \"input_task\": self.input_task,\n                        \"task\": task_per_llm,\n                        \"agents\": self.agents_descriptions,\n                    },\n                    config=config,\n                    run_depends=self._run_depends,\n                    **kwargs,\n                )\n                self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n                logger.debug(f\"LinearOrchestrator {self.id}: Assigner LLM result: {manager_result}\")\n\n                if manager_result.status == RunnableStatus.SUCCESS:\n                    try:\n                        assigned_agent_index = int(manager_result.output.get(\"content\").get(\"result\", -1))\n\n                    except ValueError:\n                        logger.warning(\n                            f\"LinearOrchestrator {self.id}: Invalid agent index: {manager_result.output.get('content').get('result', -1)}\"  # noqa: E501\n                        )\n                        try:\n                            match = re.match(\n                                r\"^\\d+\",\n                                manager_result.output.get(\"content\").get(\"result\", -1),\n                            )\n                            assigned_agent_index = int(match.group())\n                        except Exception as e:\n                            logger.error(f\"LinearOrchestrator {self.id}: Failed to extract agent index: {e}\")\n                            assigned_agent_index = -1\n\n                    if 0 &lt;= assigned_agent_index &lt; len(self.agents):\n                        assigned_agent = self.agents[assigned_agent_index]\n                        logger.debug(\n                            f\"LinearOrchestrator {self.id}: Execute task {task.id}.{task.name} \"\n                            f\"by agent {assigned_agent_index}.{assigned_agent.name}\"\n                        )\n\n                        result = assigned_agent.run(\n                            input_data={\"input\": task_per_llm},\n                            config=config,\n                            run_depends=self._run_depends,\n                            **kwargs,\n                        )\n                        self._run_depends = [NodeDependency(node=assigned_agent).to_dict()]\n                        if result.status != RunnableStatus.SUCCESS:\n                            raise ValueError(\n                                f\"Failed to execute task {task.id}.{task.name} \"\n                                f\"by agent {assigned_agent_index}.{assigned_agent.name}\"\n                            )\n\n                        self._results[task.id] = {\n                            \"name\": task.name,\n                            \"result\": result.output[\"content\"],\n                        }\n                        logger.debug(\n                            f\"LinearOrchestrator {self.id}: Task {task.id}.{task.name} \"\n                            f\"executed by agent {assigned_agent_index}\"\n                        )\n                        logger.debug(\n                            f\"LinearOrchestrator {self.id}: Task {task.id}.{task.name}\\\n                                output: {result.output['content']}\"\n                        )\n                        success_flag = True\n                        break\n                task_per_llm += f\"Error occured {manager_result.output}\"\n\n            if success_flag:\n                continue\n\n            else:\n                raise ValueError(f\"Failed to assign task {task.id}.{task.name} by Manager Agent\")\n\n    def execute(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; dict:\n        \"\"\"\n        Execute the LinearOrchestrator flow.\n\n        Args:\n            input_data (Any): The input data containing the objective or additional context.\n            config (Optional[RunnableConfig]): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: The result of the orchestration process.\n        \"\"\"\n        self.reset_run_state()\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        self.input_task = input_data.get(\"input\") or self.input_task\n\n        logger.debug(\n            f\"LinearOrchestrator {self.id}: starting the flow with input_task:\\n```{self.input_task}```\"\n        )\n        run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        run_kwargs.pop(\"run_depends\", None)\n        if self.streaming.enabled:\n            self.manager.streaming = self.streaming\n            for agent in self.agents:\n                agent.streaming = self.streaming\n\n        tasks = self.get_tasks(config=config, **run_kwargs)\n        logger.debug(f\"LinearOrchestrator {self.id}: tasks initialized:\\n '{tasks}'\")\n        self.run_tasks(tasks=tasks, config=config, **run_kwargs)\n        logger.debug(f\"LinearOrchestrator {self.id}: tasks assigned and executed.\")\n        result = self.get_final_result(config=config, **run_kwargs)\n\n        logger.debug(f\"LinearOrchestrator {self.id}: output collected\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.agents_descriptions","title":"<code>agents_descriptions: str</code>  <code>cached</code> <code>property</code>","text":"<p>Generate a string description of all agents.</p>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the LinearOrchestrator flow.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data containing the objective or additional context.</p> required <code>config</code> <code>Optional[RunnableConfig]</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, Any]: The result of the orchestration process.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def execute(self, input_data: Any, config: RunnableConfig = None, **kwargs) -&gt; dict:\n    \"\"\"\n    Execute the LinearOrchestrator flow.\n\n    Args:\n        input_data (Any): The input data containing the objective or additional context.\n        config (Optional[RunnableConfig]): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: The result of the orchestration process.\n    \"\"\"\n    self.reset_run_state()\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    self.input_task = input_data.get(\"input\") or self.input_task\n\n    logger.debug(\n        f\"LinearOrchestrator {self.id}: starting the flow with input_task:\\n```{self.input_task}```\"\n    )\n    run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    run_kwargs.pop(\"run_depends\", None)\n    if self.streaming.enabled:\n        self.manager.streaming = self.streaming\n        for agent in self.agents:\n            agent.streaming = self.streaming\n\n    tasks = self.get_tasks(config=config, **run_kwargs)\n    logger.debug(f\"LinearOrchestrator {self.id}: tasks initialized:\\n '{tasks}'\")\n    self.run_tasks(tasks=tasks, config=config, **run_kwargs)\n    logger.debug(f\"LinearOrchestrator {self.id}: tasks assigned and executed.\")\n    result = self.get_final_result(config=config, **run_kwargs)\n\n    logger.debug(f\"LinearOrchestrator {self.id}: output collected\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.get_dependency_outputs","title":"<code>get_dependency_outputs(dependencies)</code>","text":"<p>Format the outputs of dependent tasks.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def get_dependency_outputs(self, dependencies: list[int]) -&gt; str:\n    \"\"\"Format the outputs of dependent tasks.\"\"\"\n    if not dependencies:\n        return \"\"\n\n    dependencies_formatted = \"**Here is the previously collected information:**\\n\"\n    for dep in dependencies:\n        if dep in self._results:\n            task_name = self._results[dep][\"name\"]\n            task_result = str(self._results[dep][\"result\"])\n            dependencies_formatted += (\n                f\"**Task:** {task_name}\\n**Result:** {task_result}\\n\\n\"\n            )\n\n    return dependencies_formatted.strip()\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.get_final_result","title":"<code>get_final_result(config=None, **kwargs)</code>","text":"<p>Generate the final result.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def get_final_result(self, config: RunnableConfig = None, **kwargs) -&gt; str:\n    \"\"\"Generate the final result.\"\"\"\n    final_task_output = \"\\n\\n\".join(\n        f\"**Task:** {result['name']}\\n**Result:** {result['result']}\"\n        for result in self._results.values()\n        if result\n    )\n\n    if self.use_summarizer:\n        if not self.summarize_all_answers:\n            final_task_id = max(self._results.keys(), default=None)\n            logger.debug(f\"LinearOrchestrator {self.id}: Final task id: {final_task_id}\")\n\n            if final_task_id is not None:\n                final_task_output = self._results[final_task_id].get(\"result\", \"\")\n\n            logger.debug(f\"LinearOrchestrator {self.id}: Final task output: {final_task_output}\")\n\n        logger.debug(f\"LinearOrchestrator {self.id}: Running final summarizer\")\n        manager_result = self.manager.run(\n            input_data={\n                \"action\": \"final\",\n                \"input_task\": self.input_task,\n                \"tasks_outputs\": final_task_output,\n            },\n            config=config,\n            run_depends=self._run_depends,\n            **kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n        if manager_result.status != RunnableStatus.SUCCESS:\n            raise ValueError(\"Agent LLM failed to generate final result\")\n\n        result = manager_result.output.get(\"content\")\n    else:\n        logger.debug(f\"LinearOrchestrator {self.id}: Returning final task output\")\n        result = final_task_output\n\n    return result\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.get_tasks","title":"<code>get_tasks(config=None, **kwargs)</code>","text":"<p>Generate tasks using the manager agent.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def get_tasks(self, config: RunnableConfig = None, **kwargs) -&gt; list[Task]:\n    \"\"\"Generate tasks using the manager agent.\"\"\"\n    manager_result = self.manager.run(\n        input_data={\n            \"action\": \"plan\",\n            \"input_task\": self.input_task,\n            \"agents\": self.agents_descriptions,\n        },\n        config=config,\n        run_depends=self._run_depends,\n        **kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n    if manager_result.status != RunnableStatus.SUCCESS:\n        raise ValueError(\"Agent LLM failed to generate tasks\")\n\n    manager_result_content = manager_result.output.get(\"content\").get(\"result\")\n    logger.debug(\n        f\"LinearOrchestrator {self.id}: Manager plan result content: {manager_result_content}\"\n    )\n\n    tasks = self.parse_tasks_from_output(manager_result_content)\n    logger.debug(f\"LinearOrchestrator {self.id}: Task list JSON: {tasks}\")\n\n    return tasks\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize components for the manager and agents.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"Initialize components for the manager and agents.\"\"\"\n    super().init_components(connection_manager)\n    if self.manager.is_postponed_component_init:\n        self.manager.init_components(connection_manager)\n\n    for agent in self.agents:\n        if agent.is_postponed_component_init:\n            agent.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.parse_tasks_from_output","title":"<code>parse_tasks_from_output(output)</code>","text":"<p>Parse tasks from the manager's output string.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def parse_tasks_from_output(self, output: str) -&gt; list[Task]:\n    \"\"\"Parse tasks from the manager's output string.\"\"\"\n    # Remove 'output' XML tags if present\n    if \"&lt;output&gt;\" in output and \"&lt;/output&gt;\" in output:\n        output = output.split(\"&lt;output&gt;\")[1].split(\"&lt;/output&gt;\")[0]\n\n    # Remove '```' code block markers and 'json' keyword if present\n    try:\n        output = output.replace(\"```\", \"\").replace(\"json\", \"\")\n    except AttributeError as e:\n        logger.warning(\n            f\"LinearOrchestrator {self.id}: Failed to remove code block markers and 'json' keyword from output {e}\"\n        )\n\n    # Parse the JSON string\n    try:\n        task_list_json = output.strip()\n    except AttributeError as e:\n        logger.warning(\n            f\"LinearOrchestrator {self.id}: Failed to strip the output string: {e}\"\n        )\n        task_list_json = output\n    return TypeAdapter(list[Task]).validate_json(task_list_json)\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.run_tasks","title":"<code>run_tasks(tasks, config=None, **kwargs)</code>","text":"<p>Execute the tasks using appropriate agents.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def run_tasks(\n    self, tasks: list[Task], config: RunnableConfig = None, **kwargs\n) -&gt; None:\n    \"\"\"Execute the tasks using appropriate agents.\"\"\"\n    logger.debug(\n        f\"LinearOrchestrator {self.id}: Assigning and executing tasks. Agents: {self.agents_descriptions}\"\n    )\n\n    for task in tasks:\n        task_per_llm = f\"**{task.description}**\\n**Required information for output**: {task.output}\"\n        logger.debug(\n            f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} prepared for LLM: {task_per_llm}\"\n        )\n\n        dependency_outputs = self.get_dependency_outputs(task.dependencies)\n        if dependency_outputs:\n            task_per_llm += f\"\\n{dependency_outputs}\"\n            logger.debug(\n                f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} \"\n                f\"prepared for LLM with dependencies: {task_per_llm}\"\n            )\n\n        logger.debug(\n            f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} \"\n            f\"with dependencies: {task.dependencies}\"\n        )\n        logger.debug(\n            f\"LinearOrchestrator {self.id}: task {task.id}.{task.name} \"\n            f\"task dependencies output: ```{dependency_outputs}```\"\n        )\n\n        success_flag = False\n        for _ in range(self.manager.max_loops):\n            manager_result = self.manager.run(\n                input_data={\n                    \"action\": \"assign\",\n                    \"input_task\": self.input_task,\n                    \"task\": task_per_llm,\n                    \"agents\": self.agents_descriptions,\n                },\n                config=config,\n                run_depends=self._run_depends,\n                **kwargs,\n            )\n            self._run_depends = [NodeDependency(node=self.manager).to_dict()]\n\n            logger.debug(f\"LinearOrchestrator {self.id}: Assigner LLM result: {manager_result}\")\n\n            if manager_result.status == RunnableStatus.SUCCESS:\n                try:\n                    assigned_agent_index = int(manager_result.output.get(\"content\").get(\"result\", -1))\n\n                except ValueError:\n                    logger.warning(\n                        f\"LinearOrchestrator {self.id}: Invalid agent index: {manager_result.output.get('content').get('result', -1)}\"  # noqa: E501\n                    )\n                    try:\n                        match = re.match(\n                            r\"^\\d+\",\n                            manager_result.output.get(\"content\").get(\"result\", -1),\n                        )\n                        assigned_agent_index = int(match.group())\n                    except Exception as e:\n                        logger.error(f\"LinearOrchestrator {self.id}: Failed to extract agent index: {e}\")\n                        assigned_agent_index = -1\n\n                if 0 &lt;= assigned_agent_index &lt; len(self.agents):\n                    assigned_agent = self.agents[assigned_agent_index]\n                    logger.debug(\n                        f\"LinearOrchestrator {self.id}: Execute task {task.id}.{task.name} \"\n                        f\"by agent {assigned_agent_index}.{assigned_agent.name}\"\n                    )\n\n                    result = assigned_agent.run(\n                        input_data={\"input\": task_per_llm},\n                        config=config,\n                        run_depends=self._run_depends,\n                        **kwargs,\n                    )\n                    self._run_depends = [NodeDependency(node=assigned_agent).to_dict()]\n                    if result.status != RunnableStatus.SUCCESS:\n                        raise ValueError(\n                            f\"Failed to execute task {task.id}.{task.name} \"\n                            f\"by agent {assigned_agent_index}.{assigned_agent.name}\"\n                        )\n\n                    self._results[task.id] = {\n                        \"name\": task.name,\n                        \"result\": result.output[\"content\"],\n                    }\n                    logger.debug(\n                        f\"LinearOrchestrator {self.id}: Task {task.id}.{task.name} \"\n                        f\"executed by agent {assigned_agent_index}\"\n                    )\n                    logger.debug(\n                        f\"LinearOrchestrator {self.id}: Task {task.id}.{task.name}\\\n                            output: {result.output['content']}\"\n                    )\n                    success_flag = True\n                    break\n            task_per_llm += f\"Error occured {manager_result.output}\"\n\n        if success_flag:\n            continue\n\n        else:\n            raise ValueError(f\"Failed to assign task {task.id}.{task.name} by Manager Agent\")\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.LinearOrchestrator.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"manager\"] = self.manager.to_dict(**kwargs)\n    data[\"agents\"] = [agent.to_dict(**kwargs) for agent in self.agents]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear/#dynamiq.nodes.agents.orchestrators.linear.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single task in the LinearOrchestrator system.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the task.</p> <code>name</code> <code>str</code> <p>Name of the task.</p> <code>description</code> <code>str</code> <p>Detailed description of the task.</p> <code>dependencies</code> <code>list[int]</code> <p>List of task IDs that this task depends on.</p> <code>output</code> <code>dict[str, Any] | str</code> <p>dict[str, Any] | str: Expected output of the task.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear.py</code> <pre><code>class Task(BaseModel):\n    \"\"\"\n    Represents a single task in the LinearOrchestrator system.\n\n    Attributes:\n        id (int): Unique identifier for the task.\n        name (str): Name of the task.\n        description (str): Detailed description of the task.\n        dependencies (list[int]): List of task IDs that this task depends on.\n        output: dict[str, Any] | str: Expected output of the task.\n    \"\"\"\n\n    id: int\n    name: str\n    description: str\n    dependencies: list[int]\n    output: dict[str, Any] | str\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear_manager/","title":"Linear manager","text":""},{"location":"dynamiq/nodes/agents/orchestrators/linear_manager/#dynamiq.nodes.agents.orchestrators.linear_manager.LinearAgentManager","title":"<code>LinearAgentManager</code>","text":"<p>               Bases: <code>AgentManager</code></p> <p>A specialized AgentManager that manages tasks in a linear, sequential order. It uses predefined prompts to plan tasks, assign them to the appropriate agents, and compile the final result.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear_manager.py</code> <pre><code>class LinearAgentManager(AgentManager):\n    \"\"\"\n    A specialized AgentManager that manages tasks in a linear, sequential order.\n    It uses predefined prompts to plan tasks, assign them to the appropriate agents,\n    and compile the final result.\n    \"\"\"\n\n    name: str = \"Linear Manager\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LinearAgentManager and sets up the prompt blocks.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._init_prompt_blocks()\n\n    def _init_prompt_blocks(self):\n        \"\"\"\n        Initializes the prompt blocks used in the task planning, assigning,\n        and final answer generation processes.\n        \"\"\"\n        super()._init_prompt_blocks()\n        self._prompt_blocks.update(\n            {\n                \"plan\": self._get_linear_plan_prompt(),\n                \"assign\": self._get_linear_assign_prompt(),\n                \"final\": self._get_linear_final_prompt(),\n                \"run\": self._get_linear_agent_run(),\n            }\n        )\n\n    @staticmethod\n    def _get_linear_plan_prompt() -&gt; str:\n        \"\"\"\n        Returns the prompt template for planning tasks.\n        \"\"\"\n        return PROMPT_TEMPLATE_AGENT_MANAGER_LINEAR_PLAN\n\n    @staticmethod\n    def _get_linear_assign_prompt() -&gt; str:\n        \"\"\"\n        Returns the prompt template for assigning tasks to agents.\n        \"\"\"\n        return PROMPT_TEMPLATE_AGENT_MANAGER_LINEAR_ASSIGN\n\n    @staticmethod\n    def _get_linear_final_prompt() -&gt; str:\n        \"\"\"\n        Returns the prompt template for generating the final answer.\n        \"\"\"\n        return PROMPT_TEMPLATE_AGENT_MANAGER_LINEAR_FINAL_ANSWER\n\n    @staticmethod\n    def _get_linear_agent_run() -&gt; str:\n        \"\"\"\n        Returns the prompt template for question answering by Linear Manager.\n        \"\"\"\n        return PROMPT_TEMPLATE_MANAGER_LINEAR_RUN\n</code></pre>"},{"location":"dynamiq/nodes/agents/orchestrators/linear_manager/#dynamiq.nodes.agents.orchestrators.linear_manager.LinearAgentManager.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LinearAgentManager and sets up the prompt blocks.</p> Source code in <code>dynamiq/nodes/agents/orchestrators/linear_manager.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LinearAgentManager and sets up the prompt blocks.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._init_prompt_blocks()\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/","title":"Elevenlabs","text":""},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsSTS","title":"<code>ElevenLabsSTS</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for vocalizing text using the ElevenLabs API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[AUDIO]</code> <p>The group the node belongs to. name (str): The name of the node.</p> <code>connection</code> <code>ElevenLabs | None</code> <p>The connection to the ElevenLabs API. A new connection is created if none is provided.</p> <code>voice_id</code> <code>Voices | str | None</code> <p>The voice identifier, that should be used for vocalizing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>model(str)</code> <code>ErrorHandling</code> <p>The model for vocalizing, defaults to \"eleven_english_sts_v2\".</p> <code>stability(float)</code> <code>ErrorHandling</code> <p>The slider determines how stable the voice is and the randomness between each generation.</p> <code>similarity_boost(float)</code> <code>ErrorHandling</code> <p>The slider dictates how closely the AI should adhere to the original voice when attempting to replicate it.</p> <code>style(float)</code> <code>ErrorHandling</code> <p>The setting that attempts to amplify the style of the original speaker.</p> <code>use_speaker_boost(bool)</code> <code>ErrorHandling</code> <p>The setting for boosting the similarity to the original speaker</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>class ElevenLabsSTS(ConnectionNode):\n    \"\"\"\n    A component for vocalizing text using the ElevenLabs API.\n\n    Attributes:\n        group (Literal[NodeGroup.AUDIO]): The group the node belongs to. name (str): The name of the node.\n        connection (ElevenLabsConnection | None): The connection to the ElevenLabs API. A new connection is created if\n            none is provided.\n        voice_id: The voice identifier, that should be used for vocalizing.\n        error_handling (ErrorHandling): Error handling configuration.\n        model(str): The model for vocalizing, defaults to \"eleven_english_sts_v2\".\n        stability(float): The slider determines how stable the voice is and the randomness\n            between each generation.\n        similarity_boost(float): The slider dictates how closely the AI should adhere to the original voice when\n            attempting to replicate it.\n        style(float): The setting that attempts to amplify the style of the original speaker.\n        use_speaker_boost(bool):The setting for boosting the similarity to the original speaker\n    \"\"\"\n\n    group: Literal[NodeGroup.AUDIO] = NodeGroup.AUDIO\n    name: str = \"ElevenLabsSTS\"\n    voice_id: Voices | str | None = Voices.Rachel\n    connection: ElevenLabsConnection | None = None\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    model: str = \"eleven_english_sts_v2\"\n    stability: float = 0.5\n    similarity_boost: float = 0.5\n    style: float = 0\n    use_speaker_boost: bool = True\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ElevenLabs audio generation.\n\n        If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = ElevenLabsConnection()\n        super().__init__(**kwargs)\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, bytes]:\n        \"\"\"Execute the audio generation process.\n\n        This method takes input data and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing audio that should be vocalized. Audio\n                can be BytesIO or bytes format only.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n             dict: A dictionary with the following key:\n                - \"content\" (bytes): Bytes containing the audio generation result.\n        \"\"\"\n        input_dict = {\n            \"model_id\": self.model,\n            \"voice_settings\": json.dumps(\n                {\n                    \"stability\": self.stability,\n                    \"similarity_boost\": self.similarity_boost,\n                    \"style\": self.style,\n                    \"use_speaker_boost\": self.use_speaker_boost,\n                }\n            ),\n        }\n        audio = input_data[\"audio\"]\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        response = self.client.request(\n            method=self.connection.method,\n            url=format_url(\"speech-to-speech/\", self.connection.url, self.voice_id),\n            headers=self.connection.headers,\n            data=input_dict | self.connection.data,\n            files={\"audio\": audio},\n        )\n        if response.status_code != 200:\n            response.raise_for_status()\n        return {\n            \"content\": response.content,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsSTS.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ElevenLabs audio generation.</p> <p>If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ElevenLabs audio generation.\n\n    If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = ElevenLabsConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsSTS.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the audio generation process.</p> <p>This method takes input data and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing audio that should be vocalized. Audio can be BytesIO or bytes format only.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bytes]</code> <p>A dictionary with the following key: - \"content\" (bytes): Bytes containing the audio generation result.</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, bytes]:\n    \"\"\"Execute the audio generation process.\n\n    This method takes input data and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing audio that should be vocalized. Audio\n            can be BytesIO or bytes format only.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n         dict: A dictionary with the following key:\n            - \"content\" (bytes): Bytes containing the audio generation result.\n    \"\"\"\n    input_dict = {\n        \"model_id\": self.model,\n        \"voice_settings\": json.dumps(\n            {\n                \"stability\": self.stability,\n                \"similarity_boost\": self.similarity_boost,\n                \"style\": self.style,\n                \"use_speaker_boost\": self.use_speaker_boost,\n            }\n        ),\n    }\n    audio = input_data[\"audio\"]\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    response = self.client.request(\n        method=self.connection.method,\n        url=format_url(\"speech-to-speech/\", self.connection.url, self.voice_id),\n        headers=self.connection.headers,\n        data=input_dict | self.connection.data,\n        files={\"audio\": audio},\n    )\n    if response.status_code != 200:\n        response.raise_for_status()\n    return {\n        \"content\": response.content,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsTTS","title":"<code>ElevenLabsTTS</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for vocalizing text using the ElevenLabs API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[AUDIO]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>ElevenLabs | None</code> <p>The connection to the ElevenLabs API. A new connection is created if none is provided.</p> <code>voice_id</code> <code>Voices | str | None</code> <p>The voice identifier, that should be used for vocalizing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>model(str)</code> <code>ErrorHandling</code> <p>The model for vocalizing, defaults to \"eleven_monolingual_v1\"</p> <code>stability(float)</code> <code>ErrorHandling</code> <p>The slider determines how stable the voice is and the randomness</p> <code>similarity_boost(float)</code> <code>ErrorHandling</code> <p>The slider dictates how closely the AI should adhere to the original voice when</p> <code>style(float)</code> <code>ErrorHandling</code> <p>The setting that attempts to amplify the style of the original speaker.</p> <code>use_speaker_boost(bool)</code> <code>ErrorHandling</code> <p>The setting for boosting the similarity to the original speaker</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>class ElevenLabsTTS(ConnectionNode):\n    \"\"\"\n    A component for vocalizing text using the ElevenLabs API.\n\n    Attributes:\n        group (Literal[NodeGroup.AUDIO]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (ElevenLabsConnection | None): The connection to the ElevenLabs API. A new connection\n            is created if none is provided.\n        voice_id: The voice identifier, that should be used for vocalizing.\n        error_handling (ErrorHandling): Error handling configuration.\n        model(str): The model for vocalizing, defaults to \"eleven_monolingual_v1\"\n        stability(float): The slider determines how stable the voice is and the randomness\n        between each generation.\n        similarity_boost(float): The slider dictates how closely the AI should adhere to the original voice when\n        attempting to replicate it.\n        style(float): The setting that attempts to amplify the style of the original speaker.\n        use_speaker_boost(bool):The setting for boosting the similarity to the original speaker\n    \"\"\"\n\n    group: Literal[NodeGroup.AUDIO] = NodeGroup.AUDIO\n    name: str = \"ElevenLabsTTS\"\n    voice_id: Voices | str | None = Voices.Rachel\n    connection: ElevenLabsConnection | None = None\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    model: str = \"eleven_monolingual_v1\"\n    stability: float = 0.5\n    similarity_boost: float = 0.5\n    style: float = 0\n    use_speaker_boost: bool = True\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the ElevenLabs audio generation.\n\n        If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = ElevenLabsConnection()\n        super().__init__(**kwargs)\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, bytes]:\n        \"\"\"Execute the audio generation process.\n\n        This method takes input data and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n             dict: A dictionary with the following key:\n                - \"content\" (bytes): Bytes containing the audio generation result.\n        \"\"\"\n        input_dict = {\n            \"model_id\": self.model,\n            \"text\": input_data[\"text\"],\n            \"voice_settings\": {\n                \"stability\": self.stability,\n                \"similarity_boost\": self.similarity_boost,\n                \"style\": self.style,\n                \"use_speaker_boost\": self.use_speaker_boost,\n            },\n        }\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        response = self.client.request(\n            method=self.connection.method,\n            url=format_url(\"text-to-speech/\", self.connection.url, self.voice_id),\n            json={**input_dict, **self.connection.data},\n            headers=self.connection.headers,\n        )\n        if response.status_code != 200:\n            response.raise_for_status()\n        return {\n            \"content\": response.content,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsTTS.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ElevenLabs audio generation.</p> <p>If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the ElevenLabs audio generation.\n\n    If neither client nor connection is provided in kwargs, a new ElevenLabs connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = ElevenLabsConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.ElevenLabsTTS.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the audio generation process.</p> <p>This method takes input data and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bytes]</code> <p>A dictionary with the following key: - \"content\" (bytes): Bytes containing the audio generation result.</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, bytes]:\n    \"\"\"Execute the audio generation process.\n\n    This method takes input data and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n         dict: A dictionary with the following key:\n            - \"content\" (bytes): Bytes containing the audio generation result.\n    \"\"\"\n    input_dict = {\n        \"model_id\": self.model,\n        \"text\": input_data[\"text\"],\n        \"voice_settings\": {\n            \"stability\": self.stability,\n            \"similarity_boost\": self.similarity_boost,\n            \"style\": self.style,\n            \"use_speaker_boost\": self.use_speaker_boost,\n        },\n    }\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    response = self.client.request(\n        method=self.connection.method,\n        url=format_url(\"text-to-speech/\", self.connection.url, self.voice_id),\n        json={**input_dict, **self.connection.data},\n        headers=self.connection.headers,\n    )\n    if response.status_code != 200:\n        response.raise_for_status()\n    return {\n        \"content\": response.content,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/audio/elevenlabs/#dynamiq.nodes.audio.elevenlabs.format_url","title":"<code>format_url(method, url, voice_id)</code>","text":"<p>Formats the given URL by including the <code>voice_id</code> if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>type of request for vocalizer</p> required <code>voice_id</code> <code>str</code> <p>voice id for vocalizer.</p> required <code>url</code> <code>str</code> <p>The URL to format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The modified URL.</p> Source code in <code>dynamiq/nodes/audio/elevenlabs.py</code> <pre><code>def format_url(method: str, url: str, voice_id: str) -&gt; str:\n    \"\"\"Formats the given URL by including the `voice_id` if necessary.\n\n    Args:\n        method: type of request for vocalizer\n        voice_id: voice id for vocalizer.\n        url (str): The URL to format.\n\n    Returns:\n        str: The modified URL.\n    \"\"\"\n    if url.rstrip(\"/\").endswith(\"v1\"):\n        url = urljoin(url, method)\n    if \"{voice_id}\" in url:\n        url = url.format(voice_id=voice_id)\n    elif url.rstrip(\"/\").endswith(\"text-to-speech\") or url.rstrip(\"/\").endswith(\n        \"speech-to-speech\"\n    ):\n        url = urljoin(url, voice_id)\n    return url\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/","title":"Whisper","text":""},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT","title":"<code>WhisperSTT</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for transcribing audio files using the Whisper speech recognition system.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[AUDIO]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Whisper | None</code> <p>The connection to the Whisper API.A new connection is created if none is provided.</p> <code>client</code> <code>OpenAIClient | None</code> <p>The OpenAI client instance.</p> <code>model</code> <code>str</code> <p>The model name to use for transcribing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>class WhisperSTT(ConnectionNode):\n    \"\"\"\n    A component for transcribing audio files using the Whisper speech recognition system.\n\n    Attributes:\n        group (Literal[NodeGroup.AUDIO]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (WhisperConnection | None): The connection to the Whisper API.A new connection\n            is created if none is provided.\n        client (OpenAIClient | None): The OpenAI client instance.\n        model (str): The model name to use for transcribing.\n        error_handling (ErrorHandling): Error handling configuration.\n    \"\"\"\n\n    group: Literal[NodeGroup.AUDIO] = NodeGroup.AUDIO\n    name: str = \"Whisper\"\n    model: str\n    connection: WhisperConnection | OpenAIConnection | None = None\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    default_file_name: str = DEFAULT_FILE_NAME\n    default_content_type: str = DEFAULT_CONTENT_TYPE\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Whisper transcriber.\n\n        If neither client nor connection is provided in kwargs, a new Whisper connection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WhisperConnection()\n        super().__init__(**kwargs)\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"Execute the audio transcribing process.\n\n        This method takes input data, modifies it(if necessary), and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: A string containing the transcribe result.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        audio = input_data[\"audio\"]\n        if isinstance(audio, bytes):\n            audio = io.BytesIO(audio)\n\n        if not isinstance(audio, io.BytesIO):\n            raise ValueError(\"Audio must be a BytesIO object or bytes.\")\n\n        audio.name = getattr(audio, \"name\", self.default_file_name)\n        audio.content_type = getattr(audio, \"content_type\", self.default_content_type)\n\n        if isinstance(self.connection, WhisperConnection):\n            transcription = self.get_transcription_with_http_request(model=self.model, audio=audio)\n        elif isinstance(self.connection, OpenAIConnection):\n            transcription = self.get_transcription_with_openai_client(model=self.model, audio=audio)\n        else:\n            raise ValueError(f\"Connection type {type(self.connection)} does not fit required ones.\")\n\n        return {\"content\": transcription.get(\"text\", \"\")}\n\n    def get_transcription_with_http_request(self, model: str, audio: io.BytesIO):\n        \"\"\"Get the audio transcription by request.\n\n        This method takes whisper model and audio file, sends request with defined params, and returns the\n        transcription.\n\n        Args:\n            model(str): The model used for transcribing.\n            audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n        Returns:\n            dict: transcription result.\n        \"\"\"\n        connection_url = urljoin(self.connection.url, \"audio/transcriptions\")\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            headers=self.connection.headers,\n            params=self.connection.params,\n            data=self.connection.data | {\"model\": model},\n            files={\"file\": (audio.name, audio, audio.content_type)},\n        )\n        if response.status_code != 200:\n            response.raise_for_status()\n\n        return response.json()\n\n    def get_transcription_with_openai_client(self, model: str, audio: io.BytesIO):\n        \"\"\"Get the audio transcription by request.\n\n        This method takes whisper model and audio file, sends request with defined params, and returns the\n        transcription.\n\n        Args:\n            model(str): The model used for transcribing.\n            audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n        Returns:\n            dict: transcription result.\n        \"\"\"\n        response = self.client.audio.transcriptions.create(model=model, file=audio)\n\n        return {\"text\": response.text}\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Whisper transcriber.</p> <p>If neither client nor connection is provided in kwargs, a new Whisper connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Whisper transcriber.\n\n    If neither client nor connection is provided in kwargs, a new Whisper connection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WhisperConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the audio transcribing process.</p> <p>This method takes input data, modifies it(if necessary), and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A string containing the transcribe result.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"Execute the audio transcribing process.\n\n    This method takes input data, modifies it(if necessary), and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str: A string containing the transcribe result.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    audio = input_data[\"audio\"]\n    if isinstance(audio, bytes):\n        audio = io.BytesIO(audio)\n\n    if not isinstance(audio, io.BytesIO):\n        raise ValueError(\"Audio must be a BytesIO object or bytes.\")\n\n    audio.name = getattr(audio, \"name\", self.default_file_name)\n    audio.content_type = getattr(audio, \"content_type\", self.default_content_type)\n\n    if isinstance(self.connection, WhisperConnection):\n        transcription = self.get_transcription_with_http_request(model=self.model, audio=audio)\n    elif isinstance(self.connection, OpenAIConnection):\n        transcription = self.get_transcription_with_openai_client(model=self.model, audio=audio)\n    else:\n        raise ValueError(f\"Connection type {type(self.connection)} does not fit required ones.\")\n\n    return {\"content\": transcription.get(\"text\", \"\")}\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.get_transcription_with_http_request","title":"<code>get_transcription_with_http_request(model, audio)</code>","text":"<p>Get the audio transcription by request.</p> <p>This method takes whisper model and audio file, sends request with defined params, and returns the transcription.</p> <p>Parameters:</p> Name Type Description Default <code>model(str)</code> <p>The model used for transcribing.</p> required <code>audio(io.BytesIO)</code> <p>The audio file in BytesIO that should be transcribed</p> required <p>Returns:     dict: transcription result.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def get_transcription_with_http_request(self, model: str, audio: io.BytesIO):\n    \"\"\"Get the audio transcription by request.\n\n    This method takes whisper model and audio file, sends request with defined params, and returns the\n    transcription.\n\n    Args:\n        model(str): The model used for transcribing.\n        audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n    Returns:\n        dict: transcription result.\n    \"\"\"\n    connection_url = urljoin(self.connection.url, \"audio/transcriptions\")\n    response = self.client.request(\n        method=self.connection.method,\n        url=connection_url,\n        headers=self.connection.headers,\n        params=self.connection.params,\n        data=self.connection.data | {\"model\": model},\n        files={\"file\": (audio.name, audio, audio.content_type)},\n    )\n    if response.status_code != 200:\n        response.raise_for_status()\n\n    return response.json()\n</code></pre>"},{"location":"dynamiq/nodes/audio/whisper/#dynamiq.nodes.audio.whisper.WhisperSTT.get_transcription_with_openai_client","title":"<code>get_transcription_with_openai_client(model, audio)</code>","text":"<p>Get the audio transcription by request.</p> <p>This method takes whisper model and audio file, sends request with defined params, and returns the transcription.</p> <p>Parameters:</p> Name Type Description Default <code>model(str)</code> <p>The model used for transcribing.</p> required <code>audio(io.BytesIO)</code> <p>The audio file in BytesIO that should be transcribed</p> required <p>Returns:     dict: transcription result.</p> Source code in <code>dynamiq/nodes/audio/whisper.py</code> <pre><code>def get_transcription_with_openai_client(self, model: str, audio: io.BytesIO):\n    \"\"\"Get the audio transcription by request.\n\n    This method takes whisper model and audio file, sends request with defined params, and returns the\n    transcription.\n\n    Args:\n        model(str): The model used for transcribing.\n        audio(io.BytesIO): The audio file in BytesIO that should be transcribed\n    Returns:\n        dict: transcription result.\n    \"\"\"\n    response = self.client.audio.transcriptions.create(model=model, file=audio)\n\n    return {\"text\": response.text}\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/","title":"Llm text extractor","text":""},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter","title":"<code>LLMImageConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node class for extracting text from images using a Large Language Model (LLM).</p> <p>This class extracts text from the images using an LLM and saves the text as documents with metadata.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[CONVERTERS]</code> <p>The group the node belongs to. Default is NodeGroup.CONVERTERS.</p> <code>name</code> <code>str</code> <p>The name of the node. Default is \"LLMImageConverter\".</p> <code>extraction_instruction</code> <code>str</code> <p>The instruction for text extraction. Default is DEFAULT_EXTRACTION_INSTRUCTION.</p> <code>document_creation_mode</code> <code>DocumentCreationMode</code> <p>The mode for document creation. Default is DocumentCreationMode.ONE_DOC_PER_FILE.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM instance used for text extraction. Default is None.</p> <p>Example:</p> <pre><code>from dynamiq.nodes.extractors import ImageLLMExtractor\nfrom io import BytesIO\n\n# Initialize the extractor\nextractor = ImageLLMExtractor(llm=my_llm_instance)\n\n# Example input data\ninput_data = {\n    \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n    \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\n# Execute the extractor\noutput = extractor.execute(input_data)\n\n# Output will be a dictionary with extracted documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>class LLMImageConverter(Node):\n    \"\"\"\n    A Node class for extracting text from images using a Large Language Model (LLM).\n\n    This class extracts text from the images using an LLM and saves the text as documents with metadata.\n\n    Attributes:\n        group (Literal[NodeGroup.CONVERTERS]): The group the node belongs to. Default is NodeGroup.CONVERTERS.\n        name (str): The name of the node. Default is \"LLMImageConverter\".\n        extraction_instruction (str): The instruction for text extraction.\n            Default is DEFAULT_EXTRACTION_INSTRUCTION.\n        document_creation_mode (DocumentCreationMode): The mode for document creation.\n            Default is DocumentCreationMode.ONE_DOC_PER_FILE.\n        llm (BaseLLM): The LLM instance used for text extraction. Default is None.\n\n    Example:\n\n        from dynamiq.nodes.extractors import ImageLLMExtractor\n        from io import BytesIO\n\n        # Initialize the extractor\n        extractor = ImageLLMExtractor(llm=my_llm_instance)\n\n        # Example input data\n        input_data = {\n            \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n            \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        # Execute the extractor\n        output = extractor.execute(input_data)\n\n        # Output will be a dictionary with extracted documents\n        print(output)\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"LLMImageConverter\"\n    extraction_instruction: str = DEFAULT_EXTRACTION_INSTRUCTION\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    llm: Node\n    vision_prompt: Prompt = Field(default_factory=create_vision_prompt_template)\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LLMImageConverter with the given parameters and creates a default LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the document extractor component.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Default is a new instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the image text extraction process.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the images to be processed.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the extracted documents.\n\n        Example:\n\n            input_data = {\n                \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n                \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n                \"metadata\": {\"source\": \"example source\"}\n            }\n\n            output = extractor.execute(input_data)\n\n            # output will be a dictionary with extracted documents\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = self.extract_text_from_images(\n            file_paths=input_data.get(\"file_paths\"),\n            files=input_data.get(\"files\"),\n            metadata=input_data.get(\"metadata\"),\n            config=config,\n            **kwargs,\n        )\n\n        return {\"documents\": documents}\n\n    def extract_text_from_images(\n        self,\n        file_paths: list[str] | None = None,\n        files: list[BytesIO] | None = None,\n        metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Extracts text from images using an LLM.\n\n        Args:\n            file_paths (list[str], optional): List of paths to image files. Default is None.\n            files (list[BytesIO], optional): List of image files as BytesIO objects. Default is None.\n            metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of extracted documents.\n        \"\"\"\n        if file_paths is None and files is None:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n\n        documents = []\n\n        if file_paths is not None:\n            paths_obj = [Path(path) for path in file_paths]\n            filepaths = [path for path in paths_obj if path.is_file()]\n            filepaths_in_directories = [\n                filepath\n                for path in paths_obj\n                if path.is_dir()\n                for filepath in path.glob(\"*.*\")\n                if filepath.is_file()\n            ]\n            if filepaths_in_directories and isinstance(metadata, list):\n                raise ValueError(\n                    \"If providing directories in the `file_paths` parameter, \"\n                    \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                    \"and not a list. To specify different metadata for each file, \"\n                    \"provide an explicit list of direct paths instead.\"\n                )\n\n            all_filepaths = filepaths + filepaths_in_directories\n            meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n            for file_path, meta in zip(all_filepaths, meta_list):\n                with open(file_path, \"rb\") as upload_file:\n                    file = BytesIO(upload_file.read())\n                    file.name = upload_file.name\n\n                image = self._load_image(file)\n                meta[\"filename\"] = str(file_path)\n                documents.extend(self._process_images([image], meta, config, **kwargs))\n\n        if files is not None:\n            meta_list = self._normalize_metadata(metadata, len(files))\n\n            for file, meta in zip(files, meta_list):\n                if not isinstance(file, BytesIO):\n                    raise ValueError(\"All files must be of type BytesIO.\")\n                image = self._load_image(file)\n                meta[\"filename\"] = get_filename_for_bytesio(file)\n                documents.extend(self._process_images([image], meta, config, **kwargs))\n\n        return documents\n\n    def _load_image(self, file: BytesIO) -&gt; \"Image\":\n        \"\"\"\n        Loads an image from a BytesIO object.\n\n        Args:\n            file (BytesIO): The BytesIO object containing the image data.\n\n        Returns:\n            Image: The loaded image.\n        \"\"\"\n        from PIL import Image\n\n        return Image.open(file)\n\n    def _process_images(\n        self,\n        images: list[\"Image\"],\n        metadata: dict[str, Any],\n        config: RunnableConfig,\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Extracts text from images using a vision prompt.\n\n        Args:\n            images (list[Image]): List of images.\n            metadata (dict[str, Any]): Metadata for the documents.\n            config (RunnableConfig): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of extracted documents.\n        \"\"\"\n        urls = self._convert_images_to_urls(images)\n\n        outputs = self.perform_llm_extraction(urls, config, **kwargs)\n\n        if self.document_creation_mode == DocumentCreationMode.ONE_DOC_PER_FILE:\n            document_content = \"\".join(output[\"content\"] for output in outputs)\n            return [Document(content=document_content, metadata=metadata)]\n        else:\n            documents = [\n                Document(content=output[\"content\"], metadata=metadata)\n                for output in outputs\n            ]\n            return documents\n\n    def perform_llm_extraction(\n        self, urls: list[str], config: RunnableConfig, **kwargs\n    ) -&gt; list[dict]:\n        \"\"\"\n        Performs the actual extraction of text from images using the LLM.\n\n        Args:\n            urls (list[str]): The list of image URLs to extract text from.\n            config (RunnableConfig): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[dict]: A list of extracted text results from the LLM.\n\n        Example:\n\n            urls = [\"data:image/jpeg;base64,...\", \"data:image/jpeg;base64,...\"]\n\n            extracted_texts = extractor.perform_llm_extraction(urls, config)\n\n            # extracted_texts will be a list of dictionaries with extracted text\n        \"\"\"\n        run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        inputs = [\n            {\"extraction_instruction\": self.extraction_instruction, \"img_url\": url}\n            for url in urls\n        ]\n\n        prompt = self.vision_prompt\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            llm_results = list(\n                executor.map(\n                    lambda input_data: self.call_llm(\n                        input_data, prompt, config, **run_kwargs\n                    ),\n                    inputs,\n                )\n            )\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} images\"\n        )\n\n        return llm_results\n\n    def call_llm(self, input_data, prompt, config, **run_kwargs):\n        \"\"\"\n        Calls the LLM with the given input data and prompt.\n\n        Args:\n            input_data (dict): The input data for the LLM.\n            prompt (Prompt): The prompt to be used with the LLM.\n            config (RunnableConfig): Configuration for the execution.\n            **run_kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: The result from the LLM.\n        \"\"\"\n        llm_result = self.llm.run(\n            input_data=input_data,\n            prompt=prompt,\n            config=config,\n            run_depends=self._run_depends,\n            **run_kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n\n        if llm_result.status != RunnableStatus.SUCCESS:\n            logger.error(f\"Node {self.name} - {self.id}: LLM execution failed\")\n            raise ValueError(\"ImageLLMExtractor LLM execution failed\")\n        return llm_result.output\n\n    @staticmethod\n    def _convert_image_to_url(image: \"Image\") -&gt; str:\n        \"\"\"\n        Converts a PIL Image to a base64-encoded URL.\n\n        Args:\n            image (Image): The image to convert.\n\n        Returns:\n            str: The base64-encoded URL of the image.\n        \"\"\"\n        # Ensure the image is in RGB mode (required for JPEG)\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        buffered = BytesIO()\n        image.save(buffered, format=\"JPEG\")\n        buffered.seek(0)  # Ensure the buffer is at the beginning\n        decoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n        url = f\"data:image/jpeg;base64,{decoded_image}\"\n        return url\n\n    @staticmethod\n    def _convert_images_to_urls(images: list[\"Image\"]) -&gt; list[str]:\n        \"\"\"\n        Converts a list of PIL Images to a list of base64-encoded URLs.\n\n        Args:\n            images (List[Image]): The list of images to convert.\n\n        Returns:\n            List[str]: The list of base64-encoded URLs.\n        \"\"\"\n        return [LLMImageConverter._convert_image_to_url(image) for image in images]\n\n    @staticmethod\n    def _normalize_metadata(\n        metadata: dict[str, Any] | list[dict[str, Any]] | None, sources_count: int\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Normalizes metadata input for a converter.\n\n        Given all possible values of the metadata input for a converter (None, dictionary, or list of\n        dicts), ensures to return a list of dictionaries of the correct length for the converter to use.\n\n        Args:\n            metadata: The meta input of the converter, as-is. Can be None, a dictionary, or a list of\n                dictionaries.\n            sources_count: The number of sources the converter received.\n\n        Returns:\n            A list of dictionaries of the same length as the sources list.\n\n        Raises:\n            ValueError: If metadata is not None, a dictionary, or a list of dictionaries, or if the length\n                of the metadata list doesn't match the number of sources.\n        \"\"\"\n        if metadata is None:\n            return [{} for _ in range(sources_count)]\n        if isinstance(metadata, dict):\n            return [copy.deepcopy(metadata) for _ in range(sources_count)]\n        if isinstance(metadata, list):\n            if sources_count != len(metadata):\n                raise ValueError(\n                    \"The length of the metadata list must match the number of sources.\"\n                )\n            return metadata\n        raise ValueError(\n            \"metadata must be either None, a dictionary or a list of dictionaries.\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LLMImageConverter with the given parameters and creates a default LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LLMImageConverter with the given parameters and creates a default LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.call_llm","title":"<code>call_llm(input_data, prompt, config, **run_kwargs)</code>","text":"<p>Calls the LLM with the given input data and prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>The input data for the LLM.</p> required <code>prompt</code> <code>Prompt</code> <p>The prompt to be used with the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**run_kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The result from the LLM.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def call_llm(self, input_data, prompt, config, **run_kwargs):\n    \"\"\"\n    Calls the LLM with the given input data and prompt.\n\n    Args:\n        input_data (dict): The input data for the LLM.\n        prompt (Prompt): The prompt to be used with the LLM.\n        config (RunnableConfig): Configuration for the execution.\n        **run_kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: The result from the LLM.\n    \"\"\"\n    llm_result = self.llm.run(\n        input_data=input_data,\n        prompt=prompt,\n        config=config,\n        run_depends=self._run_depends,\n        **run_kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n\n    if llm_result.status != RunnableStatus.SUCCESS:\n        logger.error(f\"Node {self.name} - {self.id}: LLM execution failed\")\n        raise ValueError(\"ImageLLMExtractor LLM execution failed\")\n    return llm_result.output\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the image text extraction process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the images to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the extracted documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n    \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\noutput = extractor.execute(input_data)\n\n# output will be a dictionary with extracted documents\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the image text extraction process.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the images to be processed.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the extracted documents.\n\n    Example:\n\n        input_data = {\n            \"file_paths\": [\"path/to/image1.jpeg\", \"path/to/image2.png\"],\n            \"files\": [BytesIO(b\"image1 content\"), BytesIO(b\"image2 content\")],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        output = extractor.execute(input_data)\n\n        # output will be a dictionary with extracted documents\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = self.extract_text_from_images(\n        file_paths=input_data.get(\"file_paths\"),\n        files=input_data.get(\"files\"),\n        metadata=input_data.get(\"metadata\"),\n        config=config,\n        **kwargs,\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.extract_text_from_images","title":"<code>extract_text_from_images(file_paths=None, files=None, metadata=None, config=None, **kwargs)</code>","text":"<p>Extracts text from images using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of paths to image files. Default is None.</p> <code>None</code> <code>files</code> <code>list[BytesIO]</code> <p>List of image files as BytesIO objects. Default is None.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | list[dict[str, Any]]</code> <p>Metadata for the documents. Default is None.</p> <code>None</code> <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of extracted documents.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def extract_text_from_images(\n    self,\n    file_paths: list[str] | None = None,\n    files: list[BytesIO] | None = None,\n    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; list[Document]:\n    \"\"\"\n    Extracts text from images using an LLM.\n\n    Args:\n        file_paths (list[str], optional): List of paths to image files. Default is None.\n        files (list[BytesIO], optional): List of image files as BytesIO objects. Default is None.\n        metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[Document]: A list of extracted documents.\n    \"\"\"\n    if file_paths is None and files is None:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n\n    documents = []\n\n    if file_paths is not None:\n        paths_obj = [Path(path) for path in file_paths]\n        filepaths = [path for path in paths_obj if path.is_file()]\n        filepaths_in_directories = [\n            filepath\n            for path in paths_obj\n            if path.is_dir()\n            for filepath in path.glob(\"*.*\")\n            if filepath.is_file()\n        ]\n        if filepaths_in_directories and isinstance(metadata, list):\n            raise ValueError(\n                \"If providing directories in the `file_paths` parameter, \"\n                \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                \"and not a list. To specify different metadata for each file, \"\n                \"provide an explicit list of direct paths instead.\"\n            )\n\n        all_filepaths = filepaths + filepaths_in_directories\n        meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n        for file_path, meta in zip(all_filepaths, meta_list):\n            with open(file_path, \"rb\") as upload_file:\n                file = BytesIO(upload_file.read())\n                file.name = upload_file.name\n\n            image = self._load_image(file)\n            meta[\"filename\"] = str(file_path)\n            documents.extend(self._process_images([image], meta, config, **kwargs))\n\n    if files is not None:\n        meta_list = self._normalize_metadata(metadata, len(files))\n\n        for file, meta in zip(files, meta_list):\n            if not isinstance(file, BytesIO):\n                raise ValueError(\"All files must be of type BytesIO.\")\n            image = self._load_image(file)\n            meta[\"filename\"] = get_filename_for_bytesio(file)\n            documents.extend(self._process_images([image], meta, config, **kwargs))\n\n    return documents\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the document extractor component.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Default is a new instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the document extractor component.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Default is a new instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.perform_llm_extraction","title":"<code>perform_llm_extraction(urls, config, **kwargs)</code>","text":"<p>Performs the actual extraction of text from images using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list[str]</code> <p>The list of image URLs to extract text from.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of extracted text results from the LLM.</p> <p>Example:</p> <pre><code>urls = [\"data:image/jpeg;base64,...\", \"data:image/jpeg;base64,...\"]\n\nextracted_texts = extractor.perform_llm_extraction(urls, config)\n\n# extracted_texts will be a list of dictionaries with extracted text\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def perform_llm_extraction(\n    self, urls: list[str], config: RunnableConfig, **kwargs\n) -&gt; list[dict]:\n    \"\"\"\n    Performs the actual extraction of text from images using the LLM.\n\n    Args:\n        urls (list[str]): The list of image URLs to extract text from.\n        config (RunnableConfig): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[dict]: A list of extracted text results from the LLM.\n\n    Example:\n\n        urls = [\"data:image/jpeg;base64,...\", \"data:image/jpeg;base64,...\"]\n\n        extracted_texts = extractor.perform_llm_extraction(urls, config)\n\n        # extracted_texts will be a list of dictionaries with extracted text\n    \"\"\"\n    run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    inputs = [\n        {\"extraction_instruction\": self.extraction_instruction, \"img_url\": url}\n        for url in urls\n    ]\n\n    prompt = self.vision_prompt\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        llm_results = list(\n            executor.map(\n                lambda input_data: self.call_llm(\n                    input_data, prompt, config, **run_kwargs\n                ),\n                inputs,\n            )\n        )\n\n    logger.debug(\n        f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} images\"\n    )\n\n    return llm_results\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMImageConverter.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter","title":"<code>LLMPDFConverter</code>","text":"<p>               Bases: <code>LLMImageConverter</code></p> <p>A Node class for extracting text from PDFs using a Large Language Model (LLM).</p> <p>This class converts PDFs to images, extracts text from the images using an LLM, and saves the text as documents with metadata.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[CONVERTERS]</code> <p>The group the node belongs to. Default is NodeGroup.CONVERTERS.</p> <code>name</code> <code>str</code> <p>The name of the node. Default is \"LLMPDFConverter\".</p> <code>extraction_instruction</code> <code>str</code> <p>The instruction for text extraction. Default is DEFAULT_EXTRACTION_INSTRUCTION.</p> <code>document_creation_mode</code> <code>DocumentCreationMode</code> <p>The mode for document creation. Default is DocumentCreationMode.ONE_DOC_PER_FILE.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM instance used for text extraction. Default is None.</p> <p>Example:</p> <pre><code>from dynamiq.nodes.converters import LLMPDFConverter\nfrom io import BytesIO\n\n# Initialize the extractor\nconverter = LLMPDFConverter(llm=my_llm_instance)\n\n# Example input data\ninput_data = {\n    \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n    \"files\": [BytesIO(b\"pdf1 content\"), BytesIO(b\"pdf2 content\")],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\n# Execute the converter\noutput = converter.execute(input_data)\n\n# Output will be a dictionary with extracted documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>class LLMPDFConverter(LLMImageConverter):\n    \"\"\"\n    A Node class for extracting text from PDFs using a Large Language Model (LLM).\n\n    This class converts PDFs to images, extracts text from the images using an LLM,\n    and saves the text as documents with metadata.\n\n    Attributes:\n        group (Literal[NodeGroup.CONVERTERS]): The group the node belongs to. Default is NodeGroup.CONVERTERS.\n        name (str): The name of the node. Default is \"LLMPDFConverter\".\n        extraction_instruction (str): The instruction for text extraction.\n            Default is DEFAULT_EXTRACTION_INSTRUCTION.\n        document_creation_mode (DocumentCreationMode): The mode for document creation.\n            Default is DocumentCreationMode.ONE_DOC_PER_FILE.\n        llm (BaseLLM): The LLM instance used for text extraction. Default is None.\n\n    Example:\n\n        from dynamiq.nodes.converters import LLMPDFConverter\n        from io import BytesIO\n\n        # Initialize the extractor\n        converter = LLMPDFConverter(llm=my_llm_instance)\n\n        # Example input data\n        input_data = {\n            \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n            \"files\": [BytesIO(b\"pdf1 content\"), BytesIO(b\"pdf2 content\")],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        # Execute the converter\n        output = converter.execute(input_data)\n\n        # Output will be a dictionary with extracted documents\n        print(output)\n    \"\"\"\n\n    _convert_from_bytes: Any = PrivateAttr()\n    _convert_from_path: Any = PrivateAttr()\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LLMPDFConverter with the given parameters and creates a default LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        from pdf2image import convert_from_bytes, convert_from_path\n\n        super().__init__(**kwargs)\n        self._convert_from_bytes = convert_from_bytes\n        self._convert_from_path = convert_from_path\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the PDF text extraction process.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the file paths or files of PDFs to be processed.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the extracted documents.\n\n        Example:\n\n            input_data = {\n                \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n                \"metadata\": {\"source\": \"example source\"}\n            }\n\n            output = extractor.execute(input_data)\n\n            # output will be a dictionary with extracted documents\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = self.extract_text_from_pdfs(\n            file_paths=input_data.get(\"file_paths\"),\n            files=input_data.get(\"files\"),\n            metadata=input_data.get(\"metadata\"),\n            config=config,\n            **kwargs,\n        )\n\n        return {\"documents\": documents}\n\n    def extract_text_from_pdfs(\n        self,\n        file_paths: list[str] | None = None,\n        files: list[BytesIO] | None = None,\n        metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Extracts text from PDFs by converting them to images and using an LLM.\n\n        Args:\n            file_paths (list[str], optional): List of paths to PDF files. Default is None.\n            files (list[BytesIO], optional): List of PDF files as BytesIO objects. Default is None.\n            metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of extracted documents.\n        \"\"\"\n        if file_paths is None and files is None:\n            raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n\n        documents = []\n\n        if file_paths is not None:\n            paths_obj = [Path(path) for path in file_paths]\n            filepaths = [path for path in paths_obj if path.is_file()]\n            filepaths_in_directories = [\n                filepath\n                for path in paths_obj\n                if path.is_dir()\n                for filepath in path.glob(\"*.*\")\n                if filepath.is_file()\n            ]\n            if filepaths_in_directories and isinstance(metadata, list):\n                raise ValueError(\n                    \"If providing directories in the `file_paths` parameter, \"\n                    \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                    \"and not a list. To specify different metadata for each file, \"\n                    \"provide an explicit list of direct paths instead.\"\n                )\n\n            all_filepaths = filepaths + filepaths_in_directories\n            meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n            for file_path, meta in zip(all_filepaths, meta_list):\n                images = self._convert_from_path(file_path)\n                meta[\"filename\"] = str(file_path)\n                documents.extend(self._process_images(images, meta, config, **kwargs))\n\n        if files is not None:\n            meta_list = self._normalize_metadata(metadata, len(files))\n\n            for file, meta in zip(files, meta_list):\n                if not isinstance(file, BytesIO):\n                    raise ValueError(\"All files must be of type BytesIO.\")\n                images = self._convert_from_bytes(file.read())\n                meta[\"filename\"] = get_filename_for_bytesio(file)\n                documents.extend(self._process_images(images, meta, config, **kwargs))\n\n        return documents\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LLMPDFConverter with the given parameters and creates a default LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LLMPDFConverter with the given parameters and creates a default LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    from pdf2image import convert_from_bytes, convert_from_path\n\n    super().__init__(**kwargs)\n    self._convert_from_bytes = convert_from_bytes\n    self._convert_from_path = convert_from_path\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the PDF text extraction process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the file paths or files of PDFs to be processed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the extracted documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n    \"metadata\": {\"source\": \"example source\"}\n}\n\noutput = extractor.execute(input_data)\n\n# output will be a dictionary with extracted documents\n</code></pre> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the PDF text extraction process.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the file paths or files of PDFs to be processed.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the extracted documents.\n\n    Example:\n\n        input_data = {\n            \"file_paths\": [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"],\n            \"metadata\": {\"source\": \"example source\"}\n        }\n\n        output = extractor.execute(input_data)\n\n        # output will be a dictionary with extracted documents\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = self.extract_text_from_pdfs(\n        file_paths=input_data.get(\"file_paths\"),\n        files=input_data.get(\"files\"),\n        metadata=input_data.get(\"metadata\"),\n        config=config,\n        **kwargs,\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.LLMPDFConverter.extract_text_from_pdfs","title":"<code>extract_text_from_pdfs(file_paths=None, files=None, metadata=None, config=None, **kwargs)</code>","text":"<p>Extracts text from PDFs by converting them to images and using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>List of paths to PDF files. Default is None.</p> <code>None</code> <code>files</code> <code>list[BytesIO]</code> <p>List of PDF files as BytesIO objects. Default is None.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | list[dict[str, Any]]</code> <p>Metadata for the documents. Default is None.</p> <code>None</code> <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of extracted documents.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def extract_text_from_pdfs(\n    self,\n    file_paths: list[str] | None = None,\n    files: list[BytesIO] | None = None,\n    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,\n    config: RunnableConfig = None,\n    **kwargs,\n) -&gt; list[Document]:\n    \"\"\"\n    Extracts text from PDFs by converting them to images and using an LLM.\n\n    Args:\n        file_paths (list[str], optional): List of paths to PDF files. Default is None.\n        files (list[BytesIO], optional): List of PDF files as BytesIO objects. Default is None.\n        metadata (dict[str, Any] | list[dict[str, Any]], optional): Metadata for the documents. Default is None.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[Document]: A list of extracted documents.\n    \"\"\"\n    if file_paths is None and files is None:\n        raise ValueError(\"Either `file_paths` or `files` must be provided.\")\n\n    documents = []\n\n    if file_paths is not None:\n        paths_obj = [Path(path) for path in file_paths]\n        filepaths = [path for path in paths_obj if path.is_file()]\n        filepaths_in_directories = [\n            filepath\n            for path in paths_obj\n            if path.is_dir()\n            for filepath in path.glob(\"*.*\")\n            if filepath.is_file()\n        ]\n        if filepaths_in_directories and isinstance(metadata, list):\n            raise ValueError(\n                \"If providing directories in the `file_paths` parameter, \"\n                \"`metadata` can only be a dictionary (metadata applied to every file), \"\n                \"and not a list. To specify different metadata for each file, \"\n                \"provide an explicit list of direct paths instead.\"\n            )\n\n        all_filepaths = filepaths + filepaths_in_directories\n        meta_list = self._normalize_metadata(metadata, len(all_filepaths))\n\n        for file_path, meta in zip(all_filepaths, meta_list):\n            images = self._convert_from_path(file_path)\n            meta[\"filename\"] = str(file_path)\n            documents.extend(self._process_images(images, meta, config, **kwargs))\n\n    if files is not None:\n        meta_list = self._normalize_metadata(metadata, len(files))\n\n        for file, meta in zip(files, meta_list):\n            if not isinstance(file, BytesIO):\n                raise ValueError(\"All files must be of type BytesIO.\")\n            images = self._convert_from_bytes(file.read())\n            meta[\"filename\"] = get_filename_for_bytesio(file)\n            documents.extend(self._process_images(images, meta, config, **kwargs))\n\n    return documents\n</code></pre>"},{"location":"dynamiq/nodes/converters/llm_text_extractor/#dynamiq.nodes.converters.llm_text_extractor.create_vision_prompt_template","title":"<code>create_vision_prompt_template()</code>","text":"<p>Creates a vision prompt template.</p> <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>The vision prompt template.</p> Source code in <code>dynamiq/nodes/converters/llm_text_extractor.py</code> <pre><code>def create_vision_prompt_template() -&gt; Prompt:\n    \"\"\"\n    Creates a vision prompt template.\n\n    Returns:\n        Prompt: The vision prompt template.\n    \"\"\"\n    text_message = VisionMessageTextContent(text=\"{{extraction_instruction}}\")\n    image_message = VisionMessageImageContent(image_url=VisionMessageImageURL(url=\"{{img_url}}\"))\n    vision_message = VisionMessage(content=[text_message, image_message], role=\"user\")\n    vision_prompt = Prompt(messages=[vision_message])\n    return vision_prompt\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/","title":"Pptx","text":""},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverter","title":"<code>PPTXFileConverter</code>","text":"<p>               Bases: <code>Node</code></p> <p>A component for converting files to Documents using the pptx converter.</p> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>class PPTXFileConverter(Node):\n    \"\"\"\n    A component for converting files to Documents using the pptx converter.\n\n    Args:\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"],\n            optional): Determines how to create Documents from the elements returned by PdfReader.\n            Options are:\n            - \"one-doc-per-file\": Creates one Document per file.\n                All elements are concatenated into one text field.\n            - \"one-doc-per-page\": Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            Defaults to \"one-doc-per-file\".\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"PPTX File Converter\"\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    file_converter: PPTXConverterComponent = None\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"\n        Initialize the components of the PPTXConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = PPTXConverterComponent(\n                document_creation_mode=self.document_creation_mode,\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the PPTXConverter to convert files to Documents.\n\n        Args:\n            input_data: Dict containing 'file_paths', 'files', and/or 'metadata' keys.\n            config: Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.pptx\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.get(\"file_paths\")\n        files = input_data.get(\"files\")\n        metadata = input_data.get(\"metadata\")\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the PPTXConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Dict containing 'file_paths', 'files', and/or 'metadata' keys.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.pptx\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the PPTXConverter to convert files to Documents.\n\n    Args:\n        input_data: Dict containing 'file_paths', 'files', and/or 'metadata' keys.\n        config: Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.pptx\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.get(\"file_paths\")\n    files = input_data.get(\"files\")\n    metadata = input_data.get(\"metadata\")\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \" f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/pptx/#dynamiq.nodes.converters.pptx.PPTXFileConverter.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the PPTXConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/converters/pptx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"\n    Initialize the components of the PPTXConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = PPTXConverterComponent(\n            document_creation_mode=self.document_creation_mode,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/converters/pypdf/","title":"Pypdf","text":"<p>pdf</p>"},{"location":"dynamiq/nodes/converters/unstructured/","title":"Unstructured","text":""},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter","title":"<code>UnstructuredFileConverter</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for converting files to Documents using the Unstructured API (hosted or running locally).</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>UnstructuredConnection</code> <p>The connection to use for the Unstructured API. Defaults to None, which will initialize a new UnstructuredConnection.</p> required <code>strategy</code> <code>Literal['auto', 'fast', 'hi_res', 'ocr_only']</code> <p>The strategy to use for document processing. Defaults to \"auto\".</p> required <code>unstructured_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional parameters to pass to the Unstructured API. See Unstructured API docs for available parameters. Defaults to None.</p> required Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>class UnstructuredFileConverter(ConnectionNode):\n    \"\"\"\n    A component for converting files to Documents using the Unstructured API (hosted or running locally).\n\n    Args:\n        connection (UnstructuredConnection, optional): The connection to use for the Unstructured API.\n            Defaults to None, which will initialize a new UnstructuredConnection.\n        document_creation_mode (Literal[\"one-doc-per-file\", \"one-doc-per-page\", \"one-doc-per-element\"],\n            optional): Determines how to create Documents from the elements returned by Unstructured.\n            Options are:\n            - \"one-doc-per-file\": Creates one Document per file.\n                All elements are concatenated into one text field.\n            - \"one-doc-per-page\": Creates one Document per page.\n                All elements on a page are concatenated into one text field.\n            - \"one-doc-per-element\": Creates one Document per element.\n                Each element is converted to a separate Document.\n            Defaults to \"one-doc-per-file\".\n        strategy (Literal[\"auto\", \"fast\", \"hi_res\", \"ocr_only\"], optional): The strategy to use for\n            document processing. Defaults to \"auto\".\n        unstructured_kwargs (Optional[dict[str, Any]], optional): Additional parameters to pass to the\n            Unstructured API. See Unstructured API docs for available parameters. Defaults to None.\n    \"\"\"\n\n    group: Literal[NodeGroup.CONVERTERS] = NodeGroup.CONVERTERS\n    name: str = \"Unstructured File Converter\"\n    connection: Unstructured = None\n    document_creation_mode: DocumentCreationMode = DocumentCreationMode.ONE_DOC_PER_FILE\n    strategy: ConvertStrategy = ConvertStrategy.AUTO\n    unstructured_kwargs: dict[str, Any] | None = None\n    file_converter: UnstructuredFileConverterComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the UnstructuredFileConverter.\n\n        If no connection is provided, a new Unstructured connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the UnstructuredFileConverter.\n        \"\"\"\n        if kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Unstructured()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"file_converter\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the UnstructuredFileConverter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.file_converter is None:\n            self.file_converter = UnstructuredFileConverterComponent(\n                connection=self.connection,\n                document_creation_mode=self.document_creation_mode,\n                strategy=self.strategy,\n                unstructured_kwargs=self.unstructured_kwargs,\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, list[Any]]:\n        \"\"\"\n        Execute the UnstructuredFileConverter to convert files to Documents.\n\n        Args:\n            input_data: Dict containing 'file_paths', 'files', and/or 'metadata' keys.\n            config: Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict with 'documents' key containing a list of converted Documents.\n\n        Raises:\n            KeyError: If required keys are missing in input_data.\n\n        Example:\n            input_data = {\n                \"file_paths\": [\"/path/to/file1.pdf\", \"/path/to/file2.docx\"],\n                \"files\": [BytesIO(b\"file content\")],\n                \"metadata\": {\"source\": \"user_upload\"}\n            }\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        file_paths = input_data.get(\"file_paths\")\n        files = input_data.get(\"files\")\n        metadata = input_data.get(\"metadata\")\n\n        output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n        documents = output[\"documents\"]\n\n        count_file_paths = len(file_paths) if file_paths else 0\n        count_files = len(files) if files else 0\n\n        logger.debug(\n            f\"Converted {count_file_paths} file paths and {count_files} file objects \"\n            f\"to {len(documents)} Documents.\"\n        )\n\n        return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the UnstructuredFileConverter.</p> <p>If no connection is provided, a new Unstructured connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the UnstructuredFileConverter.</p> <code>{}</code> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the UnstructuredFileConverter.\n\n    If no connection is provided, a new Unstructured connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the UnstructuredFileConverter.\n    \"\"\"\n    if kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Unstructured()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the UnstructuredFileConverter to convert files to Documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Dict containing 'file_paths', 'files', and/or 'metadata' keys.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dict with 'documents' key containing a list of converted Documents.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required keys are missing in input_data.</p> Example <p>input_data = {     \"file_paths\": [\"/path/to/file1.pdf\", \"/path/to/file2.docx\"],     \"files\": [BytesIO(b\"file content\")],     \"metadata\": {\"source\": \"user_upload\"} }</p> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Execute the UnstructuredFileConverter to convert files to Documents.\n\n    Args:\n        input_data: Dict containing 'file_paths', 'files', and/or 'metadata' keys.\n        config: Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict with 'documents' key containing a list of converted Documents.\n\n    Raises:\n        KeyError: If required keys are missing in input_data.\n\n    Example:\n        input_data = {\n            \"file_paths\": [\"/path/to/file1.pdf\", \"/path/to/file2.docx\"],\n            \"files\": [BytesIO(b\"file content\")],\n            \"metadata\": {\"source\": \"user_upload\"}\n        }\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    file_paths = input_data.get(\"file_paths\")\n    files = input_data.get(\"files\")\n    metadata = input_data.get(\"metadata\")\n\n    output = self.file_converter.run(file_paths=file_paths, files=files, metadata=metadata)\n    documents = output[\"documents\"]\n\n    count_file_paths = len(file_paths) if file_paths else 0\n    count_files = len(files) if files else 0\n\n    logger.debug(\n        f\"Converted {count_file_paths} file paths and {count_files} file objects \"\n        f\"to {len(documents)} Documents.\"\n    )\n\n    return {\"documents\": documents}\n</code></pre>"},{"location":"dynamiq/nodes/converters/unstructured/#dynamiq.nodes.converters.unstructured.UnstructuredFileConverter.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the UnstructuredFileConverter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/converters/unstructured.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the UnstructuredFileConverter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.file_converter is None:\n        self.file_converter = UnstructuredFileConverterComponent(\n            connection=self.connection,\n            document_creation_mode=self.document_creation_mode,\n            strategy=self.strategy,\n            unstructured_kwargs=self.unstructured_kwargs,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/","title":"Bedrock","text":""},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder","title":"<code>BedrockDocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Provides functionality to compute embeddings for documents using Bedrock models.</p> <p>This class extends ConnectionNode to create embeddings for documents using Bedrock API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>AWS | None</code> <p>The connection to the Bedrock API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>BedrockDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[AWS]</code> <p>The connection to the Bedrock API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'amazon.titan-embed-text-v1'.</p> required Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>class BedrockDocumentEmbedder(ConnectionNode):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Bedrock models.\n\n    This class extends ConnectionNode to create embeddings for documents using Bedrock API.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (BedrockConnection | None): The connection to the Bedrock API.\n        model (str): The model name to use for embedding.\n        document_embedder (BedrockDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[BedrockConnection]): The connection to the Bedrock API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'amazon.titan-embed-text-v1'.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"AmazonBedrockDocumentEmbedder\"\n    connection: BedrockConnection | None = None\n    model: str = \"amazon.titan-embed-text-v1\"\n    document_embedder: BedrockEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the BedrockDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = BedrockConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initializes the components of the BedrockDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = BedrockEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings using the Bedrock API, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n                'documents' key with the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data[\"documents\"])\n        logger.debug(\"BedrockDocumentEmbedder executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the BedrockDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new BedrockConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the BedrockDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = BedrockConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings using the Bedrock API, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings using the Bedrock API, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n            'documents' key with the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data[\"documents\"])\n    logger.debug(\"BedrockDocumentEmbedder executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the BedrockDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initializes the components of the BedrockDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = BedrockEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder","title":"<code>BedrockTextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component designed to embed strings using specified Cohere models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using Bedrock API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[AWS]</code> <p>An existing connection to Bedrock API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Bedrock model for text embeddings. Defaults to 'amazon.titan-embed-text-v1'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>AWS | None</code> <p>The connection to Bedrock API.</p> <code>model</code> <code>str</code> <p>The Bedrock model identifier for text embeddings.</p> <code>text_embedder</code> <code>BedrockTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>class BedrockTextEmbedder(ConnectionNode):\n    \"\"\"\n    A component designed to embed strings using specified Cohere models.\n\n    This class extends ConnectionNode to provide text embedding functionality using Bedrock API.\n\n    Args:\n        connection (Optional[BedrockConnection]): An existing connection to Bedrock API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Bedrock model for text embeddings. Defaults to\n            'amazon.titan-embed-text-v1'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (BedrockConnection | None): The connection to Bedrock API.\n        model (str): The Bedrock model identifier for text embeddings.\n        text_embedder (BedrockTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"BedrockTextEmbedder\"\n    connection: BedrockConnection | None = None\n    model: str = \"amazon.titan-embed-text-v1\"\n    text_embedder: BedrockEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the BedrockTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = BedrockConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the BedrockTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = BedrockEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes input data, runs the text embedding, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.text_embedder.embed_text(input_data[\"query\"])\n        logger.debug(f\"BedrockTextEmbedder: {output['meta']}\")\n        return {\n            \"embedding\": output[\"embedding\"],\n            \"query\": input_data[\"query\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BedrockTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new BedrockConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the BedrockTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new BedrockConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = BedrockConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes input data, runs the text embedding, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes input data, runs the text embedding, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.text_embedder.embed_text(input_data[\"query\"])\n    logger.debug(f\"BedrockTextEmbedder: {output['meta']}\")\n    return {\n        \"embedding\": output[\"embedding\"],\n        \"query\": input_data[\"query\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/bedrock/#dynamiq.nodes.embedders.bedrock.BedrockTextEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the BedrockTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/bedrock.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the BedrockTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = BedrockEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/","title":"Cohere","text":""},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder","title":"<code>CohereDocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Provides functionality to compute embeddings for documents using Cohere models.</p> <p>This class extends ConnectionNode to create embeddings for documents using Cohere API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Cohere | None</code> <p>The connection to the Cohere API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>CohereDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Cohere]</code> <p>The connection to the Cohere API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'cohere/embed-english-v2.0'.</p> required Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>class CohereDocumentEmbedder(ConnectionNode):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Cohere models.\n\n    This class extends ConnectionNode to create embeddings for documents using Cohere API.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (CohereConnection | None): The connection to the Cohere API.\n        model (str): The model name to use for embedding.\n        document_embedder (CohereDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[CohereConnection]): The connection to the Cohere API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'cohere/embed-english-v2.0'.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"CohereDocumentEmbedder\"\n    connection: CohereConnection | None = None\n    model: str = \"cohere/embed-english-v2.0\"\n    document_embedder: CohereEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the CohereDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initializes the components of the CohereDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = CohereEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings using the Cohere API, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n                'documents' key with the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data[\"documents\"])\n        logger.debug(\"CohereDocumentEmbedder executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the CohereDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new CohereConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the CohereDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CohereConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings using the Cohere API, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings using the Cohere API, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n            'documents' key with the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data[\"documents\"])\n    logger.debug(\"CohereDocumentEmbedder executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the CohereDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initializes the components of the CohereDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = CohereEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder","title":"<code>CohereTextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component designed to embed strings using specified Cohere models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using litellm embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Cohere]</code> <p>An existing connection to Cohere API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Cohere model for text embeddings. Defaults to 'cohere/embed-english-v2.0'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Cohere | None</code> <p>The connection to Cohere API.</p> <code>model</code> <code>str</code> <p>The Cohere model identifier for text embeddings.</p> <code>text_embedder</code> <code>CohereTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>class CohereTextEmbedder(ConnectionNode):\n    \"\"\"\n    A component designed to embed strings using specified Cohere models.\n\n    This class extends ConnectionNode to provide text embedding functionality using litellm embedding.\n\n    Args:\n        connection (Optional[CohereConnection]): An existing connection to Cohere API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Cohere model for text embeddings. Defaults to\n            'cohere/embed-english-v2.0'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (CohereConnection | None): The connection to Cohere API.\n        model (str): The Cohere model identifier for text embeddings.\n        text_embedder (CohereTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"CohereTextEmbedder\"\n    connection: CohereConnection | None = None\n    model: str = \"cohere/embed-english-v2.0\"\n    text_embedder: CohereEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the CohereTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the CohereTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = CohereEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes input data, runs the text embedding, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.text_embedder.embed_text(input_data[\"query\"])\n        logger.debug(f\"CohereTextEmbedder: {output['meta']}\")\n        return {\n            \"embedding\": output[\"embedding\"],\n            \"query\": input_data[\"query\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the CohereTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new CohereConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the CohereTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new CohereConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CohereConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes input data, runs the text embedding, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes input data, runs the text embedding, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.text_embedder.embed_text(input_data[\"query\"])\n    logger.debug(f\"CohereTextEmbedder: {output['meta']}\")\n    return {\n        \"embedding\": output[\"embedding\"],\n        \"query\": input_data[\"query\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/cohere/#dynamiq.nodes.embedders.cohere.CohereTextEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the CohereTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/cohere.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the CohereTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = CohereEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/","title":"Huggingface","text":""},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder","title":"<code>HuggingFaceDocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Provides functionality to compute embeddings for documents using HuggingFace models.</p> <p>This class extends ConnectionNode to create embeddings for documents using litellm embedding.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>HuggingFace | None</code> <p>The connection to the HuggingFace API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>HuggingFaceDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[HuggingFace]</code> <p>The connection to the HuggingFace API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'huggingface/microsoft/codebert-base'.</p> required Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>class HuggingFaceDocumentEmbedder(ConnectionNode):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using HuggingFace models.\n\n    This class extends ConnectionNode to create embeddings for documents using litellm embedding.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (HuggingFaceConnection | None): The connection to the HuggingFace API.\n        model (str): The model name to use for embedding.\n        document_embedder (HuggingFaceDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[HuggingFaceConnection]): The connection to the HuggingFace API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'huggingface/microsoft/codebert-base'.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"HuggingFaceDocumentEmbedder\"\n    connection: HuggingFaceConnection | None = None\n    model: str = \"huggingface/BAAI/bge-large-zh\"\n    document_embedder: HuggingFaceEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the HuggingFaceDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initializes the components of the HuggingFaceDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = HuggingFaceEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings using the litellm embedding HuggingFace , and\n        returns the result.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n                'documents' key with the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data[\"documents\"])\n        logger.debug(\"HuggingFaceDocumentEmbedder executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the HuggingFaceDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the HuggingFaceDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = HuggingFaceConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings using the litellm embedding HuggingFace , and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings using the litellm embedding HuggingFace , and\n    returns the result.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n            'documents' key with the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data[\"documents\"])\n    logger.debug(\"HuggingFaceDocumentEmbedder executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the HuggingFaceDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initializes the components of the HuggingFaceDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = HuggingFaceEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder","title":"<code>HuggingFaceTextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component designed to embed strings using specified HuggingFace models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using litellm embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[HuggingFace]</code> <p>An existing connection to HuggingFace's API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the HuggingFace model for text embeddings. Defaults to 'huggingface/microsoft/codebert-base'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>HuggingFace | None</code> <p>The connection to HuggingFace API.</p> <code>model</code> <code>str</code> <p>The HuggingFace model identifier for text embeddings.</p> <code>text_embedder</code> <code>HuggingFaceTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>class HuggingFaceTextEmbedder(ConnectionNode):\n    \"\"\"\n    A component designed to embed strings using specified HuggingFace models.\n\n    This class extends ConnectionNode to provide text embedding functionality using litellm embedding.\n\n    Args:\n        connection (Optional[HuggingFaceConnection]): An existing connection to HuggingFace's API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the HuggingFace model for text embeddings. Defaults to\n            'huggingface/microsoft/codebert-base'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (HuggingFaceConnection | None): The connection to HuggingFace API.\n        model (str): The HuggingFace model identifier for text embeddings.\n        text_embedder (HuggingFaceTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"HuggingFaceTextEmbedder\"\n    connection: HuggingFaceConnection | None = None\n    model: str = \"huggingface/microsoft/codebert-base\"\n    text_embedder: HuggingFaceEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the HuggingFaceTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the HuggingFaceTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = HuggingFaceEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes input data, runs the text embedding, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.text_embedder.embed_text(input_data[\"query\"])\n        logger.debug(f\"HuggingFaceTextEmbedder: {output['meta']}\")\n        return {\n            \"embedding\": output[\"embedding\"],\n            \"query\": input_data[\"query\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the HuggingFaceTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the HuggingFaceTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new HuggingFaceConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = HuggingFaceConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes input data, runs the text embedding, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes input data, runs the text embedding, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.text_embedder.embed_text(input_data[\"query\"])\n    logger.debug(f\"HuggingFaceTextEmbedder: {output['meta']}\")\n    return {\n        \"embedding\": output[\"embedding\"],\n        \"query\": input_data[\"query\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/huggingface/#dynamiq.nodes.embedders.huggingface.HuggingFaceTextEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the HuggingFaceTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/huggingface.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the HuggingFaceTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = HuggingFaceEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/","title":"Mistral","text":""},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder","title":"<code>MistralDocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Provides functionality to compute embeddings for documents using Mistral models.</p> <p>This class extends ConnectionNode to create embeddings for documents using litellm embedding.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Mistral | None</code> <p>The connection to the Mistral API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>MistralDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Mistral]</code> <p>The connection to the Mistral API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'mistral/mistral-embed'. only by 'text-embedding-3' and later models. Defaults to None.</p> required Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>class MistralDocumentEmbedder(ConnectionNode):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using Mistral models.\n\n    This class extends ConnectionNode to create embeddings for documents using litellm embedding.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (MistralConnection | None): The connection to the Mistral API.\n        model (str): The model name to use for embedding.\n        document_embedder (MistralDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[MistralConnection]): The connection to the Mistral API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'mistral/mistral-embed'.\n            only by 'text-embedding-3' and later models. Defaults to None.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"MistralDocumentEmbedder\"\n    connection: MistralConnection | None = None\n    model: str = \"mistral/mistral-embed\"\n    document_embedder: MistralEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the MistralDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initializes the components of the MistralDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = MistralEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings using the Mistral API, and\n        returns the result.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n                'documents' key with the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data[\"documents\"])\n        logger.debug(\"MistralDocumentEmbedder executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the MistralDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new MistralConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the MistralDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = MistralConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings using the Mistral API, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings using the Mistral API, and\n    returns the result.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n            'documents' key with the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data[\"documents\"])\n    logger.debug(\"MistralDocumentEmbedder executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the MistralDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initializes the components of the MistralDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = MistralEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder","title":"<code>MistralTextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component designed to embed strings using specified Mistral models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using Mistral API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Mistral]</code> <p>An existing connection to Mistral API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the Mistral model for text embeddings. Defaults to 'mistral/mistral-embed'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Mistral | None</code> <p>The connection to Mistral's API.</p> <code>model</code> <code>str</code> <p>The Mistral model identifier for text embeddings.</p> <code>text_embedder</code> <code>MistralTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>class MistralTextEmbedder(ConnectionNode):\n    \"\"\"\n    A component designed to embed strings using specified Mistral models.\n\n    This class extends ConnectionNode to provide text embedding functionality using Mistral API.\n\n    Args:\n        connection (Optional[MistralConnection]): An existing connection to Mistral API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the Mistral model for text embeddings. Defaults to\n            'mistral/mistral-embed'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (MistralConnection | None): The connection to Mistral's API.\n        model (str): The Mistral model identifier for text embeddings.\n        text_embedder (MistralTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"MistralTextEmbedder\"\n    connection: MistralConnection | None = None\n    model: str = \"mistral/mistral-embed\"\n    text_embedder: MistralEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the MistralTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the MistralTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = MistralEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes input data, runs the text embedding, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.text_embedder.embed_text(input_data[\"query\"])\n        logger.debug(f\"MistralTextEmbedder: {output['meta']}\")\n        return {\n            \"embedding\": output[\"embedding\"],\n            \"query\": input_data[\"query\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the MistralTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new MistralConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the MistralTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new MistralConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = MistralConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes input data, runs the text embedding, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes input data, runs the text embedding, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.text_embedder.embed_text(input_data[\"query\"])\n    logger.debug(f\"MistralTextEmbedder: {output['meta']}\")\n    return {\n        \"embedding\": output[\"embedding\"],\n        \"query\": input_data[\"query\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/mistral/#dynamiq.nodes.embedders.mistral.MistralTextEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the MistralTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/mistral.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the MistralTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = MistralEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/","title":"Openai","text":""},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder","title":"<code>OpenAIDocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Provides functionality to compute embeddings for documents using OpenAI's models.</p> <p>This class extends ConnectionNode to create embeddings for documents using OpenAI's API.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>OpenAI | None</code> <p>The connection to the OpenAI API.</p> <code>client</code> <code>OpenAIClient | None</code> <p>The OpenAI client instance.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>dimensions</code> <code>int | None</code> <p>The number of dimensions for the output embeddings.</p> <code>document_embedder</code> <code>OpenAIDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[OpenAI]</code> <p>The connection to the OpenAI API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'text-embedding-3-small'.</p> required <code>dimensions</code> <code>Optional[int]</code> <p>The number of dimensions for the output embeddings. Supported only by 'text-embedding-3' and later models. Defaults to None.</p> required Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>class OpenAIDocumentEmbedder(ConnectionNode):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using OpenAI's models.\n\n    This class extends ConnectionNode to create embeddings for documents using OpenAI's API.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (OpenAIConnection | None): The connection to the OpenAI API.\n        client (OpenAIClient | None): The OpenAI client instance.\n        model (str): The model name to use for embedding.\n        dimensions (int | None): The number of dimensions for the output embeddings.\n        document_embedder (OpenAIDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[OpenAIConnection]): The connection to the OpenAI API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'text-embedding-3-small'.\n        dimensions (Optional[int]): The number of dimensions for the output embeddings. Supported\n            only by 'text-embedding-3' and later models. Defaults to None.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"OpenAIDocumentEmbedder\"\n    connection: OpenAIConnection | None = None\n    model: str = \"text-embedding-3-small\"\n    dimensions: int | None = None\n    document_embedder: OpenAIEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the OpenAIDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initializes the components of the OpenAIDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = OpenAIEmbedderComponent(\n                connection=self.connection,\n                model=self.model,\n                dimensions=self.dimensions,\n                client=self.client,\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings using the OpenAI API, and\n        returns the result.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n                'documents' key with the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data[\"documents\"])\n        logger.debug(\"OpenAIDocumentEmbedder executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the OpenAIDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the OpenAIDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings using the OpenAI API, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings using the OpenAI API, and\n    returns the result.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n            'documents' key with the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data[\"documents\"])\n    logger.debug(\"OpenAIDocumentEmbedder executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAIDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the OpenAIDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initializes the components of the OpenAIDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = OpenAIEmbedderComponent(\n            connection=self.connection,\n            model=self.model,\n            dimensions=self.dimensions,\n            client=self.client,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder","title":"<code>OpenAITextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component designed to embed strings using specified OpenAI models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[OpenAI]</code> <p>An existing connection to OpenAI's API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the OpenAI model for text embeddings. Defaults to 'text-embedding-3-small'.</p> required <code>dimensions</code> <code>Optional[int]</code> <p>Desired dimensionality of output embeddings. Defaults to None, using the model's default output dimensionality.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>OpenAI | None</code> <p>The connection to OpenAI's API.</p> <code>client</code> <code>OpenAIClient | None</code> <p>The OpenAI client instance.</p> <code>model</code> <code>str</code> <p>The OpenAI model identifier for text embeddings.</p> <code>dimensions</code> <code>int | None</code> <p>The desired dimensionality of output embeddings.</p> <code>text_embedder</code> <code>OpenAITextEmbedderComponent</code> <p>The component for text embedding.</p> Notes <p>The <code>dimensions</code> parameter is model-dependent and may not be supported by all models.</p> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>class OpenAITextEmbedder(ConnectionNode):\n    \"\"\"\n    A component designed to embed strings using specified OpenAI models.\n\n    This class extends ConnectionNode to provide text embedding functionality using OpenAI's API.\n\n    Args:\n        connection (Optional[OpenAIConnection]): An existing connection to OpenAI's API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the OpenAI model for text embeddings. Defaults to\n            'text-embedding-3-small'.\n        dimensions (Optional[int]): Desired dimensionality of output embeddings. Defaults to None,\n            using the model's default output dimensionality.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (OpenAIConnection | None): The connection to OpenAI's API.\n        client (OpenAIClient | None): The OpenAI client instance.\n        model (str): The OpenAI model identifier for text embeddings.\n        dimensions (int | None): The desired dimensionality of output embeddings.\n        text_embedder (OpenAITextEmbedderComponent): The component for text embedding.\n\n    Notes:\n        The `dimensions` parameter is model-dependent and may not be supported by all models.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"OpenAITextEmbedder\"\n    connection: OpenAIConnection | None = None\n    model: str = \"text-embedding-3-small\"\n    dimensions: int | None = None\n    text_embedder: OpenAIEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the OpenAITextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the OpenAITextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = OpenAIEmbedderComponent(\n                connection=self.connection,\n                model=self.model,\n                dimensions=self.dimensions,\n                client=self.client,\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes input data, runs the text embedding, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.text_embedder.embed_text(input_data[\"query\"])\n        logger.debug(f\"OpenAITextEmbedder: {output['meta']}\")\n        return {\n            \"embedding\": output[\"embedding\"],\n            \"query\": input_data[\"query\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the OpenAITextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the OpenAITextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new OpenAIConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes input data, runs the text embedding, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes input data, runs the text embedding, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.text_embedder.embed_text(input_data[\"query\"])\n    logger.debug(f\"OpenAITextEmbedder: {output['meta']}\")\n    return {\n        \"embedding\": output[\"embedding\"],\n        \"query\": input_data[\"query\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/openai/#dynamiq.nodes.embedders.openai.OpenAITextEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the OpenAITextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/openai.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the OpenAITextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = OpenAIEmbedderComponent(\n            connection=self.connection,\n            model=self.model,\n            dimensions=self.dimensions,\n            client=self.client,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/","title":"Watsonx","text":""},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder","title":"<code>WatsonXDocumentEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Provides functionality to compute embeddings for documents using WatsonX models.</p> <p>This class extends ConnectionNode to create embeddings for documents using litellm embedding.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>WatsonX | None</code> <p>The connection to the WatsonX API.</p> <code>model</code> <code>str</code> <p>The model name to use for embedding.</p> <code>document_embedder</code> <code>WatsonXDocumentEmbedderComponent</code> <p>The component for document embedding.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[WatsonX]</code> <p>The connection to the WatsonX API. A new connection is created if none is provided.</p> required <code>model</code> <code>str</code> <p>The model name to use for embedding. Defaults to 'watsonx/ibm/slate-30m-english-rtrvr'.</p> required Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>class WatsonXDocumentEmbedder(ConnectionNode):\n    \"\"\"\n    Provides functionality to compute embeddings for documents using WatsonX models.\n\n    This class extends ConnectionNode to create embeddings for documents using litellm embedding.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (WatsonXConnection | None): The connection to the WatsonX API.\n        model (str): The model name to use for embedding.\n        document_embedder (WatsonXDocumentEmbedderComponent): The component for document embedding.\n\n    Args:\n        connection (Optional[WatsonXConnection]): The connection to the WatsonX API. A new connection\n            is created if none is provided.\n        model (str): The model name to use for embedding. Defaults to 'watsonx/ibm/slate-30m-english-rtrvr'.\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"WatsonXDocumentEmbedder\"\n    connection: WatsonXConnection | None = None\n    model: str = \"watsonx/ibm/slate-30m-english-rtrvr\"\n    document_embedder: WatsonXEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the WatsonXDocumentEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_embedder\": True}\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"\n        Initializes the components of the WatsonXDocumentEmbedder.\n\n        This method sets up the document_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_embedder is None:\n            self.document_embedder = WatsonXEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Executes the document embedding process.\n\n        This method takes input documents, computes their embeddings using the WatsonX API, and\n        returns the result.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n                'documents' key with the documents to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The output from the document_embedder component, typically the computed embeddings.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.document_embedder.embed_documents(input_data[\"documents\"])\n        logger.debug(\"WatsonXDocumentEmbedder executed successfully.\")\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the WatsonXDocumentEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the WatsonXDocumentEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WatsonXConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document embedding process.</p> <p>This method takes input documents, computes their embeddings using the WatsonX API, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output from the document_embedder component, typically the computed embeddings.</p> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Executes the document embedding process.\n\n    This method takes input documents, computes their embeddings using the WatsonX API, and\n    returns the result.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data. Expected to have a\n            'documents' key with the documents to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The output from the document_embedder component, typically the computed embeddings.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.document_embedder.embed_documents(input_data[\"documents\"])\n    logger.debug(\"WatsonXDocumentEmbedder executed successfully.\")\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXDocumentEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the WatsonXDocumentEmbedder.</p> <p>This method sets up the document_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"\n    Initializes the components of the WatsonXDocumentEmbedder.\n\n    This method sets up the document_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_embedder is None:\n        self.document_embedder = WatsonXEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder","title":"<code>WatsonXTextEmbedder</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component designed to embed strings using specified WatsonX models.</p> <p>This class extends ConnectionNode to provide text embedding functionality using WatsonX API.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[WatsonX]</code> <p>An existing connection to WatsonX API. If not provided, a new connection will be established using environment variables.</p> required <code>model</code> <code>str</code> <p>The identifier of the WatsonX model for text embeddings. Defaults to 'watsonx/ibm/slate-30m-english-rtrvr'.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[EMBEDDERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>WatsonX | None</code> <p>The connection to WatsonX's API.</p> <code>model</code> <code>str</code> <p>The WatsonX model identifier for text embeddings.</p> <code>text_embedder</code> <code>WatsonXTextEmbedderComponent</code> <p>The component for text embedding.</p> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>class WatsonXTextEmbedder(ConnectionNode):\n    \"\"\"\n    A component designed to embed strings using specified WatsonX models.\n\n    This class extends ConnectionNode to provide text embedding functionality using WatsonX API.\n\n    Args:\n        connection (Optional[WatsonXConnection]): An existing connection to WatsonX API. If not\n            provided, a new connection will be established using environment variables.\n        model (str): The identifier of the WatsonX model for text embeddings. Defaults to\n            'watsonx/ibm/slate-30m-english-rtrvr'.\n\n    Attributes:\n        group (Literal[NodeGroup.EMBEDDERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (WatsonXConnection | None): The connection to WatsonX's API.\n        model (str): The WatsonX model identifier for text embeddings.\n        text_embedder (WatsonXTextEmbedderComponent): The component for text embedding.\n\n    \"\"\"\n\n    group: Literal[NodeGroup.EMBEDDERS] = NodeGroup.EMBEDDERS\n    name: str = \"WatsonXTextEmbedder\"\n    connection: WatsonXConnection | None = None\n    model: str = \"watsonx/ibm/slate-30m-english-rtrvr\"\n    text_embedder: WatsonXEmbedderComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the WatsonXTextEmbedder.\n\n        If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the node.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"text_embedder\": True}\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"\n        Initialize the components of the WatsonXTextEmbedder.\n\n        This method sets up the text_embedder component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n                ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.text_embedder is None:\n            self.text_embedder = WatsonXEmbedderComponent(\n                connection=self.connection, model=self.model, client=self.client\n            )\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the text embedding process.\n\n        This method takes input data, runs the text embedding, and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query to embed.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the embedding and the original query.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        output = self.text_embedder.embed_text(input_data[\"query\"])\n        logger.debug(f\"WatsonXTextEmbedder: {output['meta']}\")\n        return {\n            \"embedding\": output[\"embedding\"],\n            \"query\": input_data[\"query\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WatsonXTextEmbedder.</p> <p>If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the WatsonXTextEmbedder.\n\n    If neither client nor connection is provided in kwargs, a new WatsonXConnection is created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the node.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WatsonXConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the text embedding process.</p> <p>This method takes input data, runs the text embedding, and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query to embed.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the embedding and the original query.</p> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the text embedding process.\n\n    This method takes input data, runs the text embedding, and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query to embed.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the embedding and the original query.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    output = self.text_embedder.embed_text(input_data[\"query\"])\n    logger.debug(f\"WatsonXTextEmbedder: {output['meta']}\")\n    return {\n        \"embedding\": output[\"embedding\"],\n        \"query\": input_data[\"query\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/embedders/watsonx/#dynamiq.nodes.embedders.watsonx.WatsonXTextEmbedder.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the WatsonXTextEmbedder.</p> <p>This method sets up the text_embedder component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/embedders/watsonx.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"\n    Initialize the components of the WatsonXTextEmbedder.\n\n    This method sets up the text_embedder component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Defaults to a new\n            ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.text_embedder is None:\n        self.text_embedder = WatsonXEmbedderComponent(\n            connection=self.connection, model=self.model, client=self.client\n        )\n</code></pre>"},{"location":"dynamiq/nodes/llms/ai21/","title":"Ai21","text":""},{"location":"dynamiq/nodes/llms/ai21/#dynamiq.nodes.llms.ai21.AI21","title":"<code>AI21</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>AI21 LLM node.</p> <p>This class provides an implementation for the AI21 Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AI21</code> <p>The connection to use for the AI21 LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the AI21 model name.</p> Source code in <code>dynamiq/nodes/llms/ai21.py</code> <pre><code>class AI21(BaseLLM):\n    \"\"\"AI21 LLM node.\n\n    This class provides an implementation for the AI21 Language Model node.\n\n    Attributes:\n        connection (AI21Connection): The connection to use for the AI21 LLM.\n        MODEL_PREFIX (str): The prefix for the AI21 model name.\n    \"\"\"\n    connection: AI21Connection\n    MODEL_PREFIX = \"ai21/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the AI21 LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AI21Connection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/ai21/#dynamiq.nodes.llms.ai21.AI21.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AI21 LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/ai21.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the AI21 LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AI21Connection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anthropic/","title":"Anthropic","text":""},{"location":"dynamiq/nodes/llms/anthropic/#dynamiq.nodes.llms.anthropic.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Anthropic LLM node.</p> <p>This class provides an implementation for the Anthropic Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Anthropic | None</code> <p>The connection to use for the Anthropic LLM.</p> Source code in <code>dynamiq/nodes/llms/anthropic.py</code> <pre><code>class Anthropic(BaseLLM):\n    \"\"\"Anthropic LLM node.\n\n    This class provides an implementation for the Anthropic Language Model node.\n\n    Attributes:\n        connection (AnthropicConnection | None): The connection to use for the Anthropic LLM.\n    \"\"\"\n    connection: AnthropicConnection | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Anthropic LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AnthropicConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anthropic/#dynamiq.nodes.llms.anthropic.Anthropic.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Anthropic LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/anthropic.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Anthropic LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AnthropicConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anyscale/","title":"Anyscale","text":""},{"location":"dynamiq/nodes/llms/anyscale/#dynamiq.nodes.llms.anyscale.Anyscale","title":"<code>Anyscale</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Anyscale LLM node.</p> <p>This class provides an implementation for the Anyscale Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Anyscale | None</code> <p>The connection to use for the Anyscale LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Anyscale model name.</p> Source code in <code>dynamiq/nodes/llms/anyscale.py</code> <pre><code>class Anyscale(BaseLLM):\n    \"\"\"Anyscale LLM node.\n\n    This class provides an implementation for the Anyscale Language Model node.\n\n    Attributes:\n        connection (AnyscaleConnection | None): The connection to use for the Anyscale LLM.\n        MODEL_PREFIX (str): The prefix for the Anyscale model name.\n    \"\"\"\n    connection: AnyscaleConnection | None = None\n    MODEL_PREFIX = \"anyscale/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Anyscale LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AnyscaleConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/anyscale/#dynamiq.nodes.llms.anyscale.Anyscale.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Anyscale LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/anyscale.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Anyscale LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AnyscaleConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/azureai/","title":"Azureai","text":""},{"location":"dynamiq/nodes/llms/azureai/#dynamiq.nodes.llms.azureai.AzureAI","title":"<code>AzureAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>AzureAI LLM node.</p> <p>This class provides an implementation for the AzureAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AzureAI | None</code> <p>The connection to use for the AzureAI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the AzureAI model name.</p> Source code in <code>dynamiq/nodes/llms/azureai.py</code> <pre><code>class AzureAI(BaseLLM):\n    \"\"\"AzureAI LLM node.\n\n    This class provides an implementation for the AzureAI Language Model node.\n\n    Attributes:\n        connection (AzureAIConnection | None): The connection to use for the AzureAI LLM.\n        MODEL_PREFIX (str): The prefix for the AzureAI model name.\n    \"\"\"\n    connection: AzureAIConnection | None = None\n    MODEL_PREFIX = \"azure/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the AzureAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AzureAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/azureai/#dynamiq.nodes.llms.azureai.AzureAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AzureAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/azureai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the AzureAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AzureAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/","title":"Base","text":""},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM","title":"<code>BaseLLM</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>Base class for all LLM nodes.</p> <p>Attributes:</p> Name Type Description <code>MODEL_PREFIX</code> <code>ClassVar[str | None]</code> <p>Optional model prefix.</p> <code>name</code> <code>str | None</code> <p>Name of the LLM node. Defaults to \"LLM\".</p> <code>model</code> <code>str</code> <p>Model to use for the LLM.</p> <code>prompt</code> <code>Prompt | None</code> <p>Prompt to use for the LLM.</p> <code>connection</code> <code>BaseConnection</code> <p>Connection to use for the LLM.</p> <code>group</code> <code>Literal[LLMS]</code> <p>Group for the node. Defaults to NodeGroup.LLMS.</p> <code>temperature</code> <code>float</code> <p>Temperature for the LLM. Defaults to 0.1.</p> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens for the LLM. Defaults to 1000.</p> <code>stop</code> <code>list[str]</code> <p>List of tokens to stop at for the LLM.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling config. Defaults to ErrorHandling(timeout_seconds=600).</p> <code>top_p</code> <code>float | None</code> <p>Value to consider tokens with top_p probability.</p> <code>seed</code> <code>int | None</code> <p>Seed for generating the same result for repeated requests.</p> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on their existence in the text.</p> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on their frequency in the text.</p> <code>tool_choice</code> <code>str | None</code> <p>Value to control which function is called by the model.</p> <code>inference_mode</code> <code>InferenceMode</code> <p>Determines how the model handles inference tasks and formats outputs.</p> <code>-</code> <code>DEFAULT</code> <p>Generates unstructured, free-form natural language text.</p> <code>-</code> <code>STRUCTURED_OUTPUT</code> <p>Produces structured JSON output.</p> <code>-</code> <code>FUNCTION_CALLING</code> <p>Structured output for tools (functions) to be called.</p> <code>dict[str,</code> <code>Any] | type[BaseModel] | None</code> <p>schema_ for structured output. Defaults to empty dict.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>class BaseLLM(ConnectionNode):\n    \"\"\"Base class for all LLM nodes.\n\n    Attributes:\n        MODEL_PREFIX (ClassVar[str | None]): Optional model prefix.\n        name (str | None): Name of the LLM node. Defaults to \"LLM\".\n        model (str): Model to use for the LLM.\n        prompt (Prompt | None): Prompt to use for the LLM.\n        connection (BaseConnection): Connection to use for the LLM.\n        group (Literal[NodeGroup.LLMS]): Group for the node. Defaults to NodeGroup.LLMS.\n        temperature (float): Temperature for the LLM. Defaults to 0.1.\n        max_tokens (int): Maximum number of tokens for the LLM. Defaults to 1000.\n        stop (list[str]): List of tokens to stop at for the LLM.\n        error_handling (ErrorHandling): Error handling config. Defaults to ErrorHandling(timeout_seconds=600).\n        top_p (float | None): Value to consider tokens with top_p probability.\n        seed (int | None): Seed for generating the same result for repeated requests.\n        presence_penalty (float | None): Penalize new tokens based on their existence in the text.\n        frequency_penalty (float | None): Penalize new tokens based on their frequency in the text.\n        tool_choice (str | None): Value to control which function is called by the model.\n        inference_mode (InferenceMode): Determines how the model handles inference tasks and formats outputs.\n        - InferenceMode.DEFAULT: Generates unstructured, free-form natural language text.\n        - InferenceMode.STRUCTURED_OUTPUT: Produces structured JSON output.\n        - InferenceMode.FUNCTION_CALLING: Structured output for tools (functions) to be called.\n        dict[str, Any] | type[BaseModel] | None: schema_ for structured output. Defaults to empty dict.\n    \"\"\"\n\n    MODEL_PREFIX: ClassVar[str | None] = None\n    name: str | None = \"LLM\"\n    model: str\n    prompt: Prompt | None = None\n    connection: BaseConnection\n    group: Literal[NodeGroup.LLMS] = NodeGroup.LLMS\n    temperature: float = 0.1\n    max_tokens: int = 1000\n    stop: list[str] = None\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    top_p: float | None = None\n    seed: int | None = None\n    presence_penalty: float | None = None\n    frequency_penalty: float | None = None\n    tool_choice: str | None = None\n    inference_mode: InferenceMode = InferenceMode.DEFAULT\n    schema_: dict[str, Any] | type[BaseModel] | None = Field(\n        None, description=\"Schema for structured output or function calling.\", alias=\"schema\"\n    )\n\n    _completion: Callable = PrivateAttr()\n    _stream_chunk_builder: Callable = PrivateAttr()\n\n    @field_validator(\"model\")\n    @classmethod\n    def set_model(cls, value: str | None) -&gt; str:\n        \"\"\"Set the model with the appropriate prefix.\n\n        Args:\n            value (str | None): The model value.\n\n        Returns:\n            str: The model value with the prefix.\n        \"\"\"\n        if cls.MODEL_PREFIX is not None and not value.startswith(cls.MODEL_PREFIX):\n            value = f\"{cls.MODEL_PREFIX}{value}\"\n        return value\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the BaseLLM instance.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n\n        # Save a bit of loading time as litellm is slow\n        from litellm import completion, stream_chunk_builder\n\n        # Avoid the same imports multiple times and for future usage in execute\n        self._completion = completion\n        self._stream_chunk_builder = stream_chunk_builder\n\n    @classmethod\n    def get_usage_data(\n        cls,\n        model: str,\n        completion: \"ModelResponse\",\n    ) -&gt; BaseLLMUsageData:\n        \"\"\"Get usage data for the LLM.\n\n        This method generates usage data for the LLM based on the provided messages.\n\n        Args:\n            model (str): The model to use for generating the usage data.\n            completion (ModelResponse): The completion response from the LLM.\n\n        Returns:\n            BaseLLMUsageData: A model containing the usage data for the LLM.\n        \"\"\"\n        from litellm import cost_per_token\n\n        usage = completion.model_extra[\"usage\"]\n        prompt_tokens = usage.prompt_tokens\n        completion_tokens = usage.completion_tokens\n        total_tokens = usage.total_tokens\n\n        try:\n            prompt_tokens_cost_usd, completion_tokens_cost_usd = cost_per_token(\n                model=model, prompt_tokens=prompt_tokens, completion_tokens=completion_tokens\n            )\n            total_tokens_cost_usd = prompt_tokens_cost_usd + completion_tokens_cost_usd\n        except Exception:\n            prompt_tokens_cost_usd, completion_tokens_cost_usd, total_tokens_cost_usd = None, None, None\n\n        return BaseLLMUsageData(\n            prompt_tokens=prompt_tokens,\n            prompt_tokens_cost_usd=prompt_tokens_cost_usd,\n            completion_tokens=completion_tokens,\n            completion_tokens_cost_usd=completion_tokens_cost_usd,\n            total_tokens=total_tokens,\n            total_tokens_cost_usd=total_tokens_cost_usd,\n        )\n\n    def _handle_completion_response(\n        self,\n        response: Union[\"ModelResponse\", \"CustomStreamWrapper\"],\n        config: RunnableConfig = None,\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"Handle completion response.\n\n        Args:\n            response (ModelResponse | CustomStreamWrapper): The response from the LLM.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content and tool calls.\n        \"\"\"\n        content = response.choices[0].message.content\n        if tool_calls := response.choices[0].message.tool_calls:\n            tool_calls = [tc.model_dump() for tc in tool_calls]\n\n        usage_data = self.get_usage_data(model=self.model, completion=response).model_dump()\n        self.run_on_node_execute_run(callbacks=config.callbacks, usage_data=usage_data, **kwargs)\n\n        return {\"content\": content, \"tool_calls\": tool_calls}\n\n    def _handle_streaming_completion_response(\n        self,\n        response: Union[\"ModelResponse\", \"CustomStreamWrapper\"],\n        messages: list[dict],\n        config: RunnableConfig = None,\n        **kwargs,\n    ):\n        \"\"\"Handle streaming completion response.\n\n        Args:\n            response (ModelResponse | CustomStreamWrapper): The response from the LLM.\n            messages (list[dict]): The messages used for the LLM.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content and tool calls.\n        \"\"\"\n        chunks = []\n        for chunk in response:\n            chunks.append(chunk)\n\n            self.run_on_node_execute_stream(\n                config.callbacks,\n                chunk.model_dump(),\n                **kwargs,\n            )\n\n        full_response = self._stream_chunk_builder(chunks=chunks, messages=messages)\n        return self._handle_completion_response(response=full_response, config=config, **kwargs)\n\n    def _get_response_format_and_tools(\n        self, inference_mode: InferenceMode, schema: dict[str, Any] | type[BaseModel] | None\n    ) -&gt; tuple[dict[str, Any] | None, dict[str, Any] | None]:\n        \"\"\"Get response format and tools based on inference mode and schema.\n\n        Args:\n            inference_mode (InferenceMode): The inference mode to use.\n            schema (dict[str, Any] | type[BaseModel] | None): The schema to use.\n\n        Returns:\n            tuple[dict[str, Any] | None, dict[str, Any] | None]: Response format and tools.\n\n        Raises:\n            ValueError: If schema is None when using STRUCTURED_OUTPUT or FUNCTION_CALLING modes.\n        \"\"\"\n        response_format = None\n        tools = None\n\n        match inference_mode:\n            case InferenceMode.STRUCTURED_OUTPUT:\n                if schema is None:\n                    raise ValueError(\"Schema must be provided when using STRUCTURED_OUTPUT inference mode\")\n                response_format = schema\n            case InferenceMode.FUNCTION_CALLING:\n                if schema is None:\n                    raise ValueError(\"Schema must be provided when using FUNCTION_CALLING inference mode\")\n                tools = schema\n\n        return response_format, tools\n\n    def execute(\n        self,\n        input_data: dict[str, Any],\n        config: RunnableConfig = None,\n        prompt: Prompt | None = None,\n        schema: dict | None = None,\n        inference_mode: InferenceMode | None = None,\n        **kwargs,\n    ):\n        \"\"\"Execute the LLM node.\n\n        This method processes the input data, formats the prompt, and generates a response using\n        the configured LLM.\n\n        Args:\n            input_data (dict[str, Any]): The input data for the LLM.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            prompt (Prompt, optional): The prompt to use for this execution. Defaults to None.\n            schema (Dict[str, Any], optional): schema_ for structured output or function calling.\n                Overrides instance schema_ if provided.\n            inference_mode (InferenceMode, optional): Mode of inference.\n                Overrides instance inference_mode if provided.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the generated content and tool calls.\n        \"\"\"\n        config = ensure_config(config)\n        prompt = prompt or self.prompt or Prompt(messages=[], tools=None)\n        messages = prompt.format_messages(**input_data)\n        base_tools = prompt.format_tools(**input_data)\n        self.run_on_node_execute_run(callbacks=config.callbacks, prompt_messages=messages, **kwargs)\n\n        # Use initialized client if it possible\n        params = self.connection.conn_params\n        if self.client and not isinstance(self.connection, HttpApiKey):\n            params = {\"client\": self.client}\n\n        current_inference_mode = inference_mode or self.inference_mode\n        current_schema = schema or self.schema_\n        response_format, tools = self._get_response_format_and_tools(\n            inference_mode=current_inference_mode, schema=current_schema\n        )\n        tools = tools or base_tools\n\n        response = self._completion(\n            model=self.model,\n            messages=messages,\n            stream=self.streaming.enabled,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            tools=tools,\n            tool_choice=self.tool_choice,\n            stop=self.stop,\n            top_p=self.top_p,\n            seed=self.seed,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            response_format=response_format,\n            drop_params=True,\n            **params,\n        )\n\n        handle_completion = (\n            self._handle_streaming_completion_response if self.streaming.enabled else self._handle_completion_response\n        )\n\n        return handle_completion(response=response, messages=messages, config=config, input_data=input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BaseLLM instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the BaseLLM instance.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    # Save a bit of loading time as litellm is slow\n    from litellm import completion, stream_chunk_builder\n\n    # Avoid the same imports multiple times and for future usage in execute\n    self._completion = completion\n    self._stream_chunk_builder = stream_chunk_builder\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.execute","title":"<code>execute(input_data, config=None, prompt=None, schema=None, inference_mode=None, **kwargs)</code>","text":"<p>Execute the LLM node.</p> <p>This method processes the input data, formats the prompt, and generates a response using the configured LLM.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data for the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>prompt</code> <code>Prompt</code> <p>The prompt to use for this execution. Defaults to None.</p> <code>None</code> <code>schema</code> <code>Dict[str, Any]</code> <p>schema_ for structured output or function calling. Overrides instance schema_ if provided.</p> <code>None</code> <code>inference_mode</code> <code>InferenceMode</code> <p>Mode of inference. Overrides instance inference_mode if provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the generated content and tool calls.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>def execute(\n    self,\n    input_data: dict[str, Any],\n    config: RunnableConfig = None,\n    prompt: Prompt | None = None,\n    schema: dict | None = None,\n    inference_mode: InferenceMode | None = None,\n    **kwargs,\n):\n    \"\"\"Execute the LLM node.\n\n    This method processes the input data, formats the prompt, and generates a response using\n    the configured LLM.\n\n    Args:\n        input_data (dict[str, Any]): The input data for the LLM.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        prompt (Prompt, optional): The prompt to use for this execution. Defaults to None.\n        schema (Dict[str, Any], optional): schema_ for structured output or function calling.\n            Overrides instance schema_ if provided.\n        inference_mode (InferenceMode, optional): Mode of inference.\n            Overrides instance inference_mode if provided.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the generated content and tool calls.\n    \"\"\"\n    config = ensure_config(config)\n    prompt = prompt or self.prompt or Prompt(messages=[], tools=None)\n    messages = prompt.format_messages(**input_data)\n    base_tools = prompt.format_tools(**input_data)\n    self.run_on_node_execute_run(callbacks=config.callbacks, prompt_messages=messages, **kwargs)\n\n    # Use initialized client if it possible\n    params = self.connection.conn_params\n    if self.client and not isinstance(self.connection, HttpApiKey):\n        params = {\"client\": self.client}\n\n    current_inference_mode = inference_mode or self.inference_mode\n    current_schema = schema or self.schema_\n    response_format, tools = self._get_response_format_and_tools(\n        inference_mode=current_inference_mode, schema=current_schema\n    )\n    tools = tools or base_tools\n\n    response = self._completion(\n        model=self.model,\n        messages=messages,\n        stream=self.streaming.enabled,\n        temperature=self.temperature,\n        max_tokens=self.max_tokens,\n        tools=tools,\n        tool_choice=self.tool_choice,\n        stop=self.stop,\n        top_p=self.top_p,\n        seed=self.seed,\n        presence_penalty=self.presence_penalty,\n        frequency_penalty=self.frequency_penalty,\n        response_format=response_format,\n        drop_params=True,\n        **params,\n    )\n\n    handle_completion = (\n        self._handle_streaming_completion_response if self.streaming.enabled else self._handle_completion_response\n    )\n\n    return handle_completion(response=response, messages=messages, config=config, input_data=input_data, **kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.get_usage_data","title":"<code>get_usage_data(model, completion)</code>  <code>classmethod</code>","text":"<p>Get usage data for the LLM.</p> <p>This method generates usage data for the LLM based on the provided messages.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use for generating the usage data.</p> required <code>completion</code> <code>ModelResponse</code> <p>The completion response from the LLM.</p> required <p>Returns:</p> Name Type Description <code>BaseLLMUsageData</code> <code>BaseLLMUsageData</code> <p>A model containing the usage data for the LLM.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>@classmethod\ndef get_usage_data(\n    cls,\n    model: str,\n    completion: \"ModelResponse\",\n) -&gt; BaseLLMUsageData:\n    \"\"\"Get usage data for the LLM.\n\n    This method generates usage data for the LLM based on the provided messages.\n\n    Args:\n        model (str): The model to use for generating the usage data.\n        completion (ModelResponse): The completion response from the LLM.\n\n    Returns:\n        BaseLLMUsageData: A model containing the usage data for the LLM.\n    \"\"\"\n    from litellm import cost_per_token\n\n    usage = completion.model_extra[\"usage\"]\n    prompt_tokens = usage.prompt_tokens\n    completion_tokens = usage.completion_tokens\n    total_tokens = usage.total_tokens\n\n    try:\n        prompt_tokens_cost_usd, completion_tokens_cost_usd = cost_per_token(\n            model=model, prompt_tokens=prompt_tokens, completion_tokens=completion_tokens\n        )\n        total_tokens_cost_usd = prompt_tokens_cost_usd + completion_tokens_cost_usd\n    except Exception:\n        prompt_tokens_cost_usd, completion_tokens_cost_usd, total_tokens_cost_usd = None, None, None\n\n    return BaseLLMUsageData(\n        prompt_tokens=prompt_tokens,\n        prompt_tokens_cost_usd=prompt_tokens_cost_usd,\n        completion_tokens=completion_tokens,\n        completion_tokens_cost_usd=completion_tokens_cost_usd,\n        total_tokens=total_tokens,\n        total_tokens_cost_usd=total_tokens_cost_usd,\n    )\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLM.set_model","title":"<code>set_model(value)</code>  <code>classmethod</code>","text":"<p>Set the model with the appropriate prefix.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>The model value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The model value with the prefix.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>@field_validator(\"model\")\n@classmethod\ndef set_model(cls, value: str | None) -&gt; str:\n    \"\"\"Set the model with the appropriate prefix.\n\n    Args:\n        value (str | None): The model value.\n\n    Returns:\n        str: The model value with the prefix.\n    \"\"\"\n    if cls.MODEL_PREFIX is not None and not value.startswith(cls.MODEL_PREFIX):\n        value = f\"{cls.MODEL_PREFIX}{value}\"\n    return value\n</code></pre>"},{"location":"dynamiq/nodes/llms/base/#dynamiq.nodes.llms.base.BaseLLMUsageData","title":"<code>BaseLLMUsageData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for LLM usage data.</p> <p>Attributes:</p> Name Type Description <code>prompt_tokens</code> <code>int</code> <p>Number of prompt tokens.</p> <code>prompt_tokens_cost_usd</code> <code>float | None</code> <p>Cost of prompt tokens in USD.</p> <code>completion_tokens</code> <code>int</code> <p>Number of completion tokens.</p> <code>completion_tokens_cost_usd</code> <code>float | None</code> <p>Cost of completion tokens in USD.</p> <code>total_tokens</code> <code>int</code> <p>Total number of tokens.</p> <code>total_tokens_cost_usd</code> <code>float | None</code> <p>Total cost of tokens in USD.</p> Source code in <code>dynamiq/nodes/llms/base.py</code> <pre><code>class BaseLLMUsageData(BaseModel):\n    \"\"\"Model for LLM usage data.\n\n    Attributes:\n        prompt_tokens (int): Number of prompt tokens.\n        prompt_tokens_cost_usd (float | None): Cost of prompt tokens in USD.\n        completion_tokens (int): Number of completion tokens.\n        completion_tokens_cost_usd (float | None): Cost of completion tokens in USD.\n        total_tokens (int): Total number of tokens.\n        total_tokens_cost_usd (float | None): Total cost of tokens in USD.\n    \"\"\"\n    prompt_tokens: int\n    prompt_tokens_cost_usd: float | None\n    completion_tokens: int\n    completion_tokens_cost_usd: float | None\n    total_tokens: int\n    total_tokens_cost_usd: float | None\n</code></pre>"},{"location":"dynamiq/nodes/llms/bedrock/","title":"Bedrock","text":""},{"location":"dynamiq/nodes/llms/bedrock/#dynamiq.nodes.llms.bedrock.Bedrock","title":"<code>Bedrock</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Bedrock LLM node.</p> <p>This class provides an implementation for the Bedrock Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>AWS | None</code> <p>The connection to use for the Bedrock LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Bedrock model name.</p> Source code in <code>dynamiq/nodes/llms/bedrock.py</code> <pre><code>class Bedrock(BaseLLM):\n    \"\"\"Bedrock LLM node.\n\n    This class provides an implementation for the Bedrock Language Model node.\n\n    Attributes:\n        connection (AWSConnection | None): The connection to use for the Bedrock LLM.\n        MODEL_PREFIX (str): The prefix for the Bedrock model name.\n    \"\"\"\n    connection: AWSConnection | None = None\n    MODEL_PREFIX = \"bedrock/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Bedrock LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = AWSConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/bedrock/#dynamiq.nodes.llms.bedrock.Bedrock.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Bedrock LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/bedrock.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Bedrock LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = AWSConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cerebras/","title":"Cerebras","text":""},{"location":"dynamiq/nodes/llms/cerebras/#dynamiq.nodes.llms.cerebras.Cerebras","title":"<code>Cerebras</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Cerebras LLM node.</p> <p>This class provides an implementation for the Cerebras Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Cerebras</code> <p>The connection to use for the Cerebras LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Cerebras model name.</p> Source code in <code>dynamiq/nodes/llms/cerebras.py</code> <pre><code>class Cerebras(BaseLLM):\n    \"\"\"Cerebras LLM node.\n\n    This class provides an implementation for the Cerebras Language Model node.\n\n    Attributes:\n        connection (CerebrasConnection): The connection to use for the Cerebras LLM.\n        MODEL_PREFIX (str): The prefix for the Cerebras model name.\n    \"\"\"\n    connection: CerebrasConnection\n    MODEL_PREFIX = \"cerebras/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Cerebras LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CerebrasConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cerebras/#dynamiq.nodes.llms.cerebras.Cerebras.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Cerebras LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/cerebras.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Cerebras LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CerebrasConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cohere/","title":"Cohere","text":""},{"location":"dynamiq/nodes/llms/cohere/#dynamiq.nodes.llms.cohere.Cohere","title":"<code>Cohere</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Cohere LLM node.</p> <p>This class provides an implementation for the Cohere Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Cohere</code> <p>The connection to use for the Cohere LLM.</p> Source code in <code>dynamiq/nodes/llms/cohere.py</code> <pre><code>class Cohere(BaseLLM):\n    \"\"\"Cohere LLM node.\n\n    This class provides an implementation for the Cohere Language Model node.\n\n    Attributes:\n        connection (CohereConnection): The connection to use for the Cohere LLM.\n    \"\"\"\n    connection: CohereConnection\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Cohere LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = CohereConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/cohere/#dynamiq.nodes.llms.cohere.Cohere.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Cohere LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/cohere.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Cohere LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = CohereConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/custom_llm/","title":"Custom llm","text":""},{"location":"dynamiq/nodes/llms/custom_llm/#dynamiq.nodes.llms.custom_llm.CustomLLM","title":"<code>CustomLLM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Class for creating custom LLM.</p> <p>This class provides a foundation for sending various requests using different LLM endpoints.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>HttpApiKey</code> <p>The connection to use for the LLM.</p> Source code in <code>dynamiq/nodes/llms/custom_llm.py</code> <pre><code>class CustomLLM(BaseLLM):\n    \"\"\"Class for creating custom LLM.\n\n    This class provides a foundation for sending various requests using different LLM endpoints.\n\n    Attributes:\n        connection (HttpApiKey): The connection to use for the LLM.\n    \"\"\"\n    connection: HttpApiKey\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepinfra/","title":"Deepinfra","text":""},{"location":"dynamiq/nodes/llms/deepinfra/#dynamiq.nodes.llms.deepinfra.DeepInfra","title":"<code>DeepInfra</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>DeepInfra LLM node.</p> <p>This class provides an implementation for the DeepInfra Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>DeepInfra</code> <p>The connection to use for the DeepInfra LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the DeepInfra model name.</p> Source code in <code>dynamiq/nodes/llms/deepinfra.py</code> <pre><code>class DeepInfra(BaseLLM):\n    \"\"\"DeepInfra LLM node.\n\n    This class provides an implementation for the DeepInfra Language Model node.\n\n    Attributes:\n        connection (DeepInfraConnection): The connection to use for the DeepInfra LLM.\n        MODEL_PREFIX (str): The prefix for the DeepInfra model name.\n    \"\"\"\n    connection: DeepInfraConnection\n    MODEL_PREFIX = \"deepinfra/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the DeepInfra LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = DeepInfraConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepinfra/#dynamiq.nodes.llms.deepinfra.DeepInfra.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the DeepInfra LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/deepinfra.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the DeepInfra LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = DeepInfraConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepseek/","title":"Deepseek","text":""},{"location":"dynamiq/nodes/llms/deepseek/#dynamiq.nodes.llms.deepseek.DeepSeek","title":"<code>DeepSeek</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>DeepSeek LLM node.</p> <p>This class provides an implementation for the DeepSeek Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>DeepSeek</code> <p>The connection to use for the DeepSeek LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the DeepSeek model name.</p> Source code in <code>dynamiq/nodes/llms/deepseek.py</code> <pre><code>class DeepSeek(BaseLLM):\n    \"\"\"DeepSeek LLM node.\n\n    This class provides an implementation for the DeepSeek Language Model node.\n\n    Attributes:\n        connection (DeepSeekConnection): The connection to use for the DeepSeek LLM.\n        MODEL_PREFIX (str): The prefix for the DeepSeek model name.\n    \"\"\"\n\n    connection: DeepSeekConnection\n    MODEL_PREFIX = \"deepseek/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Replicate LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = DeepSeekConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/deepseek/#dynamiq.nodes.llms.deepseek.DeepSeek.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Replicate LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/deepseek.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Replicate LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = DeepSeekConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/gemini/","title":"Gemini","text":""},{"location":"dynamiq/nodes/llms/gemini/#dynamiq.nodes.llms.gemini.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Gemini LLM node.</p> <p>This class provides an implementation for the Gemini Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Gemini | GeminiVertexAI</code> <p>The connection to use for the Gemini LLM.</p> Source code in <code>dynamiq/nodes/llms/gemini.py</code> <pre><code>class Gemini(BaseLLM):\n    \"\"\"Gemini LLM node.\n\n    This class provides an implementation for the Gemini Language Model node.\n\n    Attributes:\n        connection (GeminiConnection | GeminiVertexAI): The connection to use for the Gemini LLM.\n    \"\"\"\n    connection: GeminiConnection | GeminiVertexAI\n\n    @model_validator(mode=\"after\")\n    def check_model(self) -&gt; \"Gemini\":\n        \"\"\"Validate and set the model prefix based on the connection type.\n\n        Returns:\n            self: The updated instance.\n        \"\"\"\n        connection = self.connection\n        value = self.model\n        if not value.startswith(VERTEXAI_MODEL_PREFIX) and isinstance(connection, GeminiVertexAI):\n            self.model = f\"{VERTEXAI_MODEL_PREFIX}{value}\"\n        elif not value.startswith(GEMINI_MODEL_PREFIX) and isinstance(connection, GeminiConnection):\n            self.model = f\"{GEMINI_MODEL_PREFIX}{value}\"\n        return self\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Gemini LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GeminiConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/gemini/#dynamiq.nodes.llms.gemini.Gemini.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Gemini LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/gemini.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Gemini LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = GeminiConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/gemini/#dynamiq.nodes.llms.gemini.Gemini.check_model","title":"<code>check_model()</code>","text":"<p>Validate and set the model prefix based on the connection type.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>Gemini</code> <p>The updated instance.</p> Source code in <code>dynamiq/nodes/llms/gemini.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_model(self) -&gt; \"Gemini\":\n    \"\"\"Validate and set the model prefix based on the connection type.\n\n    Returns:\n        self: The updated instance.\n    \"\"\"\n    connection = self.connection\n    value = self.model\n    if not value.startswith(VERTEXAI_MODEL_PREFIX) and isinstance(connection, GeminiVertexAI):\n        self.model = f\"{VERTEXAI_MODEL_PREFIX}{value}\"\n    elif not value.startswith(GEMINI_MODEL_PREFIX) and isinstance(connection, GeminiConnection):\n        self.model = f\"{GEMINI_MODEL_PREFIX}{value}\"\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/llms/groq/","title":"Groq","text":""},{"location":"dynamiq/nodes/llms/groq/#dynamiq.nodes.llms.groq.Groq","title":"<code>Groq</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Groq LLM node.</p> <p>This class provides an implementation for the Groq Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Groq | None</code> <p>The connection to use for the Groq LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Groq model name.</p> Source code in <code>dynamiq/nodes/llms/groq.py</code> <pre><code>class Groq(BaseLLM):\n    \"\"\"Groq LLM node.\n\n    This class provides an implementation for the Groq Language Model node.\n\n    Attributes:\n        connection (GroqConnection | None): The connection to use for the Groq LLM.\n        MODEL_PREFIX (str): The prefix for the Groq model name.\n    \"\"\"\n    connection: GroqConnection | None = None\n    MODEL_PREFIX = \"groq/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Groq LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = GroqConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/groq/#dynamiq.nodes.llms.groq.Groq.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Groq LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/groq.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Groq LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = GroqConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/huggingface/","title":"Huggingface","text":""},{"location":"dynamiq/nodes/llms/huggingface/#dynamiq.nodes.llms.huggingface.HuggingFace","title":"<code>HuggingFace</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>HuggingFace LLM node.</p> <p>This class provides an implementation for the HuggingFace Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>HuggingFace | None</code> <p>The connection to use for the HuggingFace LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the HuggingFace model name.</p> Source code in <code>dynamiq/nodes/llms/huggingface.py</code> <pre><code>class HuggingFace(BaseLLM):\n    \"\"\"HuggingFace LLM node.\n\n    This class provides an implementation for the HuggingFace Language Model node.\n\n    Attributes:\n        connection (HuggingFaceConnection | None): The connection to use for the HuggingFace LLM.\n        MODEL_PREFIX (str): The prefix for the HuggingFace model name.\n    \"\"\"\n    connection: HuggingFaceConnection | None = None\n    MODEL_PREFIX = \"huggingface/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the HuggingFace LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = HuggingFaceConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/huggingface/#dynamiq.nodes.llms.huggingface.HuggingFace.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the HuggingFace LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/huggingface.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the HuggingFace LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = HuggingFaceConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/mistral/","title":"Mistral","text":""},{"location":"dynamiq/nodes/llms/mistral/#dynamiq.nodes.llms.mistral.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Mistral LLM node.</p> <p>This class provides an implementation for the Mistral Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Mistral | None</code> <p>The connection to use for the Mistral LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Mistral model name.</p> Source code in <code>dynamiq/nodes/llms/mistral.py</code> <pre><code>class Mistral(BaseLLM):\n    \"\"\"Mistral LLM node.\n\n    This class provides an implementation for the Mistral Language Model node.\n\n    Attributes:\n        connection (MistralConnection | None): The connection to use for the Mistral LLM.\n        MODEL_PREFIX (str): The prefix for the Mistral model name.\n    \"\"\"\n    connection: MistralConnection | None = None\n    MODEL_PREFIX = \"mistral/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Mistral LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = MistralConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/mistral/#dynamiq.nodes.llms.mistral.Mistral.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Mistral LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/mistral.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Mistral LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = MistralConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/","title":"Openai","text":""},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>OpenAI LLM node.</p> <p>This class provides an implementation for the OpenAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>OpenAI | None</code> <p>The connection to use for the OpenAI LLM.</p> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>class OpenAI(BaseLLM):\n    \"\"\"OpenAI LLM node.\n\n    This class provides an implementation for the OpenAI Language Model node.\n\n    Attributes:\n        connection (OpenAIConnection | None): The connection to use for the OpenAI LLM.\n    \"\"\"\n    connection: OpenAIConnection | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the OpenAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = OpenAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/openai/#dynamiq.nodes.llms.openai.OpenAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the OpenAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/openai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the OpenAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = OpenAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/perplexity/","title":"Perplexity","text":""},{"location":"dynamiq/nodes/llms/perplexity/#dynamiq.nodes.llms.perplexity.Perplexity","title":"<code>Perplexity</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Perplexity LLM node.</p> <p>This class provides an implementation for the Perplexity Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Perplexity</code> <p>The connection to use for the Perplexity LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Perplexity model name.</p> Source code in <code>dynamiq/nodes/llms/perplexity.py</code> <pre><code>class Perplexity(BaseLLM):\n    \"\"\"Perplexity LLM node.\n\n    This class provides an implementation for the Perplexity Language Model node.\n\n    Attributes:\n        connection (PerplexityConnection): The connection to use for the Perplexity LLM.\n        MODEL_PREFIX (str): The prefix for the Perplexity model name.\n    \"\"\"\n\n    connection: PerplexityConnection\n    MODEL_PREFIX = \"perplexity/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Replicate LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = PerplexityConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/perplexity/#dynamiq.nodes.llms.perplexity.Perplexity.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Replicate LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/perplexity.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Replicate LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = PerplexityConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/replicate/","title":"Replicate","text":""},{"location":"dynamiq/nodes/llms/replicate/#dynamiq.nodes.llms.replicate.Replicate","title":"<code>Replicate</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Replicate LLM node.</p> <p>This class provides an implementation for the Replicate Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>Replicate</code> <p>The connection to use for the Replicate LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the Replicate model name.</p> Source code in <code>dynamiq/nodes/llms/replicate.py</code> <pre><code>class Replicate(BaseLLM):\n    \"\"\"Replicate LLM node.\n\n    This class provides an implementation for the Replicate Language Model node.\n\n    Attributes:\n        connection (ReplicateConnection): The connection to use for the Replicate LLM.\n        MODEL_PREFIX (str): The prefix for the Replicate model name.\n    \"\"\"\n    connection: ReplicateConnection\n    MODEL_PREFIX = \"replicate/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the Replicate LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = ReplicateConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/replicate/#dynamiq.nodes.llms.replicate.Replicate.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Replicate LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/replicate.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the Replicate LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = ReplicateConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/sambanova/","title":"Sambanova","text":""},{"location":"dynamiq/nodes/llms/sambanova/#dynamiq.nodes.llms.sambanova.SambaNova","title":"<code>SambaNova</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>SambaNova LLM node.</p> <p>This class provides an implementation for the SambaNova Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>SambaNova</code> <p>The connection to use for the SambaNova LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the SambaNova model name.</p> Source code in <code>dynamiq/nodes/llms/sambanova.py</code> <pre><code>class SambaNova(BaseLLM):\n    \"\"\"SambaNova LLM node.\n\n    This class provides an implementation for the SambaNova Language Model node.\n\n    Attributes:\n        connection (SambaNovaConnection): The connection to use for the SambaNova LLM.\n        MODEL_PREFIX (str): The prefix for the SambaNova model name.\n    \"\"\"\n    connection: SambaNovaConnection\n    MODEL_PREFIX = \"sambanova/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the SambaNova LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = SambaNovaConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/sambanova/#dynamiq.nodes.llms.sambanova.SambaNova.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the SambaNova LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/sambanova.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the SambaNova LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = SambaNovaConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/togetherai/","title":"Togetherai","text":""},{"location":"dynamiq/nodes/llms/togetherai/#dynamiq.nodes.llms.togetherai.TogetherAI","title":"<code>TogetherAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>TogetherAI LLM node.</p> <p>This class provides an implementation for the TogetherAI Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>TogetherAI | None</code> <p>The connection to use for the TogetherAI LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the TogetherAI model name.</p> Source code in <code>dynamiq/nodes/llms/togetherai.py</code> <pre><code>class TogetherAI(BaseLLM):\n    \"\"\"TogetherAI LLM node.\n\n    This class provides an implementation for the TogetherAI Language Model node.\n\n    Attributes:\n        connection (TogetherAIConnection | None): The connection to use for the TogetherAI LLM.\n        MODEL_PREFIX (str): The prefix for the TogetherAI model name.\n    \"\"\"\n    connection: TogetherAIConnection | None = None\n    MODEL_PREFIX = \"together_ai/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the TogetherAI LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = TogetherAIConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/togetherai/#dynamiq.nodes.llms.togetherai.TogetherAI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the TogetherAI LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/togetherai.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the TogetherAI LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = TogetherAIConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/watsonx/","title":"Watsonx","text":""},{"location":"dynamiq/nodes/llms/watsonx/#dynamiq.nodes.llms.watsonx.WatsonX","title":"<code>WatsonX</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>WatsonX LLM node.</p> <p>This class provides an implementation for the WatsonX Language Model node.</p> <p>Attributes:</p> Name Type Description <code>connection</code> <code>WatsonX | None</code> <p>The connection to use for the WatsonX LLM.</p> <code>MODEL_PREFIX</code> <code>str</code> <p>The prefix for the WatsonX model name.</p> Source code in <code>dynamiq/nodes/llms/watsonx.py</code> <pre><code>class WatsonX(BaseLLM):\n    \"\"\"WatsonX LLM node.\n\n    This class provides an implementation for the WatsonX Language Model node.\n\n    Attributes:\n        connection (WatsonXConnection | None): The connection to use for the WatsonX LLM.\n        MODEL_PREFIX (str): The prefix for the WatsonX model name.\n    \"\"\"\n    connection: WatsonXConnection | None = None\n    MODEL_PREFIX = \"watsonx/\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the WatsonX LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = WatsonXConnection()\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/llms/watsonx/#dynamiq.nodes.llms.watsonx.WatsonX.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WatsonX LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/llms/watsonx.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the WatsonX LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if kwargs.get(\"client\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = WatsonXConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/","title":"Operators","text":""},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice","title":"<code>Choice</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents a choice node in a flow.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class Choice(Node):\n    \"\"\"Represents a choice node in a flow.\"\"\"\n\n    name: str | None = \"Choice\"\n    group: Literal[NodeGroup.OPERATORS] = NodeGroup.OPERATORS\n    options: list[ChoiceOption] = []\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"options\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        # Separately dump ChoiceOption list as it has nested ChoiceCondition model\n        # Bug: https://github.com/pydantic/pydantic/issues/9670\n        data = self.model_dump(\n            exclude=self.to_dict_exclude_params,\n            serialize_as_any=True,\n            **kwargs,\n        )\n        data[\"options\"] = [option.model_dump(**kwargs) for option in self.options]\n        return data\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, RunnableResult]:\n        \"\"\"\n        Executes the choice node.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A dictionary of RunnableResults for each option.\n        \"\"\"\n        results = {}\n        if self.options:\n            run_id = kwargs.get(\"run_id\", uuid4())\n            config = ensure_config(config)\n            merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n            self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n            is_success_evaluation = False\n            for option in self.options:\n                if is_success_evaluation:\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.SKIP, input=input_data, output=None\n                    )\n                elif option.condition and self.evaluate(option.condition, input_data):\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.SUCCESS, input=input_data, output=True\n                    )\n                    is_success_evaluation = True\n                elif not option.condition:\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.SUCCESS, input=input_data, output=True\n                    )\n                    is_success_evaluation = True\n                else:\n                    results[option.id] = RunnableResult(\n                        status=RunnableStatus.FAILURE, input=input_data, output=False\n                    )\n\n        return results\n\n    @staticmethod\n    def evaluate(cond: ChoiceCondition, input: Any) -&gt; bool:\n        \"\"\"\n        Evaluates a choice condition.\n\n        Args:\n            cond: The condition to evaluate.\n            input: The input data to evaluate against.\n\n        Returns:\n            A boolean indicating whether the condition is met.\n\n        Raises:\n            ValueError: If the operator is not supported.\n        \"\"\"\n        value = jsonpath.filter(input, cond.variable, \"blah\")\n\n        if cond.operator == ConditionOperator.OR:\n            return (\n                any(Choice.evaluate(cond, value) for cond in cond.operands)\n                and not cond.is_not\n            )\n        elif cond.operator == ConditionOperator.AND:\n            return (\n                all(Choice.evaluate(cond, value) for cond in cond.operands)\n                and not cond.is_not\n            )\n        # boolean\n        elif cond.operator == ConditionOperator.BOOLEAN_EQUALS:\n            return value == cond.value and not cond.is_not\n        # numeric\n        if cond.operator == ConditionOperator.NUMERIC_EQUALS:\n            return value == cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN:\n            return value &gt; cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN_OR_EQUALS:\n            return value &gt;= cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN:\n            return value &lt; cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN_OR_EQUALS:\n            return value &lt;= cond.value and not cond.is_not\n        # string\n        elif cond.operator == ConditionOperator.STRING_EQUALS:\n            return value == cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.STRING_GREATER_THAN:\n            return value &gt; cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.STRING_GREATER_THAN_OR_EQUALS:\n            return value &gt;= cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.STRING_LESS_THAN:\n            return value &lt; cond.value and not cond.is_not\n        elif cond.operator == ConditionOperator.STRING_LESS_THAN_OR_EQUALS:\n            return value &lt;= cond.value and not cond.is_not\n        else:\n            raise ValueError(f\"Operator {cond.operator} not supported.\")\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice.evaluate","title":"<code>evaluate(cond, input)</code>  <code>staticmethod</code>","text":"<p>Evaluates a choice condition.</p> <p>Parameters:</p> Name Type Description Default <code>cond</code> <code>ChoiceCondition</code> <p>The condition to evaluate.</p> required <code>input</code> <code>Any</code> <p>The input data to evaluate against.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean indicating whether the condition is met.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the operator is not supported.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>@staticmethod\ndef evaluate(cond: ChoiceCondition, input: Any) -&gt; bool:\n    \"\"\"\n    Evaluates a choice condition.\n\n    Args:\n        cond: The condition to evaluate.\n        input: The input data to evaluate against.\n\n    Returns:\n        A boolean indicating whether the condition is met.\n\n    Raises:\n        ValueError: If the operator is not supported.\n    \"\"\"\n    value = jsonpath.filter(input, cond.variable, \"blah\")\n\n    if cond.operator == ConditionOperator.OR:\n        return (\n            any(Choice.evaluate(cond, value) for cond in cond.operands)\n            and not cond.is_not\n        )\n    elif cond.operator == ConditionOperator.AND:\n        return (\n            all(Choice.evaluate(cond, value) for cond in cond.operands)\n            and not cond.is_not\n        )\n    # boolean\n    elif cond.operator == ConditionOperator.BOOLEAN_EQUALS:\n        return value == cond.value and not cond.is_not\n    # numeric\n    if cond.operator == ConditionOperator.NUMERIC_EQUALS:\n        return value == cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN:\n        return value &gt; cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.NUMERIC_GREATER_THAN_OR_EQUALS:\n        return value &gt;= cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN:\n        return value &lt; cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.NUMERIC_LESS_THAN_OR_EQUALS:\n        return value &lt;= cond.value and not cond.is_not\n    # string\n    elif cond.operator == ConditionOperator.STRING_EQUALS:\n        return value == cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.STRING_GREATER_THAN:\n        return value &gt; cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.STRING_GREATER_THAN_OR_EQUALS:\n        return value &gt;= cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.STRING_LESS_THAN:\n        return value &lt; cond.value and not cond.is_not\n    elif cond.operator == ConditionOperator.STRING_LESS_THAN_OR_EQUALS:\n        return value &lt;= cond.value and not cond.is_not\n    else:\n        raise ValueError(f\"Operator {cond.operator} not supported.\")\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the choice node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, RunnableResult]</code> <p>A dictionary of RunnableResults for each option.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, RunnableResult]:\n    \"\"\"\n    Executes the choice node.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A dictionary of RunnableResults for each option.\n    \"\"\"\n    results = {}\n    if self.options:\n        run_id = kwargs.get(\"run_id\", uuid4())\n        config = ensure_config(config)\n        merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n        self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n        is_success_evaluation = False\n        for option in self.options:\n            if is_success_evaluation:\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.SKIP, input=input_data, output=None\n                )\n            elif option.condition and self.evaluate(option.condition, input_data):\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.SUCCESS, input=input_data, output=True\n                )\n                is_success_evaluation = True\n            elif not option.condition:\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.SUCCESS, input=input_data, output=True\n                )\n                is_success_evaluation = True\n            else:\n                results[option.id] = RunnableResult(\n                    status=RunnableStatus.FAILURE, input=input_data, output=False\n                )\n\n    return results\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Choice.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    # Separately dump ChoiceOption list as it has nested ChoiceCondition model\n    # Bug: https://github.com/pydantic/pydantic/issues/9670\n    data = self.model_dump(\n        exclude=self.to_dict_exclude_params,\n        serialize_as_any=True,\n        **kwargs,\n    )\n    data[\"options\"] = [option.model_dump(**kwargs) for option in self.options]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.ChoiceCondition","title":"<code>ChoiceCondition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a condition for a choice node.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class ChoiceCondition(BaseModel):\n    \"\"\"Represents a condition for a choice node.\"\"\"\n\n    variable: str = None\n    operator: ConditionOperator = None\n    value: Any = None\n    is_not: bool = False\n    operands: list[\"ChoiceCondition\"] = None\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.ChoiceExecute","title":"<code>ChoiceExecute</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the execution of a choice.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class ChoiceExecute(BaseModel):\n    \"\"\"Represents the execution of a choice.\"\"\"\n\n    condition: ChoiceCondition | None = None\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.ChoiceOption","title":"<code>ChoiceOption</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an option for a choice node.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class ChoiceOption(BaseModel):\n    \"\"\"Represents an option for a choice node.\"\"\"\n\n    id: str = Field(default_factory=generate_uuid)\n    name: str | None = None\n    condition: ChoiceCondition | None = None\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.ConditionOperator","title":"<code>ConditionOperator</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum representing various condition operators for choice nodes.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class ConditionOperator(Enum):\n    \"\"\"Enum representing various condition operators for choice nodes.\"\"\"\n\n    OR = \"or\"\n    AND = \"and\"\n    BOOLEAN_EQUALS = \"boolean-equals\"\n    BOOLEAN_EQUALS_PATH = \"boolean-equals-path\"\n    NUMERIC_EQUALS = \"numeric-equals\"\n    NUMERIC_EQUALS_PATH = \"numeric-equals-path\"\n    NUMERIC_GREATER_THAN = \"numeric-greater-than\"\n    NUMERIC_GREATER_THAN_PATH = \"numeric-greater-than-path\"\n    NUMERIC_GREATER_THAN_OR_EQUALS = \"numeric-greater-than-or-equals\"\n    NUMERIC_GREATER_THAN_OR_EQUALS_PATH = \"numeric-greater-than-or-equals-path\"\n    NUMERIC_LESS_THAN = \"numeric-less-than\"\n    NUMERIC_LESS_THAN_PATH = \"numeric-less-than-path\"\n    NUMERIC_LESS_THAN_OR_EQUALS = \"numeric-less-than-or-equals\"\n    NUMERIC_LESS_THAN_OR_EQUALS_PATH = \"numeric-less-than-or-equals-path\"\n    STRING_EQUALS = \"string-equals\"\n    STRING_EQUALS_PATH = \"string-equals-path\"\n    STRING_GREATER_THAN = \"string-greater-than\"\n    STRING_GREATER_THAN_PATH = \"string-greater-than-path\"\n    STRING_GREATER_THAN_OR_EQUALS = \"string-greater-than-or-equals\"\n    STRING_GREATER_THAN_OR_EQUALS_PATH = \"string-greater-than-or-equals-path\"\n    STRING_LESS_THAN = \"string-less-than\"\n    STRING_LESS_THAN_PATH = \"string-less-than-path\"\n    STRING_LESS_THAN_OR_EQUALS = \"string-less-than-or-equals\"\n    STRING_LESS_THAN_OR_EQUALS_PATH = \"string-less-than-or-equals-path\"\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map","title":"<code>Map</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents a map node in a flow.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class Map(Node):\n    \"\"\"Represents a map node in a flow.\"\"\"\n\n    name: str | None = \"Map\"\n    group: Literal[NodeGroup.OPERATORS] = NodeGroup.OPERATORS\n    node: Node\n    behavior: Behavior | None = Behavior.RETURN\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"node\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"node\"] = self.node.to_dict(**kwargs)\n        return data\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the map node.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A list of outputs from executing the flow on each input item.\n\n        Raises:\n            Exception: If the input is not a list or if any flow execution fails.\n        \"\"\"\n        if isinstance(input_data, dict):\n            input_data = input_data[\"inputs\"]\n\n        if not isinstance(input_data, list):\n            logger.error(f\"Map operator {self.id} input is not a list.\")\n            raise ValueError(f\"Map operator {self.id} input is not a list.\")\n\n        output = []\n        run_id = kwargs.get(\"run_id\", uuid4())\n        config = ensure_config(config)\n        merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        for index, data in enumerate(input_data, start=1):\n            result = self.node.run(data, config, **merged_kwargs)\n            if result.status != RunnableStatus.SUCCESS:\n                if self.behavior == Behavior.RAISE:\n                    raise ValueError(f\"Map node failed to execute: node under iteration index {index} has failed.\")\n                logger.error(f\"Node under iteration index {index} has failed.\")\n            output.append(result.output)\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the map node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A list of outputs from executing the flow on each input item.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the input is not a list or if any flow execution fails.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the map node.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A list of outputs from executing the flow on each input item.\n\n    Raises:\n        Exception: If the input is not a list or if any flow execution fails.\n    \"\"\"\n    if isinstance(input_data, dict):\n        input_data = input_data[\"inputs\"]\n\n    if not isinstance(input_data, list):\n        logger.error(f\"Map operator {self.id} input is not a list.\")\n        raise ValueError(f\"Map operator {self.id} input is not a list.\")\n\n    output = []\n    run_id = kwargs.get(\"run_id\", uuid4())\n    config = ensure_config(config)\n    merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    for index, data in enumerate(input_data, start=1):\n        result = self.node.run(data, config, **merged_kwargs)\n        if result.status != RunnableStatus.SUCCESS:\n            if self.behavior == Behavior.RAISE:\n                raise ValueError(f\"Map node failed to execute: node under iteration index {index} has failed.\")\n            logger.error(f\"Node under iteration index {index} has failed.\")\n        output.append(result.output)\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Map.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"node\"] = self.node.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Pass","title":"<code>Pass</code>","text":"<p>               Bases: <code>Node</code></p> <p>Represents a pass node in a flow.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>class Pass(Node):\n    \"\"\"Represents a pass node in a flow.\"\"\"\n\n    group: Literal[NodeGroup.OPERATORS] = NodeGroup.OPERATORS\n    transformers: list[Transformer] = []\n\n    def execute_with_retry(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the pass node with retry logic.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The input data if no transformers are present, otherwise the transformed data.\n        \"\"\"\n        if self.transformers:\n            return super().execute_with_retry(input_data, config, **kwargs)\n\n        return input_data\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Executes the pass node.\n\n        Args:\n            input_data: The input data for the node.\n            config: The runnable configuration.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The input data if no transformers are present, otherwise the transformed data.\n        \"\"\"\n        output = input_data\n        if self.transformers:\n            run_id = kwargs.get(\"run_id\", uuid4())\n            config = ensure_config(config)\n            merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n            self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n            for transformer in self.transformers:\n                output = self.transform(output, transformer, self.id)\n\n        return output\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Pass.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the pass node.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The input data if no transformers are present, otherwise the transformed data.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the pass node.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The input data if no transformers are present, otherwise the transformed data.\n    \"\"\"\n    output = input_data\n    if self.transformers:\n        run_id = kwargs.get(\"run_id\", uuid4())\n        config = ensure_config(config)\n        merged_kwargs = {**kwargs, \"parent_run_id\": run_id}\n\n        self.run_on_node_execute_run(config.callbacks, **merged_kwargs)\n\n        for transformer in self.transformers:\n            output = self.transform(output, transformer, self.id)\n\n    return output\n</code></pre>"},{"location":"dynamiq/nodes/operators/operators/#dynamiq.nodes.operators.operators.Pass.execute_with_retry","title":"<code>execute_with_retry(input_data, config=None, **kwargs)</code>","text":"<p>Executes the pass node with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data for the node.</p> required <code>config</code> <code>RunnableConfig</code> <p>The runnable configuration.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The input data if no transformers are present, otherwise the transformed data.</p> Source code in <code>dynamiq/nodes/operators/operators.py</code> <pre><code>def execute_with_retry(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Executes the pass node with retry logic.\n\n    Args:\n        input_data: The input data for the node.\n        config: The runnable configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        The input data if no transformers are present, otherwise the transformed data.\n    \"\"\"\n    if self.transformers:\n        return super().execute_with_retry(input_data, config, **kwargs)\n\n    return input_data\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/","title":"Llm","text":""},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker","title":"<code>LLMDocumentRanker</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node class for ranking documents using a Large Language Model (LLM).</p> <p>This class can use any LLM to rank and select relevant documents based on a query. By default, it utilizes an OpenAI language model.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RANKERS]</code> <p>The group the node belongs to. Default is NodeGroup.RANKERS.</p> <code>name</code> <code>str</code> <p>The name of the node. Default is \"LLMDocumentRanker\".</p> <code>prompt_template</code> <code>str</code> <p>The template for the prompt to be used with the LLM. Default is DEFAULT_PROMPT.</p> <code>top_k</code> <code>int</code> <p>The number of top documents to return. Default is 5.</p> <code>llm</code> <code>BaseLLM</code> <p>The LLM instance used for ranking. Default is None.</p> <p>Example:</p> <pre><code>from dynamiq.nodes.rankers import LLMDocumentRanker\nfrom dynamiq.types import Document\n\n# Initialize the ranker\nranker = LLMDocumentRanker()\n\n# Example input data\ninput_data = {\n    \"query\": \"example query\",\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\n# Execute the ranker\noutput = ranker.execute(input_data)\n\n# Output will be a dictionary with ranked documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>class LLMDocumentRanker(Node):\n    \"\"\"\n    A Node class for ranking documents using a Large Language Model (LLM).\n\n    This class can use any LLM to rank and select relevant documents based on a query. By default, it utilizes an OpenAI\n    language model.\n\n    Attributes:\n        group (Literal[NodeGroup.RANKERS]): The group the node belongs to. Default is NodeGroup.RANKERS.\n        name (str): The name of the node. Default is \"LLMDocumentRanker\".\n        prompt_template (str): The template for the prompt to be used with the LLM. Default is DEFAULT_PROMPT.\n        top_k (int): The number of top documents to return. Default is 5.\n        llm (BaseLLM): The LLM instance used for ranking. Default is None.\n\n    Example:\n\n        from dynamiq.nodes.rankers import LLMDocumentRanker\n        from dynamiq.types import Document\n\n        # Initialize the ranker\n        ranker = LLMDocumentRanker()\n\n        # Example input data\n        input_data = {\n            \"query\": \"example query\",\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        # Execute the ranker\n        output = ranker.execute(input_data)\n\n        # Output will be a dictionary with ranked documents\n        print(output)\n    \"\"\"\n\n    group: Literal[NodeGroup.RANKERS] = NodeGroup.RANKERS\n    name: str = \"LLMDocumentRanker\"\n    prompt_template: str = DEFAULT_PROMPT\n    top_k: int = 5\n    llm: Node\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the LLMDocumentRanker with the given parameters and creates a default LLM node.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._run_depends = []\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self):\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Converts the instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the document ranker component.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use. Default is a new instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the document ranking process.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the query and documents to be ranked.\n            config (RunnableConfig, optional): Configuration for the execution. Default is None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n        Example:\n\n            input_data = {\n                \"query\": \"example query\",\n                \"documents\": [\n                    Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                    Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n                ]\n            }\n\n            output = ranker.execute(input_data)\n\n            # output will be a dictionary with ranked documents\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        ranked_documents = self.perform_llm_ranking(\n            query=input_data[\"query\"],\n            documents=input_data[\"documents\"],\n            config=config,\n            **kwargs,\n        )\n\n        return {\n            \"documents\": ranked_documents,\n        }\n\n    def perform_llm_ranking(\n        self, query: str, documents: list[Document], config: RunnableConfig, **kwargs\n    ) -&gt; list[Document]:\n        \"\"\"\n        Performs the actual ranking of documents using the LLM.\n\n        Args:\n            query (str): The query to rank documents against.\n            documents (list[Document]): The list of documents to be ranked.\n            config (RunnableConfig): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            list[Document]: A list of selected documents deemed relevant by the LLM.\n\n        Example:\n\n            query = \"example query\"\n            documents = [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n\n            ranked_documents = ranker.perform_llm_ranking(query, documents, config)\n\n            # ranked_documents will be a list of documents deemed relevant by the LLM\n        \"\"\"\n        run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n        inputs = [\n            {\"query\": query, \"passage\": document.content} for document in documents\n        ]\n\n        prompt = prompts.Prompt(\n            messages=[prompts.Message(role=\"user\", content=self.prompt_template)]\n        )\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            llm_results = list(\n                executor.map(\n                    lambda input_data: self.call_llm(\n                        input_data, prompt, config, **run_kwargs\n                    ),\n                    inputs,\n                )\n            )\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} documents\"\n        )\n\n        selected_documents = []\n\n        for result, document in zip(llm_results, documents):\n            if result == \"Yes\":\n                selected_documents.append(document)\n\n        logger.debug(\n            f\"Node {self.name} - {self.id}: LLM selected {len(selected_documents)} documents for context\"\n        )\n        return selected_documents\n\n    def call_llm(self, input_data, prompt, config, **run_kwargs):\n        \"\"\"\n        Calls the LLM with the given input data and prompt.\n\n        Args:\n            input_data (dict): The input data for the LLM.\n            prompt (prompts.Prompt): The prompt to be used with the LLM.\n            config (RunnableConfig): Configuration for the execution.\n            **run_kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The result from the LLM.\n\n        Example:\n\n            input_data = {\"query\": \"example query\", \"passage\": \"Document content\"}\n            prompt = prompts.Prompt(\n                messages=[prompts.Message(role=\"user\", content=DEFAULT_PROMPT)]\n            )\n            config = RunnableConfig()\n\n            result = ranker.call_llm(input_data, prompt, config)\n\n            # result will be the LLM's response, either 'Yes' or 'No'\n        \"\"\"\n        llm_result = self.llm.run(\n            input_data=input_data,\n            prompt=prompt,\n            config=config,\n            run_depends=self._run_depends,\n            **run_kwargs,\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n        if llm_result.status != RunnableStatus.SUCCESS:\n            logger.error(f\"Node {self.name} - {self.id}: LLM execution failed\")\n            raise ValueError(\"LLMDocumentRanker LLM execution failed\")\n        return llm_result.output[\"content\"]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.to_dict_exclude_params","title":"<code>to_dict_exclude_params</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initializes the LLMDocumentRanker with the given parameters and creates a default LLM node.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be passed to the parent class constructor.</p> <code>{}</code> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the LLMDocumentRanker with the given parameters and creates a default LLM node.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed to the parent class constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.call_llm","title":"<code>call_llm(input_data, prompt, config, **run_kwargs)</code>","text":"<p>Calls the LLM with the given input data and prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>The input data for the LLM.</p> required <code>prompt</code> <code>Prompt</code> <p>The prompt to be used with the LLM.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**run_kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The result from the LLM.</p> <p>Example:</p> <pre><code>input_data = {\"query\": \"example query\", \"passage\": \"Document content\"}\nprompt = prompts.Prompt(\n    messages=[prompts.Message(role=\"user\", content=DEFAULT_PROMPT)]\n)\nconfig = RunnableConfig()\n\nresult = ranker.call_llm(input_data, prompt, config)\n\n# result will be the LLM's response, either 'Yes' or 'No'\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def call_llm(self, input_data, prompt, config, **run_kwargs):\n    \"\"\"\n    Calls the LLM with the given input data and prompt.\n\n    Args:\n        input_data (dict): The input data for the LLM.\n        prompt (prompts.Prompt): The prompt to be used with the LLM.\n        config (RunnableConfig): Configuration for the execution.\n        **run_kwargs: Additional keyword arguments.\n\n    Returns:\n        str: The result from the LLM.\n\n    Example:\n\n        input_data = {\"query\": \"example query\", \"passage\": \"Document content\"}\n        prompt = prompts.Prompt(\n            messages=[prompts.Message(role=\"user\", content=DEFAULT_PROMPT)]\n        )\n        config = RunnableConfig()\n\n        result = ranker.call_llm(input_data, prompt, config)\n\n        # result will be the LLM's response, either 'Yes' or 'No'\n    \"\"\"\n    llm_result = self.llm.run(\n        input_data=input_data,\n        prompt=prompt,\n        config=config,\n        run_depends=self._run_depends,\n        **run_kwargs,\n    )\n    self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n    if llm_result.status != RunnableStatus.SUCCESS:\n        logger.error(f\"Node {self.name} - {self.id}: LLM execution failed\")\n        raise ValueError(\"LLMDocumentRanker LLM execution failed\")\n    return llm_result.output[\"content\"]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document ranking process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the query and documents to be ranked.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the original query and the ranked documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"query\": \"example query\",\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\noutput = ranker.execute(input_data)\n\n# output will be a dictionary with ranked documents\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the document ranking process.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the query and documents to be ranked.\n        config (RunnableConfig, optional): Configuration for the execution. Default is None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n    Example:\n\n        input_data = {\n            \"query\": \"example query\",\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        output = ranker.execute(input_data)\n\n        # output will be a dictionary with ranked documents\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    ranked_documents = self.perform_llm_ranking(\n        query=input_data[\"query\"],\n        documents=input_data[\"documents\"],\n        config=config,\n        **kwargs,\n    )\n\n    return {\n        \"documents\": ranked_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the document ranker component.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Default is a new instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the document ranker component.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use. Default is a new instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.perform_llm_ranking","title":"<code>perform_llm_ranking(query, documents, config, **kwargs)</code>","text":"<p>Performs the actual ranking of documents using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to rank documents against.</p> required <code>documents</code> <code>list[Document]</code> <p>The list of documents to be ranked.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of selected documents deemed relevant by the LLM.</p> <p>Example:</p> <pre><code>query = \"example query\"\ndocuments = [\n    Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n    Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n]\n\nranked_documents = ranker.perform_llm_ranking(query, documents, config)\n\n# ranked_documents will be a list of documents deemed relevant by the LLM\n</code></pre> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def perform_llm_ranking(\n    self, query: str, documents: list[Document], config: RunnableConfig, **kwargs\n) -&gt; list[Document]:\n    \"\"\"\n    Performs the actual ranking of documents using the LLM.\n\n    Args:\n        query (str): The query to rank documents against.\n        documents (list[Document]): The list of documents to be ranked.\n        config (RunnableConfig): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        list[Document]: A list of selected documents deemed relevant by the LLM.\n\n    Example:\n\n        query = \"example query\"\n        documents = [\n            Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n            Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n        ]\n\n        ranked_documents = ranker.perform_llm_ranking(query, documents, config)\n\n        # ranked_documents will be a list of documents deemed relevant by the LLM\n    \"\"\"\n    run_kwargs = kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}\n    inputs = [\n        {\"query\": query, \"passage\": document.content} for document in documents\n    ]\n\n    prompt = prompts.Prompt(\n        messages=[prompts.Message(role=\"user\", content=self.prompt_template)]\n    )\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        llm_results = list(\n            executor.map(\n                lambda input_data: self.call_llm(\n                    input_data, prompt, config, **run_kwargs\n                ),\n                inputs,\n            )\n        )\n\n    logger.debug(\n        f\"Node {self.name} - {self.id}: LLM processed {len(llm_results)} documents\"\n    )\n\n    selected_documents = []\n\n    for result, document in zip(llm_results, documents):\n        if result == \"Yes\":\n            selected_documents.append(document)\n\n    logger.debug(\n        f\"Node {self.name} - {self.id}: LLM selected {len(selected_documents)} documents for context\"\n    )\n    return selected_documents\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/rankers/llm/#dynamiq.nodes.rankers.llm.LLMDocumentRanker.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/rankers/llm.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Converts the instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/","title":"Recency","text":""},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker","title":"<code>TimeWeightedDocumentRanker</code>","text":"<p>               Bases: <code>Node</code></p> <p>A document ranker node boosting the recent content more.</p> <p>This ranker adjusts the initial scores of documents based on their recency. The recency coefficient depends on the number of days from today. The initial score is multiplied by the recency coefficient, and the documents are re-ranked based on the adjusted score.</p> The formula for the adjustment is <p>adjusted_score = score * recency_coefficient</p> The recency coefficient is calculated as follows <p>min_coefficient &lt;= coefficient &lt;= 1 (if the same date)</p> <p>The coefficient is determined based on the number of days since the content was created.</p> <p>An exponential decay formula is used to ensure that the coefficient decreases as the number of days increases, but never goes below the specified minimum coefficient.</p> The formula used is <p>coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)</p> This ensures that <ul> <li>If days &lt;= 0, the coefficient is 1.0 (no decay).</li> <li>If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).</li> <li>For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.</li> </ul> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RANKERS]</code> <p>The group this node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>top_k</code> <code>int</code> <p>The number of top documents to return. Default is 5.</p> <code>max_days</code> <code>int</code> <p>The maximum number of days to consider for adjustment. Default is 3600.</p> <code>min_coefficient</code> <code>float</code> <p>The minimum coefficient for score adjustment. Default is 0.9.</p> <code>date_field</code> <code>str</code> <p>The field name in the metadata containing the date. Default is \"date\".</p> <code>date_format</code> <code>str</code> <p>The format of the date string. Default is \"%d %B, %Y\".</p> <p>Example:</p> <pre><code>from dynamiq.nodes.rankers import TimeWeightedDocumentRanker\nfrom dynamiq.types import Document\n\n# Initialize the ranker\nranker = TimeWeightedDocumentRanker()\n\n# Example input data\ninput_data = {\n    \"query\": \"example query\",\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\n# Execute the ranker\noutput = ranker.execute(input_data)\n\n# Output will be a dictionary with ranked documents\nprint(output)\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>class TimeWeightedDocumentRanker(Node):\n    \"\"\"\n    A document ranker node boosting the recent content more.\n\n    This ranker adjusts the initial scores of documents based on their recency. The recency coefficient\n    depends on the number of days from today. The initial score is multiplied by the recency coefficient,\n    and the documents are re-ranked based on the adjusted score.\n\n    The formula for the adjustment is:\n        adjusted_score = score * recency_coefficient\n\n    The recency coefficient is calculated as follows:\n        min_coefficient &lt;= coefficient &lt;= 1 (if the same date)\n\n    The coefficient is determined based on the number of days since the content was created.\n\n    An exponential decay formula is used to ensure that the coefficient decreases as the number of days\n    increases, but never goes below the specified minimum coefficient.\n\n    The formula used is:\n        coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)\n\n    This ensures that:\n        - If days &lt;= 0, the coefficient is 1.0 (no decay).\n        - If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).\n        - For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.\n\n    Attributes:\n        group (Literal[NodeGroup.RANKERS]): The group this node belongs to.\n        name (str): The name of the node.\n        top_k (int): The number of top documents to return. Default is 5.\n        max_days (int): The maximum number of days to consider for adjustment. Default is 3600.\n        min_coefficient (float): The minimum coefficient for score adjustment. Default is 0.9.\n        date_field (str): The field name in the metadata containing the date. Default is \"date\".\n        date_format (str): The format of the date string. Default is \"%d %B, %Y\".\n\n    Example:\n\n        from dynamiq.nodes.rankers import TimeWeightedDocumentRanker\n        from dynamiq.types import Document\n\n        # Initialize the ranker\n        ranker = TimeWeightedDocumentRanker()\n\n        # Example input data\n        input_data = {\n            \"query\": \"example query\",\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        # Execute the ranker\n        output = ranker.execute(input_data)\n\n        # Output will be a dictionary with ranked documents\n        print(output)\n    \"\"\"\n\n    group: Literal[NodeGroup.RANKERS] = NodeGroup.RANKERS\n    name: str = \"Time Weighted Document Ranker\"\n    top_k: int = 5\n    max_days: int = 3600\n    min_coefficient: float = 0.9\n    date_field: str = \"date\"\n    date_format: str = \"%d %B, %Y\"\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document ranking process.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing documents and query.\n            config (RunnableConfig, optional): Configuration for the runnable.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n        Example:\n\n            input_data = {\n                \"documents\": [\n                    Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                    Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n                ]\n            }\n\n            output = ranker.execute(input_data)\n\n            # output will be a dictionary with ranked documents\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n\n        ranked_documents = self.adjust_similarity_scores(\n            documents,\n            date_field=self.date_field,\n            max_days=self.max_days,\n            min_coefficient=self.min_coefficient,\n            date_format=self.date_format,\n        )\n\n        return {\n            \"documents\": ranked_documents,\n        }\n\n    @staticmethod\n    def date_to_days(date_string: str, date_format: str = \"%d %B, %Y\") -&gt; int:\n        \"\"\"\n        Convert a date string to the number of days since that date.\n\n        Args:\n            date_string (str): Date in the format \"dd Month, YYYY\"\n            date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n        Returns:\n            int: Number of days since the given date.\n\n        Example:\n\n            days = TimeWeightedDocumentRanker.date_to_days(\"01 January, 2022\")\n\n            # days will be the number of days since 01 January, 2022\n        \"\"\"\n        date_object = datetime.strptime(date_string, date_format)\n        current_date = datetime.now()\n        return (current_date - date_object).days\n\n    @staticmethod\n    def days_to_coefficient(\n        days: int, max_days: int = 3600, min_coefficient: float = 0.1\n    ) -&gt; float:\n        \"\"\"\n        Transform number of days into a coefficient for score adjustment.\n\n        The coefficient is calculated based on the number of days since the content was created.\n\n        The function uses an exponential decay formula to ensure that the coefficient decreases\n        as the number of days increases, but never goes below the specified minimum coefficient.\n\n        The formula used is:\n            coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)\n\n        This ensures that:\n            - If days &lt;= 0, the coefficient is 1.0 (no decay).\n            - If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).\n            - For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.\n\n        Args:\n            days (int): Number of days since the content was created.\n            max_days (int): Maximum number of days to consider (default: 3600, about 12 years).\n            min_coefficient (float): Minimum coefficient value (default: 0.1).\n\n        Returns:\n            float: Coefficient between min_coefficient and 1.\n\n        Example:\n\n            coefficient = TimeWeightedDocumentRanker.days_to_coefficient(365)\n\n            # coefficient will be a value between 0.1 and 1 based on the number of days\n        \"\"\"\n        if days &lt;= 0:\n            return 1.0\n        elif days &gt;= max_days:\n            return min_coefficient\n        else:\n            return min_coefficient + (1 - min_coefficient) * math.exp(\n                -3 * days / max_days\n            )\n\n    @staticmethod\n    def adjust_similarity_scores(\n        candidates: list[Document],\n        date_field: str = \"date\",\n        max_days: int = 3600,\n        min_coefficient: float = 0.9,\n        date_format: str = \"%d %B, %Y\",\n    ) -&gt; list[Document]:\n        \"\"\"\n        Adjust cosine similarity scores based on content recency.\n\n        Args:\n            candidates (list[Document]): List of Document objects containing candidates with 'score' and date fields.\n            date_field (str): Name of the field containing the date string (default: 'date').\n            max_days (int): Maximum number of days to consider for adjustment.\n            min_coefficient (float): Minimum coefficient for score adjustment.\n            date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n        Returns:\n            list[Document]: List of candidates with adjusted scores, sorted by the new scores.\n\n        Example:\n\n            candidates = [\n                Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2021\"})\n            ]\n\n            adjusted_candidates = TimeWeightedDocumentRanker.adjust_similarity_scores(candidates)\n\n            # adjusted_candidates will be sorted by adjusted scores\n        \"\"\"\n        for candidate in candidates:\n            days = TimeWeightedDocumentRanker.date_to_days(\n                candidate.metadata[date_field],\n                date_format=date_format,\n            )\n            coefficient = TimeWeightedDocumentRanker.days_to_coefficient(\n                days, max_days=max_days, min_coefficient=min_coefficient\n            )\n            candidate.score = candidate.score * coefficient\n\n        documents = [\n            {\"score\": candidate.score, \"document\": candidate}\n            for candidate in candidates\n        ]\n\n        sorted_documents = sorted(documents, key=lambda x: x[\"score\"], reverse=True)\n\n        return [document[\"document\"] for document in sorted_documents]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.adjust_similarity_scores","title":"<code>adjust_similarity_scores(candidates, date_field='date', max_days=3600, min_coefficient=0.9, date_format='%d %B, %Y')</code>  <code>staticmethod</code>","text":"<p>Adjust cosine similarity scores based on content recency.</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>list[Document]</code> <p>List of Document objects containing candidates with 'score' and date fields.</p> required <code>date_field</code> <code>str</code> <p>Name of the field containing the date string (default: 'date').</p> <code>'date'</code> <code>max_days</code> <code>int</code> <p>Maximum number of days to consider for adjustment.</p> <code>3600</code> <code>min_coefficient</code> <code>float</code> <p>Minimum coefficient for score adjustment.</p> <code>0.9</code> <code>date_format</code> <code>str</code> <p>The format of the date string (default: \"%d %B, %Y\").</p> <code>'%d %B, %Y'</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of candidates with adjusted scores, sorted by the new scores.</p> <p>Example:</p> <pre><code>candidates = [\n    Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2022\"}),\n    Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2021\"})\n]\n\nadjusted_candidates = TimeWeightedDocumentRanker.adjust_similarity_scores(candidates)\n\n# adjusted_candidates will be sorted by adjusted scores\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>@staticmethod\ndef adjust_similarity_scores(\n    candidates: list[Document],\n    date_field: str = \"date\",\n    max_days: int = 3600,\n    min_coefficient: float = 0.9,\n    date_format: str = \"%d %B, %Y\",\n) -&gt; list[Document]:\n    \"\"\"\n    Adjust cosine similarity scores based on content recency.\n\n    Args:\n        candidates (list[Document]): List of Document objects containing candidates with 'score' and date fields.\n        date_field (str): Name of the field containing the date string (default: 'date').\n        max_days (int): Maximum number of days to consider for adjustment.\n        min_coefficient (float): Minimum coefficient for score adjustment.\n        date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n    Returns:\n        list[Document]: List of candidates with adjusted scores, sorted by the new scores.\n\n    Example:\n\n        candidates = [\n            Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2022\"}),\n            Document(content=\"Document content\", score=0.5, metadata={\"date\": \"01 January, 2021\"})\n        ]\n\n        adjusted_candidates = TimeWeightedDocumentRanker.adjust_similarity_scores(candidates)\n\n        # adjusted_candidates will be sorted by adjusted scores\n    \"\"\"\n    for candidate in candidates:\n        days = TimeWeightedDocumentRanker.date_to_days(\n            candidate.metadata[date_field],\n            date_format=date_format,\n        )\n        coefficient = TimeWeightedDocumentRanker.days_to_coefficient(\n            days, max_days=max_days, min_coefficient=min_coefficient\n        )\n        candidate.score = candidate.score * coefficient\n\n    documents = [\n        {\"score\": candidate.score, \"document\": candidate}\n        for candidate in candidates\n    ]\n\n    sorted_documents = sorted(documents, key=lambda x: x[\"score\"], reverse=True)\n\n    return [document[\"document\"] for document in sorted_documents]\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.date_to_days","title":"<code>date_to_days(date_string, date_format='%d %B, %Y')</code>  <code>staticmethod</code>","text":"<p>Convert a date string to the number of days since that date.</p> <p>Parameters:</p> Name Type Description Default <code>date_string</code> <code>str</code> <p>Date in the format \"dd Month, YYYY\"</p> required <code>date_format</code> <code>str</code> <p>The format of the date string (default: \"%d %B, %Y\").</p> <code>'%d %B, %Y'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of days since the given date.</p> <p>Example:</p> <pre><code>days = TimeWeightedDocumentRanker.date_to_days(\"01 January, 2022\")\n\n# days will be the number of days since 01 January, 2022\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>@staticmethod\ndef date_to_days(date_string: str, date_format: str = \"%d %B, %Y\") -&gt; int:\n    \"\"\"\n    Convert a date string to the number of days since that date.\n\n    Args:\n        date_string (str): Date in the format \"dd Month, YYYY\"\n        date_format (str): The format of the date string (default: \"%d %B, %Y\").\n\n    Returns:\n        int: Number of days since the given date.\n\n    Example:\n\n        days = TimeWeightedDocumentRanker.date_to_days(\"01 January, 2022\")\n\n        # days will be the number of days since 01 January, 2022\n    \"\"\"\n    date_object = datetime.strptime(date_string, date_format)\n    current_date = datetime.now()\n    return (current_date - date_object).days\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.days_to_coefficient","title":"<code>days_to_coefficient(days, max_days=3600, min_coefficient=0.1)</code>  <code>staticmethod</code>","text":"<p>Transform number of days into a coefficient for score adjustment.</p> <p>The coefficient is calculated based on the number of days since the content was created.</p> <p>The function uses an exponential decay formula to ensure that the coefficient decreases as the number of days increases, but never goes below the specified minimum coefficient.</p> The formula used is <p>coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)</p> This ensures that <ul> <li>If days &lt;= 0, the coefficient is 1.0 (no decay).</li> <li>If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).</li> <li>For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Number of days since the content was created.</p> required <code>max_days</code> <code>int</code> <p>Maximum number of days to consider (default: 3600, about 12 years).</p> <code>3600</code> <code>min_coefficient</code> <code>float</code> <p>Minimum coefficient value (default: 0.1).</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Coefficient between min_coefficient and 1.</p> <p>Example:</p> <pre><code>coefficient = TimeWeightedDocumentRanker.days_to_coefficient(365)\n\n# coefficient will be a value between 0.1 and 1 based on the number of days\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>@staticmethod\ndef days_to_coefficient(\n    days: int, max_days: int = 3600, min_coefficient: float = 0.1\n) -&gt; float:\n    \"\"\"\n    Transform number of days into a coefficient for score adjustment.\n\n    The coefficient is calculated based on the number of days since the content was created.\n\n    The function uses an exponential decay formula to ensure that the coefficient decreases\n    as the number of days increases, but never goes below the specified minimum coefficient.\n\n    The formula used is:\n        coefficient = min_coefficient + (1 - min_coefficient) * exp(-3 * days / max_days)\n\n    This ensures that:\n        - If days &lt;= 0, the coefficient is 1.0 (no decay).\n        - If days &gt;= max_days, the coefficient is min_coefficient (maximum decay).\n        - For days in between, the coefficient smoothly transitions from 1.0 to min_coefficient.\n\n    Args:\n        days (int): Number of days since the content was created.\n        max_days (int): Maximum number of days to consider (default: 3600, about 12 years).\n        min_coefficient (float): Minimum coefficient value (default: 0.1).\n\n    Returns:\n        float: Coefficient between min_coefficient and 1.\n\n    Example:\n\n        coefficient = TimeWeightedDocumentRanker.days_to_coefficient(365)\n\n        # coefficient will be a value between 0.1 and 1 based on the number of days\n    \"\"\"\n    if days &lt;= 0:\n        return 1.0\n    elif days &gt;= max_days:\n        return min_coefficient\n    else:\n        return min_coefficient + (1 - min_coefficient) * math.exp(\n            -3 * days / max_days\n        )\n</code></pre>"},{"location":"dynamiq/nodes/rankers/recency/#dynamiq.nodes.rankers.recency.TimeWeightedDocumentRanker.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document ranking process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing documents and query.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the original query and the ranked documents.</p> <p>Example:</p> <pre><code>input_data = {\n    \"documents\": [\n        Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n        Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n    ]\n}\n\noutput = ranker.execute(input_data)\n\n# output will be a dictionary with ranked documents\n</code></pre> Source code in <code>dynamiq/nodes/rankers/recency.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document ranking process.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing documents and query.\n        config (RunnableConfig, optional): Configuration for the runnable.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the original query and the ranked documents.\n\n    Example:\n\n        input_data = {\n            \"documents\": [\n                Document(content=\"Document content\", score=0.8, metadata={\"date\": \"01 January, 2022\"}),\n                Document(content=\"Document content\", score=0.9, metadata={\"date\": \"01 January, 2021\"})\n            ]\n        }\n\n        output = ranker.execute(input_data)\n\n        # output will be a dictionary with ranked documents\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n\n    ranked_documents = self.adjust_similarity_scores(\n        documents,\n        date_field=self.date_field,\n        max_days=self.max_days,\n        min_coefficient=self.min_coefficient,\n        date_format=self.date_format,\n    )\n\n    return {\n        \"documents\": ranked_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/","title":"Chroma","text":""},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever","title":"<code>ChromaDocumentRetriever</code>","text":"<p>               Bases: <code>VectorStoreNode</code></p> <p>Document Retriever using Chroma.</p> <p>This class implements a document retriever that uses Chroma as the underlying vector store. It extends the VectorStoreNode class and provides functionality to retrieve documents based on vector similarity.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>ChromaVectorStore | None</code> <p>The ChromaVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>ChromaDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>class ChromaDocumentRetriever(VectorStoreNode):\n    \"\"\"\n    Document Retriever using Chroma.\n\n    This class implements a document retriever that uses Chroma as the underlying vector store.\n    It extends the VectorStoreNode class and provides functionality to retrieve documents\n    based on vector similarity.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (ChromaVectorStore | None): The ChromaVectorStore instance.\n        filters (dict[str, Any] | None): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (ChromaDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    group: Literal[NodeGroup.RETRIEVERS] = NodeGroup.RETRIEVERS\n    name: str = \"ChromaDocumentRetriever\"\n    connection: Chroma | None = None\n    vector_store: ChromaVectorStore | None = None\n    filters: dict[str, Any] | None = None\n    top_k: int = 10\n    document_retriever: ChromaDocumentRetrieverComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ChromaDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Chroma()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return ChromaVectorStore\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_retriever\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the ChromaDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = ChromaDocumentRetrieverComponent(\n                vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method takes an input embedding, retrieves similar documents using the\n        document retriever component, and returns the retrieved documents.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data[\"embedding\"]\n        filters = input_data.get(\"filters\") or self.filters\n        top_k = input_data.get(\"top_k\") or self.top_k\n\n        output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ChromaDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ChromaDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Chroma()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes an input embedding, retrieves similar documents using the document retriever component, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method takes an input embedding, retrieves similar documents using the\n    document retriever component, and returns the retrieved documents.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data[\"embedding\"]\n    filters = input_data.get(\"filters\") or self.filters\n    top_k = input_data.get(\"top_k\") or self.top_k\n\n    output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/chroma/#dynamiq.nodes.retrievers.chroma.ChromaDocumentRetriever.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the ChromaDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/retrievers/chroma.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the ChromaDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = ChromaDocumentRetrieverComponent(\n            vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/","title":"Milvus","text":""},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever","title":"<code>MilvusDocumentRetriever</code>","text":"<p>               Bases: <code>VectorStoreNode</code></p> <p>Document Retriever using Milvus.</p> <p>This class implements a document retriever that uses Milvus as the underlying vector store. It extends the VectorStoreNode class and provides functionality to retrieve documents based on vector similarity.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>MilvusVectorStore | None</code> <p>The MilvusVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply when retrieving documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> <code>document_retriever</code> <code>MilvusDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>class MilvusDocumentRetriever(VectorStoreNode):\n    \"\"\"\n    Document Retriever using Milvus.\n\n    This class implements a document retriever that uses Milvus as the underlying vector store.\n    It extends the VectorStoreNode class and provides functionality to retrieve documents\n    based on vector similarity.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group the node belongs to.\n        name (str): The name of the node.\n        vector_store (MilvusVectorStore | None): The MilvusVectorStore instance.\n        filters (dict[str, Any] | None): Filters to apply when retrieving documents.\n        top_k (int): The maximum number of documents to retrieve.\n        document_retriever (MilvusDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n\n    group: Literal[NodeGroup.RETRIEVERS] = NodeGroup.RETRIEVERS\n    name: str = \"MilvusDocumentRetriever\"\n    connection: Milvus | None = None\n    vector_store: MilvusVectorStore | None = None\n    filters: dict[str, Any] | None = None\n    top_k: int = 10\n    document_retriever: MilvusDocumentRetrieverComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the MilvusDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Milvus connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments for initializing the node.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Milvus()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return MilvusVectorStore\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_retriever\": True}\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"\n        Initialize the components of the MilvusDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = MilvusDocumentRetrieverComponent(\n                vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n            )\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method takes an input embedding, retrieves similar documents using the\n        document retriever component, and returns the retrieved documents.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data[\"embedding\"]\n        filters = input_data.get(\"filters\") or self.filters\n        top_k = input_data.get(\"top_k\") or self.top_k\n\n        output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the MilvusDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Milvus connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments for initializing the node.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the MilvusDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Milvus connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments for initializing the node.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Milvus()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method takes an input embedding, retrieves similar documents using the document retriever component, and returns the retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method takes an input embedding, retrieves similar documents using the\n    document retriever component, and returns the retrieved documents.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data[\"embedding\"]\n    filters = input_data.get(\"filters\") or self.filters\n    top_k = input_data.get(\"top_k\") or self.top_k\n\n    output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/milvus/#dynamiq.nodes.retrievers.milvus.MilvusDocumentRetriever.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the MilvusDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/retrievers/milvus.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"\n    Initialize the components of the MilvusDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = MilvusDocumentRetrieverComponent(\n            vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever","title":"<code>PineconeDocumentRetriever</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>PineconeVectorStoreParams</code></p> <p>Document Retriever using Pinecone.</p> <p>This class implements a document retriever that uses Pinecone as the vector store backend.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>PineconeVectorStore | None</code> <p>The Pinecone vector store.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters to apply for retrieving specific documents.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>document_retriever</code> <code>PineconeDocumentRetriever</code> <p>The document retriever component.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>class PineconeDocumentRetriever(VectorStoreNode, PineconeVectorStoreParams):\n    \"\"\"Document Retriever using Pinecone.\n\n    This class implements a document retriever that uses Pinecone as the vector store backend.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group of the node.\n        name (str): The name of the node.\n        vector_store (PineconeVectorStore | None): The Pinecone vector store.\n        filters (dict[str, Any] | None): Filters to apply for retrieving specific documents.\n        top_k (int): The maximum number of documents to return.\n        document_retriever (PineconeDocumentRetrieverComponent): The document retriever component.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    group: Literal[NodeGroup.RETRIEVERS] = NodeGroup.RETRIEVERS\n    name: str = \"PineconeDocumentRetriever\"\n    connection: Pinecone | None = None\n    vector_store: PineconeVectorStore | None = None\n    filters: dict[str, Any] | None = None\n    top_k: int = 10\n    document_retriever: PineconeDocumentRetrieverComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the PineconeDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Pinecone connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Pinecone()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return PineconeVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(PineconeVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_retriever\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the PineconeDocumentRetriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = PineconeDocumentRetrieverComponent(\n                vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method retrieves documents based on the input embedding.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data[\"embedding\"]\n        filters = input_data.get(\"filters\") or self.filters\n        top_k = input_data.get(\"top_k\") or self.top_k\n\n        output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PineconeDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Pinecone connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the PineconeDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Pinecone connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Pinecone()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method retrieves documents based on the input embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method retrieves documents based on the input embedding.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data[\"embedding\"]\n    filters = input_data.get(\"filters\") or self.filters\n    top_k = input_data.get(\"top_k\") or self.top_k\n\n    output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/pinecone/#dynamiq.nodes.retrievers.pinecone.PineconeDocumentRetriever.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the PineconeDocumentRetriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/retrievers/pinecone.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the PineconeDocumentRetriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = PineconeDocumentRetrieverComponent(\n            vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever","title":"<code>QdrantDocumentRetriever</code>","text":"<p>               Bases: <code>VectorStoreNode</code></p> <p>Document Retriever using Qdrant.</p> <p>This class implements a document retriever that uses Qdrant as the vector store backend.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>QdrantVectorStore</code> <p>An instance of QdrantVectorStore to interface with Qdrant vectors.</p> required <code>filters</code> <code>dict[str, Any]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>QdrantVectorStore | None</code> <p>The QdrantVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters for document retrieval.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>document_retriever</code> <code>QdrantDocumentRetriever</code> <p>The document retriever component.</p> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>class QdrantDocumentRetriever(VectorStoreNode):\n    \"\"\"Document Retriever using Qdrant.\n\n    This class implements a document retriever that uses Qdrant as the vector store backend.\n\n    Args:\n        vector_store (QdrantVectorStore, optional): An instance of QdrantVectorStore to interface\n            with Qdrant vectors.\n        filters (dict[str, Any], optional): Filters to apply for retrieving specific documents.\n            Defaults to None.\n        top_k (int, optional): The maximum number of documents to return. Defaults to 10.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group of the node.\n        name (str): The name of the node.\n        vector_store (QdrantVectorStore | None): The QdrantVectorStore instance.\n        filters (dict[str, Any] | None): Filters for document retrieval.\n        top_k (int): The maximum number of documents to return.\n        document_retriever (QdrantDocumentRetrieverComponent): The document retriever component.\n    \"\"\"\n\n    group: Literal[NodeGroup.RETRIEVERS] = NodeGroup.RETRIEVERS\n    name: str = \"QdrantDocumentRetriever\"\n    connection: Qdrant | None = None\n    vector_store: QdrantVectorStore | None = None\n    filters: dict[str, Any] | None = None\n    top_k: int = 10\n    document_retriever: QdrantDocumentRetrieverComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the QdrantDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Qdrant connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the retriever.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Qdrant()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return QdrantVectorStore\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_retriever\": True}\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"\n        Initialize the components of the retriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = QdrantDocumentRetrieverComponent(\n                vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n            )\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method retrieves documents based on the input embedding.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data[\"embedding\"]\n        filters = input_data.get(\"filters\") or self.filters\n        top_k = input_data.get(\"top_k\") or self.top_k\n\n        output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the QdrantDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Qdrant connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the retriever.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the QdrantDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Qdrant connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the retriever.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Qdrant()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method retrieves documents based on the input embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method retrieves documents based on the input embedding.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data[\"embedding\"]\n    filters = input_data.get(\"filters\") or self.filters\n    top_k = input_data.get(\"top_k\") or self.top_k\n\n    output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/qdrant/#dynamiq.nodes.retrievers.qdrant.QdrantDocumentRetriever.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the retriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/retrievers/qdrant.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"\n    Initialize the components of the retriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = QdrantDocumentRetrieverComponent(\n            vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n        )\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever","title":"<code>WeaviateDocumentRetriever</code>","text":"<p>               Bases: <code>VectorStoreNode</code></p> <p>Document Retriever using Weaviate.</p> <p>This class implements a document retriever that uses Weaviate as the vector store backend.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>WeaviateVectorStore</code> <p>An instance of WeaviateVectorStore to interface with Weaviate vectors.</p> required <code>filters</code> <code>dict[str, Any]</code> <p>Filters to apply for retrieving specific documents. Defaults to None.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to return. Defaults to 10.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[RETRIEVERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>vector_store</code> <code>WeaviateVectorStore | None</code> <p>The WeaviateVectorStore instance.</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Filters for document retrieval.</p> <code>top_k</code> <code>int</code> <p>The maximum number of documents to return.</p> <code>document_retriever</code> <code>WeaviateDocumentRetriever</code> <p>The document retriever component.</p> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>class WeaviateDocumentRetriever(VectorStoreNode):\n    \"\"\"Document Retriever using Weaviate.\n\n    This class implements a document retriever that uses Weaviate as the vector store backend.\n\n    Args:\n        vector_store (WeaviateVectorStore, optional): An instance of WeaviateVectorStore to interface\n            with Weaviate vectors.\n        filters (dict[str, Any], optional): Filters to apply for retrieving specific documents.\n            Defaults to None.\n        top_k (int, optional): The maximum number of documents to return. Defaults to 10.\n\n    Attributes:\n        group (Literal[NodeGroup.RETRIEVERS]): The group of the node.\n        name (str): The name of the node.\n        vector_store (WeaviateVectorStore | None): The WeaviateVectorStore instance.\n        filters (dict[str, Any] | None): Filters for document retrieval.\n        top_k (int): The maximum number of documents to return.\n        document_retriever (WeaviateDocumentRetrieverComponent): The document retriever component.\n    \"\"\"\n\n    group: Literal[NodeGroup.RETRIEVERS] = NodeGroup.RETRIEVERS\n    name: str = \"WeaviateDocumentRetriever\"\n    connection: Weaviate | None = None\n    vector_store: WeaviateVectorStore | None = None\n    filters: dict[str, Any] | None = None\n    top_k: int = 10\n    document_retriever: WeaviateDocumentRetrieverComponent = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the WeaviateDocumentRetriever.\n\n        If neither vector_store nor connection is provided in kwargs, a default Weaviate connection will be created.\n\n        Args:\n            **kwargs: Keyword arguments to initialize the retriever.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Weaviate()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return WeaviateVectorStore\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_retriever\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ):\n        \"\"\"\n        Initialize the components of the retriever.\n\n        This method sets up the document retriever component if it hasn't been initialized yet.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to a new ConnectionManager instance.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_retriever is None:\n            self.document_retriever = WeaviateDocumentRetrieverComponent(\n                vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the document retrieval process.\n\n        This method retrieves documents based on the input embedding.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the query embedding.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the retrieved documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query_embedding = input_data[\"embedding\"]\n        filters = input_data.get(\"filters\") or self.filters\n        top_k = input_data.get(\"top_k\") or self.top_k\n\n        output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n        return {\n            \"documents\": output[\"documents\"],\n        }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WeaviateDocumentRetriever.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Weaviate connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to initialize the retriever.</p> <code>{}</code> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the WeaviateDocumentRetriever.\n\n    If neither vector_store nor connection is provided in kwargs, a default Weaviate connection will be created.\n\n    Args:\n        **kwargs: Keyword arguments to initialize the retriever.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Weaviate()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document retrieval process.</p> <p>This method retrieves documents based on the input embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the query embedding.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the retrieved documents.</p> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the document retrieval process.\n\n    This method retrieves documents based on the input embedding.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the query embedding.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the retrieved documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query_embedding = input_data[\"embedding\"]\n    filters = input_data.get(\"filters\") or self.filters\n    top_k = input_data.get(\"top_k\") or self.top_k\n\n    output = self.document_retriever.run(query_embedding, filters=filters, top_k=top_k)\n\n    return {\n        \"documents\": output[\"documents\"],\n    }\n</code></pre>"},{"location":"dynamiq/nodes/retrievers/weaviate/#dynamiq.nodes.retrievers.weaviate.WeaviateDocumentRetriever.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the retriever.</p> <p>This method sets up the document retriever component if it hasn't been initialized yet.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to a new ConnectionManager instance.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/retrievers/weaviate.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n):\n    \"\"\"\n    Initialize the components of the retriever.\n\n    This method sets up the document retriever component if it hasn't been initialized yet.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to a new ConnectionManager instance.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_retriever is None:\n        self.document_retriever = WeaviateDocumentRetrieverComponent(\n            vector_store=self.vector_store, filters=self.filters, top_k=self.top_k\n        )\n</code></pre>"},{"location":"dynamiq/nodes/splitters/document/","title":"Document","text":""},{"location":"dynamiq/nodes/splitters/document/#dynamiq.nodes.splitters.document.DocumentSplitter","title":"<code>DocumentSplitter</code>","text":"<p>               Bases: <code>Node</code></p> <p>Splits a list of text documents into a list of text documents with shorter texts.</p> <p>Splitting documents with long texts is a common preprocessing step during indexing. This allows Embedders to create significant semantic representations and avoids exceeding the maximum context length of language models.</p> <p>Parameters:</p> Name Type Description Default <code>split_by</code> <code>Literal['word', 'sentence', 'page', 'passage']</code> <p>Determines the unit by which the document should be split. Defaults to \"word\".</p> required <code>split_length</code> <code>int</code> <p>Maximum number of units (as defined by <code>split_by</code>) to include in each split. Defaults to 200.</p> required <code>split_overlap</code> <code>int</code> <p>Number of units that should overlap between consecutive splits. Defaults to 0.</p> required <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[SPLITTERS]</code> <p>The group of the node.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>split_by</code> <code>DocumentSplitBy</code> <p>The unit by which the document should be split.</p> <code>split_length</code> <code>int</code> <p>The maximum number of units to include in each split.</p> <code>split_overlap</code> <code>int</code> <p>The number of units that should overlap between consecutive splits.</p> <code>document_splitter</code> <code>DocumentSplitter</code> <p>The component used for document splitting.</p> Source code in <code>dynamiq/nodes/splitters/document.py</code> <pre><code>class DocumentSplitter(Node):\n    \"\"\"Splits a list of text documents into a list of text documents with shorter texts.\n\n    Splitting documents with long texts is a common preprocessing step during indexing.\n    This allows Embedders to create significant semantic representations\n    and avoids exceeding the maximum context length of language models.\n\n    Args:\n        split_by (Literal[\"word\", \"sentence\", \"page\", \"passage\"], optional): Determines the unit by\n            which the document should be split. Defaults to \"word\".\n        split_length (int, optional): Maximum number of units (as defined by `split_by`) to include\n            in each split. Defaults to 200.\n        split_overlap (int, optional): Number of units that should overlap between consecutive\n            splits. Defaults to 0.\n\n    Attributes:\n        group (Literal[NodeGroup.SPLITTERS]): The group of the node.\n        name (str): The name of the node.\n        split_by (DocumentSplitBy): The unit by which the document should be split.\n        split_length (int): The maximum number of units to include in each split.\n        split_overlap (int): The number of units that should overlap between consecutive splits.\n        document_splitter (DocumentSplitterComponent): The component used for document splitting.\n    \"\"\"\n\n    group: Literal[NodeGroup.SPLITTERS] = NodeGroup.SPLITTERS\n    name: str = \"DocumentSplitter\"\n    split_by: DocumentSplitBy = DocumentSplitBy.PASSAGE\n    split_length: int = 10\n    split_overlap: int = 0\n    document_splitter: DocumentSplitterComponent = None\n\n    @property\n    def to_dict_exclude_params(self):\n        return super().to_dict_exclude_params | {\"document_splitter\": True}\n\n    def init_components(\n        self, connection_manager: ConnectionManager = ConnectionManager()\n    ) -&gt; None:\n        \"\"\"Initializes the components of the DocumentSplitter.\n\n        Args:\n            connection_manager (ConnectionManager, optional): The connection manager to use.\n                Defaults to ConnectionManager().\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.document_splitter is None:\n            self.document_splitter = DocumentSplitterComponent(\n                split_by=self.split_by,\n                split_length=self.split_length,\n                split_overlap=self.split_overlap,\n            )\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Executes the document splitting process.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the documents to split.\n            config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the split documents under the key \"documents\".\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n        logger.debug(f\"Splitting {len(documents)} documents\")\n        output = self.document_splitter.run(documents=documents)\n\n        split_documents = output[\"documents\"]\n        logger.debug(\n            f\"Split {len(documents)} documents into {len(split_documents)} parts\"\n        )\n\n        return {\n            \"documents\": split_documents,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/splitters/document/#dynamiq.nodes.splitters.document.DocumentSplitter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the document splitting process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the documents to split.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the split documents under the key \"documents\".</p> Source code in <code>dynamiq/nodes/splitters/document.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Executes the document splitting process.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the documents to split.\n        config (RunnableConfig, optional): The configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the split documents under the key \"documents\".\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n    logger.debug(f\"Splitting {len(documents)} documents\")\n    output = self.document_splitter.run(documents=documents)\n\n    split_documents = output[\"documents\"]\n    logger.debug(\n        f\"Split {len(documents)} documents into {len(split_documents)} parts\"\n    )\n\n    return {\n        \"documents\": split_documents,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/splitters/document/#dynamiq.nodes.splitters.document.DocumentSplitter.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initializes the components of the DocumentSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use. Defaults to ConnectionManager().</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/splitters/document.py</code> <pre><code>def init_components(\n    self, connection_manager: ConnectionManager = ConnectionManager()\n) -&gt; None:\n    \"\"\"Initializes the components of the DocumentSplitter.\n\n    Args:\n        connection_manager (ConnectionManager, optional): The connection manager to use.\n            Defaults to ConnectionManager().\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.document_splitter is None:\n        self.document_splitter = DocumentSplitterComponent(\n            split_by=self.split_by,\n            split_length=self.split_length,\n            split_overlap=self.split_overlap,\n        )\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/","title":"E2b sandbox","text":""},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterInputSchema","title":"<code>E2BInterpreterInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>class E2BInterpreterInputSchema(BaseModel):\n    packages: str = Field(default=\"\", description=\"Parameter to provide packages to install.\")\n    shell_command: str = Field(default=\"\", description=\"Parameter to provide shell command to execute.\")\n    python: str = Field(default=\"\", description=\"Parameter to provide python code to execute.\")\n\n    files: list[FileData] = Field(\n        default=None,\n        description=\"Parameter to provide files for uploading to the sandbox.\",\n        is_accessible_to_agent=False,\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_execution_commands(self):\n        \"\"\"Validate that either shell command or python code is specified\"\"\"\n        if not self.shell_command and not self.python:\n            raise ValueError(\"shell_command or python code has to be specified.\")\n        return self\n\n    @field_validator(\"files\", mode=\"before\")\n    @classmethod\n    def files_validator(cls, files: list[bytes | io.BytesIO | FileData]) -&gt; list[FileData]:\n        return handle_file_upload(files)\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterInputSchema.validate_execution_commands","title":"<code>validate_execution_commands()</code>","text":"<p>Validate that either shell command or python code is specified</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_execution_commands(self):\n    \"\"\"Validate that either shell command or python code is specified\"\"\"\n    if not self.shell_command and not self.python:\n        raise ValueError(\"shell_command or python code has to be specified.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool","title":"<code>E2BInterpreterTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for executing code and managing files in an E2B sandbox environment.</p> <p>This tool provides a secure execution environment for running Python code, shell commands, and managing file operations.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The node group identifier.</p> <code>name</code> <code>str</code> <p>The unique name of the tool.</p> <code>description</code> <code>str</code> <p>Detailed usage instructions and capabilities.</p> <code>connection</code> <code>E2B</code> <p>Configuration for E2B connection.</p> <code>installed_packages</code> <code>List[str]</code> <p>Pre-installed packages in the sandbox.</p> <code>files</code> <code>Optional[List[Union[BytesIO, bytes]]]</code> <p>Files to be uploaded.</p> <code>persistent_sandbox</code> <code>bool</code> <p>Whether to maintain sandbox between executions.</p> <code>is_files_allowed</code> <code>bool</code> <p>Whether file uploads are permitted.</p> <code>_sandbox</code> <code>Optional[Sandbox]</code> <p>Internal sandbox instance for persistent mode.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>class E2BInterpreterTool(ConnectionNode):\n    \"\"\"\n    A tool for executing code and managing files in an E2B sandbox environment.\n\n    This tool provides a secure execution environment for running Python code,\n    shell commands, and managing file operations.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The node group identifier.\n        name (str): The unique name of the tool.\n        description (str): Detailed usage instructions and capabilities.\n        connection (E2BConnection): Configuration for E2B connection.\n        installed_packages (List[str]): Pre-installed packages in the sandbox.\n        files (Optional[List[Union[io.BytesIO, bytes]]]): Files to be uploaded.\n        persistent_sandbox (bool): Whether to maintain sandbox between executions.\n        is_files_allowed (bool): Whether file uploads are permitted.\n        _sandbox (Optional[Sandbox]): Internal sandbox instance for persistent mode.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"E2b Code Interpreter Tool\"\n    description: str = DESCRIPTION\n    connection: E2BConnection\n    installed_packages: list = []\n    files: list[FileData] | None = None\n    persistent_sandbox: bool = True\n    _sandbox: Sandbox | None = None\n    is_files_allowed: bool = True\n    input_schema: ClassVar[type[E2BInterpreterInputSchema]] = E2BInterpreterInputSchema\n\n    @field_validator(\"files\", mode=\"before\")\n    @classmethod\n    def files_validator(cls, files: list[bytes | io.BytesIO | FileData]) -&gt; list[FileData]:\n        return handle_file_upload(files)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if self.persistent_sandbox:\n            self._initialize_persistent_sandbox()\n        else:\n            logger.debug(f\"Tool {self.name} - {self.id}: Will initialize sandbox on each execute\")\n\n    @property\n    def to_dict_exclude_params(self) -&gt; set:\n        \"\"\"\n        Get parameters to exclude from dictionary representation.\n\n        Returns:\n            set: Set of parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"files\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert instance to dictionary format.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Dict[str, Any]: Dictionary representation of the instance.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        if self.files:\n            data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n        return data\n\n    def _initialize_persistent_sandbox(self):\n        \"\"\"Initializes the persistent sandbox, installs packages, and uploads initial files.\"\"\"\n        logger.info(f\"Tool {self.name} - {self.id}: Initializing Persistent Sandbox\")\n        self._sandbox = Sandbox(api_key=self.connection.api_key)\n        self._install_default_packages(self._sandbox)\n        if self.files:\n            self._upload_files(files=self.files, sandbox=self._sandbox)\n\n    def _install_default_packages(self, sandbox: Sandbox) -&gt; None:\n        \"\"\"Installs the default packages in the specified sandbox.\"\"\"\n        if self.installed_packages:\n            for package in self.installed_packages:\n                self._install_packages(sandbox, package)\n\n    def _install_packages(self, sandbox: Sandbox, packages: str) -&gt; None:\n        \"\"\"Installs the specified packages in the given sandbox.\"\"\"\n        if packages:\n            logger.debug(f\"Tool {self.name} - {self.id}: Installing packages: {packages}\")\n            sandbox.process.start_and_wait(f\"pip install -qq {' '.join(packages.split(','))}\")\n\n    def _upload_files(self, files: list[FileData], sandbox: Sandbox) -&gt; str:\n        \"\"\"Uploads multiple files to the sandbox and returns details for each file.\"\"\"\n        upload_details = []\n        for file in files:\n            uploaded_path = self._upload_file(file, sandbox)\n            upload_details.append(\n                {\n                    \"original_name\": file.name,\n                    \"description\": file.description,\n                    \"uploaded_path\": uploaded_path,\n                }\n            )\n            logger.debug(f\"Tool {self.name} - {self.id}: Uploaded file '{file.name}' to {uploaded_path}\")\n\n        self._update_description_with_files(upload_details)\n        return \"\\n\".join([f\"{file['original_name']} -&gt; {file['uploaded_path']}\" for file in upload_details])\n\n    def _upload_file(self, file: FileData, sandbox: Sandbox | None = None) -&gt; str:\n        \"\"\"Uploads a single file to the specified sandbox and returns the uploaded path.\"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for file upload.\")\n\n        # Handle the file types (bytes or io.BytesIO)\n        file_like_object = io.BytesIO(file.data)\n        file_like_object.name = file.name\n\n        # Upload the file to the sandbox\n        uploaded_path = sandbox.upload_file(file=file_like_object)\n        logger.debug(f\"Tool {self.name} - {self.id}: Uploaded file to {uploaded_path}\")\n\n        return uploaded_path\n\n    def _update_description_with_files(self, upload_details: list[dict]) -&gt; None:\n        \"\"\"Updates the tool description with detailed information about the uploaded files.\"\"\"\n        if upload_details:\n            self.description = self.description.strip().replace(\"&lt;/tool_description&gt;\", \"\")\n            self.description += \"\\n\\n**Uploaded Files Details:**\"\n            for file_info in upload_details:\n                self.description += (\n                    f\"\\n- **Original File Name**: {file_info['original_name']}\\n\"\n                    f\"  **Description**: {file_info['description']}\\n\"\n                    f\"  **Uploaded Path**: {file_info['uploaded_path']}\\n\"\n                )\n            self.description += \"\\n&lt;/tool_description&gt;\"\n\n    def _execute_python_code(self, code: str, sandbox: Sandbox | None = None) -&gt; str:\n        \"\"\"Executes Python code in the specified sandbox.\"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for code execution.\")\n        code_hash = sha256(code.encode()).hexdigest()\n        filename = f\"/home/user/{code_hash}.py\"\n        sandbox.filesystem.write(filename, code)\n        process = sandbox.process.start_and_wait(f\"python3 {filename}\")\n        if not (process.stdout or process.stderr):\n            raise ToolExecutionException(\n                \"Error: No output. Please use 'print()' to display the result of your Python code.\",\n                recoverable=True,\n            )\n        if \"Error\" in process.stderr:\n            raise ToolExecutionException(f\"Error during Python code execution: {process.stderr}\", recoverable=True)\n        return process.stdout\n\n    def _execute_shell_command(self, command: str, sandbox: Sandbox | None = None) -&gt; str:\n        \"\"\"Executes a shell command in the specified sandbox.\"\"\"\n        if not sandbox:\n            raise ValueError(\"Sandbox instance is required for command execution.\")\n\n        process = sandbox.process.start(command)\n        output = process.wait()\n        if process.exit_code != 0:\n            raise ToolExecutionException(f\"Error during shell command execution: {output.stderr}\", recoverable=True)\n        return output.stdout\n\n    def execute(\n        self, input_data: E2BInterpreterInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Executes the requested action based on the input data.\"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        if self.persistent_sandbox:\n            sandbox = self._sandbox\n        else:\n            logger.info(f\"Tool {self.name} - {self.id}: Initializing Sandbox for this execution\")\n            sandbox = Sandbox(api_key=self.connection.api_key)\n            self._install_default_packages(sandbox)\n            if self.files:\n                self._upload_files(files=self.files, sandbox=sandbox)\n\n        try:\n            content = {}\n            if files := input_data.files:\n                content[\"files_installation\"] = self._upload_files(files=files, sandbox=sandbox)\n            if packages := input_data.packages:\n                self._install_packages(sandbox=sandbox, packages=packages)\n                content[\"packages_installation\"] = f\"Installed packages: {input_data.packages}\"\n            if shell_command := input_data.shell_command:\n                content[\"shell_command_execution\"] = self._execute_shell_command(shell_command, sandbox=sandbox)\n            if python := input_data.python:\n                content[\"code_execution\"] = self._execute_python_code(python, sandbox=sandbox)\n            if not (packages or files or shell_command or python):\n                raise ToolExecutionException(\n                    \"Error: Invalid input data. Please provide 'files' for file upload (bytes or BytesIO)\",\n                    recoverable=True,\n                )\n\n        finally:\n            if not self.persistent_sandbox:\n                logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n                sandbox.close()\n\n        if self.is_optimized_for_agents:\n            result = \"\"\n            if files_installation := content.get(\"files_installation\"):\n                result += \"&lt;Files installation&gt;\\n\" + files_installation + \"\\n&lt;/Files installation&gt;\"\n            if packages_installation := content.get(\"packages_installation\"):\n                result += \"&lt;Package installation&gt;\\n\" + packages_installation + \"\\n&lt;/Package installation&gt;\"\n            if shell_command_execution := content.get(\"shell_command_execution\"):\n                result += \"&lt;Shell command execution&gt;\\n\" + shell_command_execution + \"\\n&lt;/Shell command execution&gt;\"\n            if code_execution := content.get(\"code_execution\"):\n                result += \"&lt;Code execution&gt;\\n\" + code_execution + \"\\n&lt;/Code execution&gt;\"\n            content = result\n\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(content)[:50]}...\")\n        return {\"content\": content}\n\n    def close(self) -&gt; None:\n        \"\"\"Closes the persistent sandbox if it exists.\"\"\"\n        if self._sandbox and self.persistent_sandbox:\n            logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n            self._sandbox.close()\n            self._sandbox = None\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params: set</code>  <code>property</code>","text":"<p>Get parameters to exclude from dictionary representation.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>Set of parameters to exclude.</p>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.close","title":"<code>close()</code>","text":"<p>Closes the persistent sandbox if it exists.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the persistent sandbox if it exists.\"\"\"\n    if self._sandbox and self.persistent_sandbox:\n        logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n        self._sandbox.close()\n        self._sandbox = None\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the requested action based on the input data.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def execute(\n    self, input_data: E2BInterpreterInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Executes the requested action based on the input data.\"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    if self.persistent_sandbox:\n        sandbox = self._sandbox\n    else:\n        logger.info(f\"Tool {self.name} - {self.id}: Initializing Sandbox for this execution\")\n        sandbox = Sandbox(api_key=self.connection.api_key)\n        self._install_default_packages(sandbox)\n        if self.files:\n            self._upload_files(files=self.files, sandbox=sandbox)\n\n    try:\n        content = {}\n        if files := input_data.files:\n            content[\"files_installation\"] = self._upload_files(files=files, sandbox=sandbox)\n        if packages := input_data.packages:\n            self._install_packages(sandbox=sandbox, packages=packages)\n            content[\"packages_installation\"] = f\"Installed packages: {input_data.packages}\"\n        if shell_command := input_data.shell_command:\n            content[\"shell_command_execution\"] = self._execute_shell_command(shell_command, sandbox=sandbox)\n        if python := input_data.python:\n            content[\"code_execution\"] = self._execute_python_code(python, sandbox=sandbox)\n        if not (packages or files or shell_command or python):\n            raise ToolExecutionException(\n                \"Error: Invalid input data. Please provide 'files' for file upload (bytes or BytesIO)\",\n                recoverable=True,\n            )\n\n    finally:\n        if not self.persistent_sandbox:\n            logger.debug(f\"Tool {self.name} - {self.id}: Closing Sandbox\")\n            sandbox.close()\n\n    if self.is_optimized_for_agents:\n        result = \"\"\n        if files_installation := content.get(\"files_installation\"):\n            result += \"&lt;Files installation&gt;\\n\" + files_installation + \"\\n&lt;/Files installation&gt;\"\n        if packages_installation := content.get(\"packages_installation\"):\n            result += \"&lt;Package installation&gt;\\n\" + packages_installation + \"\\n&lt;/Package installation&gt;\"\n        if shell_command_execution := content.get(\"shell_command_execution\"):\n            result += \"&lt;Shell command execution&gt;\\n\" + shell_command_execution + \"\\n&lt;/Shell command execution&gt;\"\n        if code_execution := content.get(\"code_execution\"):\n            result += \"&lt;Code execution&gt;\\n\" + code_execution + \"\\n&lt;/Code execution&gt;\"\n        content = result\n\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(content)[:50]}...\")\n    return {\"content\": content}\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.E2BInterpreterTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert instance to dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation of the instance.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert instance to dictionary format.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Dict[str, Any]: Dictionary representation of the instance.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    if self.files:\n        data[\"files\"] = [{\"name\": getattr(f, \"name\", f\"file_{i}\")} for i, f in enumerate(self.files)]\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.generate_fallback_filename","title":"<code>generate_fallback_filename(file)</code>","text":"<p>Generate a unique fallback filename for uploaded files.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes | BytesIO</code> <p>File content as bytes or BytesIO object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique filename based on the object's id.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def generate_fallback_filename(file: bytes | io.BytesIO) -&gt; str:\n    \"\"\"\n    Generate a unique fallback filename for uploaded files.\n\n    Args:\n        file: File content as bytes or BytesIO object.\n\n    Returns:\n        str: A unique filename based on the object's id.\n    \"\"\"\n    return f\"uploaded_file_{id(file)}.bin\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.generate_file_description","title":"<code>generate_file_description(file, length=20)</code>","text":"<p>Generate a description for a file based on its content.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>bytes | BytesIO</code> <p>File content as bytes or BytesIO object.</p> required <code>length</code> <code>int</code> <p>Maximum number of bytes to include in the description.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A description of the file's content or existing description.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def generate_file_description(file: bytes | io.BytesIO, length: int = 20) -&gt; str:\n    \"\"\"\n    Generate a description for a file based on its content.\n\n    Args:\n        file: File content as bytes or BytesIO object.\n        length: Maximum number of bytes to include in the description.\n\n    Returns:\n        str: A description of the file's content or existing description.\n    \"\"\"\n    if description := getattr(file, \"description\", None):\n        return description\n\n    file_content = file.getbuffer()[:length] if isinstance(file, io.BytesIO) else file[:length]\n    return f\"File starting with: {file_content.hex()}\"\n</code></pre>"},{"location":"dynamiq/nodes/tools/e2b_sandbox/#dynamiq.nodes.tools.e2b_sandbox.handle_file_upload","title":"<code>handle_file_upload(files)</code>","text":"<p>Handles file uploading with additional metadata.</p> Source code in <code>dynamiq/nodes/tools/e2b_sandbox.py</code> <pre><code>def handle_file_upload(files: list[bytes | io.BytesIO | FileData]) -&gt; list[FileData]:\n    \"\"\"Handles file uploading with additional metadata.\"\"\"\n    files_data = []\n    for file in files:\n        if isinstance(file, FileData):\n            files_data.append(file)\n        elif isinstance(file, bytes | io.BytesIO):\n            file_name = getattr(file, \"name\", generate_fallback_filename(file))\n            description = getattr(file, \"description\", generate_file_description(file))\n            files_data.append(\n                FileData(\n                    data=file.getvalue() if isinstance(file, io.BytesIO) else file,\n                    name=file_name,\n                    description=description,\n                )\n            )\n        else:\n            raise ValueError(f\"Error: Invalid file data type: {type(file)}. Expected bytes or BytesIO or FileData.\")\n\n    return files_data\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/","title":"Firecrawl","text":""},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.ExtractorOptions","title":"<code>ExtractorOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options for configuring content extraction behavior.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class ExtractorOptions(BaseModel):\n    \"\"\"Options for configuring content extraction behavior.\"\"\"\n    mode: Literal[\n        \"markdown\",\n        \"llm-extraction\",\n        \"llm-extraction-from-raw-html\",\n        \"llm-extraction-from-markdown\",\n    ] = \"markdown\"\n    extraction_prompt: str | None = None\n    extraction_schema: dict | None = None\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.FirecrawlTool","title":"<code>FirecrawlTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for scraping web pages using the Firecrawl service.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class FirecrawlTool(ConnectionNode):\n    \"\"\"A tool for scraping web pages using the Firecrawl service.\"\"\"\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Firecrawl Tool\"\n    description: str = (\n        \"A tool for scraping web pages, powered by Firecrawl.\"\n        \"You can use this tool to scrape the content of a web page.\"\n    )\n    connection: Firecrawl\n    url: str | None = None\n    input_schema: ClassVar[type[FirecrawlInputSchema]] = FirecrawlInputSchema\n\n    # Default parameters\n    page_options: PageOptions = Field(\n        default_factory=PageOptions, description=\"The options for scraping the page\"\n    )\n    extractor_options: ExtractorOptions = Field(\n        default_factory=ExtractorOptions,\n        description=\"The options for extracting content via service LLM\",\n    )\n    timeout: int = 60000\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @staticmethod\n    def to_camel_case(snake_str: str) -&gt; str:\n        components = snake_str.split(\"_\")\n        return components[0] + \"\".join(x.title() for x in components[1:])\n\n    @staticmethod\n    def dict_to_camel_case(data: dict | list) -&gt; dict | list:\n        if isinstance(data, dict):\n            return {\n                FirecrawlTool.to_camel_case(key): (\n                    FirecrawlTool.dict_to_camel_case(value)\n                    if isinstance(value, (dict, list))\n                    else value\n                )\n                for key, value in data.items()\n            }\n        elif isinstance(data, list):\n            return [FirecrawlTool.dict_to_camel_case(item) for item in data]\n        else:\n            return data\n\n    def execute(\n        self, input_data: FirecrawlInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"Execute the scraping tool with the provided input data.\"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        url = input_data.url or self.url\n        if not url:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get input data.\")\n            raise ValueError(\"URL is required for scraping\")\n\n        scrape_data = {\n            \"url\": url,\n            \"pageOptions\": self.dict_to_camel_case(\n                self.page_options.model_dump(exclude_none=True)\n            ),\n            \"extractorOptions\": self.dict_to_camel_case(\n                self.extractor_options.model_dump(exclude_none=True)\n            ),\n            \"timeout\": self.timeout,\n        }\n\n        connection_url = self.connection.url + \"scrape/\"\n        logger.debug(\n            f\"Tool {self.name} - {self.id}: sending request to {connection_url}\"\n        )\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                json=scrape_data,\n                headers=self.connection.headers,\n            )\n            response.raise_for_status()\n            scrape_result = response.json()\n        except Exception as e:\n            logger.error(\n                f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\"\n            )\n            raise\n\n        logger.debug(f\"Tool {self.name} - {self.id}: scrape result {scrape_result}\")\n\n        if self.is_optimized_for_agents:\n            result = f\"&lt;Source URL&gt;\\n{url}\\n&lt;\\\\Source URL&gt;\"\n            if scrape_result.get(\"data\", {}).get(\"content\", \"\") != \"\":\n                result += f\"\\n\\n&lt;Scraped result&gt;\\n{scrape_result.get('data', {}).get('content')}\\n&lt;\\\\Scraped result&gt;\"\n            if scrape_result.get(\"data\", {}).get(\"markdown\", \"\") != \"\":\n                result += f\"\\n\\n&lt;Markdown&gt;\\n{scrape_result.get('data', {}).get('markdown')}\\n&lt;\\\\Markdown&gt;\"\n            if scrape_result.get(\"data\", {}).get(\"llm_extraction\", \"\") != \"\":\n                result += (\n                    f\"\\n\\n&lt;LLM Extraction&gt;\\n{scrape_result.get('data', {}).get('llm_extraction')}\\n&lt;\\\\LLM Extraction&gt;\"\n                )\n        else:\n            result = {\n                \"success\": scrape_result.get(\"success\", False),\n                \"url\": url,\n                \"markdown\": scrape_result.get(\"data\", {}).get(\"markdown\", \"\"),\n                \"content\": scrape_result.get(\"data\", {}).get(\"content\", \"\"),\n                \"html\": scrape_result.get(\"data\", {}).get(\"html\"),\n                \"raw_html\": scrape_result.get(\"data\", {}).get(\"rawHtml\"),\n                \"metadata\": scrape_result.get(\"data\", {}).get(\"metadata\", {}),\n                \"llm_extraction\": scrape_result.get(\"data\", {}).get(\"llm_extraction\"),\n                \"warning\": scrape_result.get(\"data\", {}).get(\"warning\"),\n            }\n\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.FirecrawlTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the scraping tool with the provided input data.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>def execute(\n    self, input_data: FirecrawlInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"Execute the scraping tool with the provided input data.\"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    url = input_data.url or self.url\n    if not url:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get input data.\")\n        raise ValueError(\"URL is required for scraping\")\n\n    scrape_data = {\n        \"url\": url,\n        \"pageOptions\": self.dict_to_camel_case(\n            self.page_options.model_dump(exclude_none=True)\n        ),\n        \"extractorOptions\": self.dict_to_camel_case(\n            self.extractor_options.model_dump(exclude_none=True)\n        ),\n        \"timeout\": self.timeout,\n    }\n\n    connection_url = self.connection.url + \"scrape/\"\n    logger.debug(\n        f\"Tool {self.name} - {self.id}: sending request to {connection_url}\"\n    )\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            json=scrape_data,\n            headers=self.connection.headers,\n        )\n        response.raise_for_status()\n        scrape_result = response.json()\n    except Exception as e:\n        logger.error(\n            f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\"\n        )\n        raise\n\n    logger.debug(f\"Tool {self.name} - {self.id}: scrape result {scrape_result}\")\n\n    if self.is_optimized_for_agents:\n        result = f\"&lt;Source URL&gt;\\n{url}\\n&lt;\\\\Source URL&gt;\"\n        if scrape_result.get(\"data\", {}).get(\"content\", \"\") != \"\":\n            result += f\"\\n\\n&lt;Scraped result&gt;\\n{scrape_result.get('data', {}).get('content')}\\n&lt;\\\\Scraped result&gt;\"\n        if scrape_result.get(\"data\", {}).get(\"markdown\", \"\") != \"\":\n            result += f\"\\n\\n&lt;Markdown&gt;\\n{scrape_result.get('data', {}).get('markdown')}\\n&lt;\\\\Markdown&gt;\"\n        if scrape_result.get(\"data\", {}).get(\"llm_extraction\", \"\") != \"\":\n            result += (\n                f\"\\n\\n&lt;LLM Extraction&gt;\\n{scrape_result.get('data', {}).get('llm_extraction')}\\n&lt;\\\\LLM Extraction&gt;\"\n            )\n    else:\n        result = {\n            \"success\": scrape_result.get(\"success\", False),\n            \"url\": url,\n            \"markdown\": scrape_result.get(\"data\", {}).get(\"markdown\", \"\"),\n            \"content\": scrape_result.get(\"data\", {}).get(\"content\", \"\"),\n            \"html\": scrape_result.get(\"data\", {}).get(\"html\"),\n            \"raw_html\": scrape_result.get(\"data\", {}).get(\"rawHtml\"),\n            \"metadata\": scrape_result.get(\"data\", {}).get(\"metadata\", {}),\n            \"llm_extraction\": scrape_result.get(\"data\", {}).get(\"llm_extraction\"),\n            \"warning\": scrape_result.get(\"data\", {}).get(\"warning\"),\n        }\n\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/firecrawl/#dynamiq.nodes.tools.firecrawl.PageOptions","title":"<code>PageOptions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options for configuring page scraping behavior.</p> Source code in <code>dynamiq/nodes/tools/firecrawl.py</code> <pre><code>class PageOptions(BaseModel):\n    \"\"\"Options for configuring page scraping behavior.\"\"\"\n    headers: dict | None = None\n    include_html: bool = False\n    only_main_content: bool = False\n    remove_tags: list[str] | None = None\n    wait_for: int = 0\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/","title":"Function tool","text":""},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool","title":"<code>FunctionTool</code>","text":"<p>               Bases: <code>Node</code>, <code>Generic[T]</code></p> <p>A tool node for executing a specified function with the given input data.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>class FunctionTool(Node, Generic[T]):\n    \"\"\"\n    A tool node for executing a specified function with the given input data.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Function Tool\"\n    description: str = Field(\n        default=\"A tool for executing a function with given input.\"\n    )\n    error_handling: ErrorHandling = Field(\n        default_factory=lambda: ErrorHandling(timeout_seconds=600)\n    )\n\n    def run_func(self, **_: Any) -&gt; Any:\n        \"\"\"\n        Execute the function logic with provided arguments.\n\n        This method must be implemented by subclasses.\n\n        :param kwargs: Arguments to pass to the function.\n        :return: Result of the function execution.\n        \"\"\"\n        raise NotImplementedError(\"run_func must be implemented by subclasses\")\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the tool with the provided input data and configuration.\n\n        :param input_data: Dictionary of input data to be passed to the tool.\n        :param config: Optional configuration for the runnable instance.\n        :return: Dictionary with the execution result.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        result = self.run_func(input_data)\n\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {result}\")\n        return {\"content\": result}\n\n    def get_schema(self):\n        \"\"\"\n        Generate the schema for the input and output of the tool.\n\n        :return: Dictionary representing the input and output schema.\n        \"\"\"\n        cls = self.__class__\n        run_tool_method = self.run_func\n        if hasattr(cls, \"_original_func\"):\n            run_tool_method = cls._original_func\n\n        signature = inspect.signature(run_tool_method)\n        parameters = signature.parameters\n\n        fields = {}\n        for name, param in parameters.items():\n            if name == \"self\":\n                continue\n            annotation = (\n                param.annotation if param.annotation != inspect.Parameter.empty else Any\n            )\n            default = ... if param.default == inspect.Parameter.empty else param.default\n            fields[name] = (annotation, default)\n\n        input_model = create_model(f\"{cls.__name__}Input\", **fields)\n\n        return {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"input_schema\": input_model.schema(),\n            \"output_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\"content\": {\"type\": \"any\"}},\n            },\n        }\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the tool with the provided input data and configuration.</p> <p>:param input_data: Dictionary of input data to be passed to the tool. :param config: Optional configuration for the runnable instance. :return: Dictionary with the execution result.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the tool with the provided input data and configuration.\n\n    :param input_data: Dictionary of input data to be passed to the tool.\n    :param config: Optional configuration for the runnable instance.\n    :return: Dictionary with the execution result.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    result = self.run_func(input_data)\n\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {result}\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool.get_schema","title":"<code>get_schema()</code>","text":"<p>Generate the schema for the input and output of the tool.</p> <p>:return: Dictionary representing the input and output schema.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def get_schema(self):\n    \"\"\"\n    Generate the schema for the input and output of the tool.\n\n    :return: Dictionary representing the input and output schema.\n    \"\"\"\n    cls = self.__class__\n    run_tool_method = self.run_func\n    if hasattr(cls, \"_original_func\"):\n        run_tool_method = cls._original_func\n\n    signature = inspect.signature(run_tool_method)\n    parameters = signature.parameters\n\n    fields = {}\n    for name, param in parameters.items():\n        if name == \"self\":\n            continue\n        annotation = (\n            param.annotation if param.annotation != inspect.Parameter.empty else Any\n        )\n        default = ... if param.default == inspect.Parameter.empty else param.default\n        fields[name] = (annotation, default)\n\n    input_model = create_model(f\"{cls.__name__}Input\", **fields)\n\n    return {\n        \"name\": self.name,\n        \"description\": self.description,\n        \"input_schema\": input_model.schema(),\n        \"output_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\"content\": {\"type\": \"any\"}},\n        },\n    }\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.FunctionTool.run_func","title":"<code>run_func(**_)</code>","text":"<p>Execute the function logic with provided arguments.</p> <p>This method must be implemented by subclasses.</p> <p>:param kwargs: Arguments to pass to the function. :return: Result of the function execution.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def run_func(self, **_: Any) -&gt; Any:\n    \"\"\"\n    Execute the function logic with provided arguments.\n\n    This method must be implemented by subclasses.\n\n    :param kwargs: Arguments to pass to the function.\n    :return: Result of the function execution.\n    \"\"\"\n    raise NotImplementedError(\"run_func must be implemented by subclasses\")\n</code></pre>"},{"location":"dynamiq/nodes/tools/function_tool/#dynamiq.nodes.tools.function_tool.function_tool","title":"<code>function_tool(func)</code>","text":"<p>Decorator to convert a function into a FunctionTool subclass.</p> <p>:param func: Function to be converted into a tool. :return: A FunctionTool subclass that wraps the provided function.</p> Source code in <code>dynamiq/nodes/tools/function_tool.py</code> <pre><code>def function_tool(func: Callable[..., T]) -&gt; type[FunctionTool[T]]:\n    \"\"\"\n    Decorator to convert a function into a FunctionTool subclass.\n\n    :param func: Function to be converted into a tool.\n    :return: A FunctionTool subclass that wraps the provided function.\n    \"\"\"\n    def create_input_schema(func) -&gt; type[BaseModel]:\n        signature = inspect.signature(func)\n        params_dict = {param.name: (param.annotation, param.default) for param in signature.parameters.values()}\n        return create_model(\n            \"FunctionToolInputSchema\",\n            **{k: (v[0], ...) if v[1] is inspect.Parameter.empty else (v[0], v[1]) for k, v in params_dict.items()},\n        )\n\n    class FunctionToolFromDecorator(FunctionTool[T]):\n        name: str = Field(default=func.__name__)\n        description: str = Field(\n            default=func.__doc__\n            or f\"A tool for executing the {func.__name__} function.\"\n        )\n        _original_func = staticmethod(func)\n        input_schema: ClassVar[type[BaseModel]] = create_input_schema(func)\n\n        def run_func(self, input_data: BaseModel, **_) -&gt; T:\n            return func(**input_data.model_dump())\n\n    FunctionToolFromDecorator.__name__ = func.__name__\n    FunctionToolFromDecorator.__qualname__ = (\n        f\"FunctionToolFromDecorator({func.__name__})\"\n    )\n    FunctionToolFromDecorator.__module__ = func.__module__\n\n    return FunctionToolFromDecorator\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/","title":"Http api call","text":""},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.HttpApiCall","title":"<code>HttpApiCall</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A component for sending API requests using requests library.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group the node belongs to.</p> <code>connection</code> <code>Http | None</code> <p>The connection based on sending http requests.A new connection is created if none is provided.</p> <code>success_codes(list[int])</code> <code>Http | None</code> <p>The list of codes when request is successful.</p> <code>timeout</code> <code>float</code> <p>The timeout in seconds.</p> <code>data(dict[str,Any])</code> <code>float</code> <p>The data to send as body of request.</p> <code>headers(dict[str,Any])</code> <code>float</code> <p>The headers of request.</p> <code>params(dict[str,Any])</code> <code>float</code> <p>The additional query params of request.</p> <code>response_type(ResponseType|str)</code> <code>float</code> <p>The type of response content.</p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>class HttpApiCall(ConnectionNode):\n    \"\"\"\n    A component for sending API requests using requests library.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group the node belongs to.\n        connection (HttpConnection | None): The connection based on sending http requests.A new connection\n            is created if none is provided.\n        success_codes(list[int]): The list of codes when request is successful.\n        timeout (float): The timeout in seconds.\n        data(dict[str,Any]): The data to send as body of request.\n        headers(dict[str,Any]): The headers of request.\n        params(dict[str,Any]): The additional query params of request.\n        response_type(ResponseType|str): The type of response content.\n    \"\"\"\n\n    name: str = \"Api Call Tool\"\n    description: str = \"The description of the API call tool\"\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    connection: HttpConnection\n    success_codes: list[int] = [200]\n    timeout: float = 30\n    data: dict[str, Any] = Field(default_factory=dict)\n    headers: dict[str, Any] = Field(default_factory=dict)\n    params: dict[str, Any] = Field(default_factory=dict)\n    url: str = \"\"\n    response_type: ResponseType | str | None = ResponseType.RAW\n    input_schema: ClassVar[type[HttpApiCallInputSchema]] = HttpApiCallInputSchema\n\n    def execute(self, input_data: HttpApiCallInputSchema, config: RunnableConfig = None, **kwargs):\n        \"\"\"Execute the API call.\n\n        This method takes input data and returns content of API call response.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing(optionally) data, headers,\n                params for request.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n             dict: A dictionary with the following keys:\n                - \"content\" (bytes|string|dict[str,Any]): Value containing the result of request.\n                - \"status_code\" (int): The status code of the request.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        data = input_data.data\n\n        url = input_data.url or self.url or self.connection.url\n        if not url:\n            raise ValueError(\"No url provided.\")\n        headers = input_data.headers\n        params = input_data.params\n\n        response = self.client.request(\n            method=self.connection.method,\n            url=url,\n            headers=self.connection.headers | self.headers | headers,\n            params=self.connection.params | self.params | params,\n            data=self.connection.data | self.data | data,\n            timeout=self.timeout,\n        )\n\n        if response.status_code not in self.success_codes:\n            raise ToolExecutionException(\n                f\"Request failed with unexpected status code: {response.status_code} and response: {response.text}\"\n            )\n\n        response_type = self.response_type\n        if (\n            \"response_type\" not in self.model_fields_set\n            and response.headers.get(\"content-type\") == \"application/json\"\n        ):\n            response_type = ResponseType.JSON\n\n        if response_type == ResponseType.TEXT:\n            content = response.text\n        elif response_type == ResponseType.RAW:\n            content = response.content\n        elif response_type == ResponseType.JSON:\n            content = response.json()\n        else:\n            allowed_types = [item.value for item in ResponseType]\n            raise ValueError(\n                f\"Response type must be one of the following: {', '.join(allowed_types)}\"\n            )\n        return {\"content\": content, \"status_code\": response.status_code}\n</code></pre>"},{"location":"dynamiq/nodes/tools/http_api_call/#dynamiq.nodes.tools.http_api_call.HttpApiCall.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the API call.</p> <p>This method takes input data and returns content of API call response.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing(optionally) data, headers, params for request.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the following keys: - \"content\" (bytes|string|dict[str,Any]): Value containing the result of request. - \"status_code\" (int): The status code of the request.</p> Source code in <code>dynamiq/nodes/tools/http_api_call.py</code> <pre><code>def execute(self, input_data: HttpApiCallInputSchema, config: RunnableConfig = None, **kwargs):\n    \"\"\"Execute the API call.\n\n    This method takes input data and returns content of API call response.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing(optionally) data, headers,\n            params for request.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n         dict: A dictionary with the following keys:\n            - \"content\" (bytes|string|dict[str,Any]): Value containing the result of request.\n            - \"status_code\" (int): The status code of the request.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    data = input_data.data\n\n    url = input_data.url or self.url or self.connection.url\n    if not url:\n        raise ValueError(\"No url provided.\")\n    headers = input_data.headers\n    params = input_data.params\n\n    response = self.client.request(\n        method=self.connection.method,\n        url=url,\n        headers=self.connection.headers | self.headers | headers,\n        params=self.connection.params | self.params | params,\n        data=self.connection.data | self.data | data,\n        timeout=self.timeout,\n    )\n\n    if response.status_code not in self.success_codes:\n        raise ToolExecutionException(\n            f\"Request failed with unexpected status code: {response.status_code} and response: {response.text}\"\n        )\n\n    response_type = self.response_type\n    if (\n        \"response_type\" not in self.model_fields_set\n        and response.headers.get(\"content-type\") == \"application/json\"\n    ):\n        response_type = ResponseType.JSON\n\n    if response_type == ResponseType.TEXT:\n        content = response.text\n    elif response_type == ResponseType.RAW:\n        content = response.content\n    elif response_type == ResponseType.JSON:\n        content = response.json()\n    else:\n        allowed_types = [item.value for item in ResponseType]\n        raise ValueError(\n            f\"Response type must be one of the following: {', '.join(allowed_types)}\"\n        )\n    return {\"content\": content, \"status_code\": response.status_code}\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/","title":"Human feedback","text":""},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool","title":"<code>HumanFeedbackTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for gathering user information through human feedback.</p> <p>This tool prompts the user for input and returns the response. It should be used to check actual information from the user or to gather additional input during a process.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool's purpose.</p> <code>input_method</code> <code>InputMethod</code> <p>The method used to gather user input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class HumanFeedbackTool(Node):\n    \"\"\"\n    A tool for gathering user information through human feedback.\n\n    This tool prompts the user for input and returns the response. It should be used to check actual\n    information from the user or to gather additional input during a process.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group the node belongs to.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool's purpose.\n        input_method (InputMethod): The method used to gather user input.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Human Feedback Tool\"\n    description: str = \"Tool to gather user information. Use it to check actual information or get additional input.\"\n    input_method: InputMethod | InputMethodCallable = InputMethod.console\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[HumanFeedbackInputSchema]] = HumanFeedbackInputSchema\n\n    def input_method_console(self, prompt: str) -&gt; str:\n        \"\"\"\n        Get input from the user using the console input method.\n\n        Args:\n            prompt (str): The prompt to display to the user.\n\n        Returns:\n            str: The user's input.\n        \"\"\"\n        return input(prompt)\n\n    def input_method_streaming(self, prompt: str, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"\n        Get input from the user using the queue streaming input method.\n\n        Args:\n            prompt (str): The prompt to display to the user.\n            config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n\n        Returns:\n            str: The user's input.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with prompt {prompt}\")\n        event = HFStreamingOutputEventMessage(\n            wf_run_id=config.run_id,\n            entity_id=self.id,\n            data=HFStreamingOutputEventMessageData(prompt=prompt),\n            event=self.streaming.event,\n        )\n        logger.debug(f\"Tool {self.name} - {self.id}: sending output event {event}\")\n        self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n        event = self.get_input_streaming_event(\n            event_msg_type=HFStreamingInputEventMessage,\n            event=self.streaming.event,\n            config=config,\n        )\n        logger.debug(f\"Tool {self.name} - {self.id}: received input event {event}\")\n\n        return event.data.content\n\n    def execute(\n        self, input_data: HumanFeedbackInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the tool with the provided input data and configuration.\n\n        This method prompts the user for input using the specified input method and returns the result.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the prompt for the user.\n            config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n            **kwargs: Additional keyword arguments to be passed to the node execute run.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the user's input under the 'content' key.\n\n        Raises:\n            ValueError: If the input_data does not contain an 'input' key.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        input_text = input_data.input\n        if isinstance(self.input_method, InputMethod):\n            if self.input_method == InputMethod.console:\n                result = self.input_method_console(input_text)\n            elif self.input_method == InputMethod.stream:\n                streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n                if not streaming.input_streaming_enabled:\n                    raise ValueError(\n                        f\"'{InputMethod.stream.value}' input method requires enabled input and output streaming.\"\n                    )\n\n                result = self.input_method_streaming(prompt=input_text, config=config, **kwargs)\n            else:\n                raise ValueError(f\"Unsupported input method: {self.input_method}\")\n        else:\n            result = self.input_method.get_input(input_text)\n\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {result}\")\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the tool with the provided input data and configuration.</p> <p>This method prompts the user for input using the specified input method and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the prompt for the user.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the runnable. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to the node execute run.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the user's input under the 'content' key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input_data does not contain an 'input' key.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def execute(\n    self, input_data: HumanFeedbackInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the tool with the provided input data and configuration.\n\n    This method prompts the user for input using the specified input method and returns the result.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the prompt for the user.\n        config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to the node execute run.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the user's input under the 'content' key.\n\n    Raises:\n        ValueError: If the input_data does not contain an 'input' key.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    input_text = input_data.input\n    if isinstance(self.input_method, InputMethod):\n        if self.input_method == InputMethod.console:\n            result = self.input_method_console(input_text)\n        elif self.input_method == InputMethod.stream:\n            streaming = getattr(config.nodes_override.get(self.id), \"streaming\", None) or self.streaming\n            if not streaming.input_streaming_enabled:\n                raise ValueError(\n                    f\"'{InputMethod.stream.value}' input method requires enabled input and output streaming.\"\n                )\n\n            result = self.input_method_streaming(prompt=input_text, config=config, **kwargs)\n        else:\n            raise ValueError(f\"Unsupported input method: {self.input_method}\")\n    else:\n        result = self.input_method.get_input(input_text)\n\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {result}\")\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.input_method_console","title":"<code>input_method_console(prompt)</code>","text":"<p>Get input from the user using the console input method.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to display to the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def input_method_console(self, prompt: str) -&gt; str:\n    \"\"\"\n    Get input from the user using the console input method.\n\n    Args:\n        prompt (str): The prompt to display to the user.\n\n    Returns:\n        str: The user's input.\n    \"\"\"\n    return input(prompt)\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.HumanFeedbackTool.input_method_streaming","title":"<code>input_method_streaming(prompt, config, **kwargs)</code>","text":"<p>Get input from the user using the queue streaming input method.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to display to the user.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for the runnable. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>def input_method_streaming(self, prompt: str, config: RunnableConfig, **kwargs) -&gt; str:\n    \"\"\"\n    Get input from the user using the queue streaming input method.\n\n    Args:\n        prompt (str): The prompt to display to the user.\n        config (RunnableConfig, optional): The configuration for the runnable. Defaults to None.\n\n    Returns:\n        str: The user's input.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with prompt {prompt}\")\n    event = HFStreamingOutputEventMessage(\n        wf_run_id=config.run_id,\n        entity_id=self.id,\n        data=HFStreamingOutputEventMessageData(prompt=prompt),\n        event=self.streaming.event,\n    )\n    logger.debug(f\"Tool {self.name} - {self.id}: sending output event {event}\")\n    self.run_on_node_execute_stream(callbacks=config.callbacks, event=event, **kwargs)\n    event = self.get_input_streaming_event(\n        event_msg_type=HFStreamingInputEventMessage,\n        event=self.streaming.event,\n        config=config,\n    )\n    logger.debug(f\"Tool {self.name} - {self.id}: received input event {event}\")\n\n    return event.data.content\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.InputMethodCallable","title":"<code>InputMethodCallable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for input methods.</p> <p>This class defines the interface for various input methods that can be used to gather user input in the HumanFeedbackTool.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>class InputMethodCallable(ABC):\n    \"\"\"\n    Abstract base class for input methods.\n\n    This class defines the interface for various input methods that can be used\n    to gather user input in the HumanFeedbackTool.\n    \"\"\"\n\n    @abstractmethod\n    def get_input(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"\n        Get input from the user.\n\n        Args:\n            prompt (str): The prompt to display to the user.\n\n        Returns:\n            str: The user's input.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/nodes/tools/human_feedback/#dynamiq.nodes.tools.human_feedback.InputMethodCallable.get_input","title":"<code>get_input(prompt, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Get input from the user.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to display to the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The user's input.</p> Source code in <code>dynamiq/nodes/tools/human_feedback.py</code> <pre><code>@abstractmethod\ndef get_input(self, prompt: str, **kwargs) -&gt; str:\n    \"\"\"\n    Get input from the user.\n\n    Args:\n        prompt (str): The prompt to display to the user.\n\n    Returns:\n        str: The user's input.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/","title":"Llm summarizer","text":""},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool","title":"<code>SummarizerTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>A tool for summarizing and cleaning up text extracted from HTML.</p> <p>This tool processes input text, typically extracted from HTML, by removing unnecessary content, cleaning up the remaining text, and formatting it into a coherent and well-organized summary.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group this node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A description of the tool's functionality.</p> <code>llm</code> <code>Node</code> <p>The language model node used for text processing.</p> <code>chunk_size</code> <code>int</code> <p>The maximum number of words in each chunk for processing.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Configuration for error handling.</p> <code>prompt_template</code> <code>str</code> <p>The prompt template used for text summarization.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>class SummarizerTool(Node):\n    \"\"\"\n    A tool for summarizing and cleaning up text extracted from HTML.\n\n    This tool processes input text, typically extracted from HTML, by removing unnecessary content,\n    cleaning up the remaining text, and formatting it into a coherent and well-organized summary.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group this node belongs to.\n        name (str): The name of the tool.\n        description (str): A description of the tool's functionality.\n        llm (Node): The language model node used for text processing.\n        chunk_size (int): The maximum number of words in each chunk for processing.\n        error_handling (ErrorHandling): Configuration for error handling.\n        prompt_template (str): The prompt template used for text summarization.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Summarizer Tool\"\n    description: str = (\n        \"A tool for summarizing and cleaning up text extracted from HTML. \"\n    )\n    llm: Node\n    chunk_size: int = Field(default=4000, description=\"The maximum number of words in each chunk\")\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    prompt_template: str = Field(\n        default=PROMPT_TEMPLATE_SUMMARIZER,\n        description=\"The prompt template for the summarizer\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[SummarizerInputSchema]] = SummarizerInputSchema\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()) -&gt; None:\n        \"\"\"\n        Initialize the components of the tool.\n\n        Args:\n            connection_manager (ConnectionManager): The connection manager to use for initialization.\n        \"\"\"\n        super().init_components(connection_manager)\n        if self.llm.is_postponed_component_init:\n            self.llm.init_components(connection_manager)\n\n    def reset_run_state(self):\n        \"\"\"\n        Reset the intermediate steps (run_depends) of the node.\n        \"\"\"\n        self._run_depends = []\n\n    @property\n    def to_dict_exclude_params(self) -&gt; dict:\n        \"\"\"\n        Property to define which parameters should be excluded when converting the class instance to a dictionary.\n\n        Returns:\n            dict: A dictionary defining the parameters to exclude.\n        \"\"\"\n        return super().to_dict_exclude_params | {\"llm\": True}\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"\n        Convert the tool to a dictionary representation.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary representation of the tool.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict(**kwargs)\n        return data\n\n    def _process_chunk(self, chunk: str, config: RunnableConfig, **kwargs) -&gt; str:\n        \"\"\"\n        Process a single chunk of text using the language model.\n\n        Args:\n            chunk (str): The text chunk to process.\n            config (RunnableConfig): The configuration for running the model.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The processed text chunk.\n\n        Raises:\n            ValueError: If the language model execution fails.\n        \"\"\"\n        prompt = self.prompt_template.format(input=chunk)\n        result = self.llm.run(\n            input_data={},\n            prompt=Prompt(messages=[Message(role=\"user\", content=prompt)]),\n            config=config,\n            **(kwargs | {\"parent_run_id\": kwargs.get(\"run_id\")}),\n        )\n        self._run_depends = [NodeDependency(node=self.llm).to_dict()]\n        if result.status != RunnableStatus.SUCCESS:\n            raise ValueError(\"LLM execution failed\")\n        return result.output[\"content\"]\n\n    def execute(\n        self, input_data: SummarizerInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Execute the summarization tool on the input data.\n\n        This method processes the input text, potentially breaking it into chunks if it exceeds\n        the specified chunk size, and then summarizes each chunk using the language model.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input text under the 'input' key.\n            config (RunnableConfig, optional): The configuration for running the tool.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the summarized content under the 'content' key.\n\n        Raises:\n            ValueError: If the input_data does not contain an 'input' key.\n        \"\"\"\n        config = ensure_config(config)\n        self.reset_run_state()\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        input_text = input_data.input\n        logger.debug(\n            f\"Tool {self.name} - {self.id}: started with input text length: {len(input_text)}, \"\n            f\"word count: {len(input_text.split())}\"\n        )\n\n        words = input_text.split()\n        if len(words) &gt; self.chunk_size:\n            content_chunks = [\n                \" \".join(words[i : i + self.chunk_size])\n                for i in range(0, len(words), self.chunk_size)\n            ]\n            summaries = [self._process_chunk(chunk, config, **kwargs) for chunk in content_chunks]\n            summary = \"\\n\".join(summaries)\n        else:\n            summary = self._process_chunk(input_text, config, **kwargs)\n\n        logger.debug(\n            f\"Tool {self.name} - {self.id}: finished with result length: {len(summary)}, \"\n            f\"word count: {len(summary.split())}\"\n        )\n        return {\"content\": summary}\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.to_dict_exclude_params","title":"<code>to_dict_exclude_params: dict</code>  <code>property</code>","text":"<p>Property to define which parameters should be excluded when converting the class instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary defining the parameters to exclude.</p>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the summarization tool on the input data.</p> <p>This method processes the input text, potentially breaking it into chunks if it exceeds the specified chunk size, and then summarizes each chunk using the language model.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input text under the 'input' key.</p> required <code>config</code> <code>RunnableConfig</code> <p>The configuration for running the tool.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the summarized content under the 'content' key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input_data does not contain an 'input' key.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def execute(\n    self, input_data: SummarizerInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute the summarization tool on the input data.\n\n    This method processes the input text, potentially breaking it into chunks if it exceeds\n    the specified chunk size, and then summarizes each chunk using the language model.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input text under the 'input' key.\n        config (RunnableConfig, optional): The configuration for running the tool.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the summarized content under the 'content' key.\n\n    Raises:\n        ValueError: If the input_data does not contain an 'input' key.\n    \"\"\"\n    config = ensure_config(config)\n    self.reset_run_state()\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    input_text = input_data.input\n    logger.debug(\n        f\"Tool {self.name} - {self.id}: started with input text length: {len(input_text)}, \"\n        f\"word count: {len(input_text.split())}\"\n    )\n\n    words = input_text.split()\n    if len(words) &gt; self.chunk_size:\n        content_chunks = [\n            \" \".join(words[i : i + self.chunk_size])\n            for i in range(0, len(words), self.chunk_size)\n        ]\n        summaries = [self._process_chunk(chunk, config, **kwargs) for chunk in content_chunks]\n        summary = \"\\n\".join(summaries)\n    else:\n        summary = self._process_chunk(input_text, config, **kwargs)\n\n    logger.debug(\n        f\"Tool {self.name} - {self.id}: finished with result length: {len(summary)}, \"\n        f\"word count: {len(summary.split())}\"\n    )\n    return {\"content\": summary}\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize the components of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>The connection manager to use for initialization.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()) -&gt; None:\n    \"\"\"\n    Initialize the components of the tool.\n\n    Args:\n        connection_manager (ConnectionManager): The connection manager to use for initialization.\n    \"\"\"\n    super().init_components(connection_manager)\n    if self.llm.is_postponed_component_init:\n        self.llm.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.reset_run_state","title":"<code>reset_run_state()</code>","text":"<p>Reset the intermediate steps (run_depends) of the node.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def reset_run_state(self):\n    \"\"\"\n    Reset the intermediate steps (run_depends) of the node.\n    \"\"\"\n    self._run_depends = []\n</code></pre>"},{"location":"dynamiq/nodes/tools/llm_summarizer/#dynamiq.nodes.tools.llm_summarizer.SummarizerTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the tool to a dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the tool.</p> Source code in <code>dynamiq/nodes/tools/llm_summarizer.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the tool to a dictionary representation.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary representation of the tool.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data[\"llm\"] = self.llm.to_dict(**kwargs)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/python/","title":"Python","text":"<p>thon</p>"},{"location":"dynamiq/nodes/tools/retriever/","title":"Retriever","text":""},{"location":"dynamiq/nodes/tools/retriever/#dynamiq.nodes.tools.retriever.RetrievalTool","title":"<code>RetrievalTool</code>","text":"<p>               Bases: <code>Node</code></p> <p>Tool for retrieving relevant documents based on a query.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>Group for the node. Defaults to NodeGroup.TOOLS.</p> <code>name</code> <code>str</code> <p>Name of the tool. Defaults to \"Retrieval Tool\".</p> <code>description</code> <code>str</code> <p>Description of the tool.</p> <code>error_handling</code> <code>ErrorHandling</code> <p>Error handling configuration.</p> <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Connection manager.</p> <code>text_embedder</code> <code>ConnectionNode | None</code> <p>Text embedder node.</p> <code>document_retriever</code> <code>ConnectionNode | None</code> <p>Document retriever node.</p> Source code in <code>dynamiq/nodes/tools/retriever.py</code> <pre><code>class RetrievalTool(Node):\n    \"\"\"Tool for retrieving relevant documents based on a query.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): Group for the node. Defaults to NodeGroup.TOOLS.\n        name (str): Name of the tool. Defaults to \"Retrieval Tool\".\n        description (str): Description of the tool.\n        error_handling (ErrorHandling): Error handling configuration.\n        connection_manager (ConnectionManager | None): Connection manager.\n        text_embedder (ConnectionNode | None): Text embedder node.\n        document_retriever (ConnectionNode | None): Document retriever node.\n    \"\"\"\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Retrieval Tool\"\n    description: str = \"A tool for retrieving relevant documents based on a query.\"\n    error_handling: ErrorHandling = ErrorHandling(timeout_seconds=600)\n    connection_manager: ConnectionManager | None = None\n    text_embedder: ConnectionNode | None = None\n    document_retriever: ConnectionNode | None = None\n    input_schema: ClassVar[type[RetrievalInputSchema]] = RetrievalInputSchema\n\n    def __init__(\n        self,\n        connection_manager: ConnectionManager | None = None,\n        text_embedder: ConnectionNode | None = None,\n        document_retriever: Any | None = None,\n        **data,\n    ):\n        \"\"\"Initialize the RetrievalTool.\n\n        Args:\n            connection_manager (ConnectionManager | None): Connection manager.\n            text_embedder (ConnectionNode | None): Text embedder node.\n            document_retriever (Any | None): Document retriever node.\n            **data: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**data)\n        self.connection_manager = connection_manager\n        self.text_embedder = text_embedder\n        self.document_retriever = document_retriever\n\n    def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n        \"\"\"Initialize components with the connection manager.\n\n        Args:\n            connection_manager (ConnectionManager): Connection manager.\n        \"\"\"\n        super().init_components(connection_manager)\n        if (\n            hasattr(self.text_embedder, \"is_postponed_component_init\")\n            and self.text_embedder.is_postponed_component_init\n        ):\n            self.text_embedder.init_components(connection_manager)\n        if (\n            hasattr(self.document_retriever, \"is_postponed_component_init\")\n            and self.document_retriever.is_postponed_component_init\n        ):\n            self.document_retriever.init_components(connection_manager)\n\n    def format_content(self, documents: list[Document], metadata_fields: list[str] = [\"title\", \"url\"]) -&gt; str:\n        \"\"\"Format the retrieved documents' metadata and content.\n\n        Args:\n            documents (list[Document]): List of retrieved documents.\n            metadata_fields (list[str]): Metadata fields to include.\n\n        Returns:\n            str: Formatted content of the documents.\n        \"\"\"\n        formatted_docs = []\n        for i, doc in enumerate(documents):\n            metadata = doc.metadata\n            formatted_doc = f\"Source {i + 1}\\n\"\n            for field in metadata_fields:\n                if field in metadata:\n                    formatted_doc += f\"{field.capitalize()}: {metadata[field]}\\n\"\n            formatted_doc += f\"Content: {doc.content}\\n\"\n            formatted_docs.append(formatted_doc)\n        return \"\\n\\n\".join(formatted_docs)\n\n    def execute(self, input_data: RetrievalInputSchema, **_) -&gt; dict[str, Any]:\n        \"\"\"Execute the retrieval tool.\n\n        Args:\n            input_data (dict[str, Any]): Input data for the tool.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: Result of the retrieval.\n        \"\"\"\n\n        logger.debug(f\"Tool {self.name} - {self.id}: started with query '{input_data.model_dump()}'\")\n\n        if not self.text_embedder:\n            raise ValueError(f\"{self.name}: Text embedder is not initialized.\")\n        if not self.document_retriever:\n            raise ValueError(f\"{self.name}: Document retriever is not initialized.\")\n\n        try:\n            text_embedder_output = self.text_embedder.run(input_data={\"query\": input_data.query})\n            embedding = text_embedder_output.output.get(\"embedding\")\n\n            document_retriever_output = self.document_retriever.run(input_data={\"embedding\": embedding})\n            retrieved_documents = document_retriever_output.output.get(\"documents\", [])\n            logger.info(f\"Tool {self.name} - {self.id}: retrieved {len(retrieved_documents)} documents\")\n\n            formatted_content = self.format_content(retrieved_documents)\n            logger.debug(f\"Tool {self.name} - {self.id}: finished retrieval. Content: {formatted_content[:100]}...\")\n\n            return {\"content\": formatted_content}\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: execution error: {str(e)}\", exc_info=True)\n            raise\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert the RetrievalTool object to a dictionary.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: Dictionary representation of the object.\n        \"\"\"\n        data = super().to_dict(**kwargs)\n        data.pop(\"text_embedder\", None)\n        data.pop(\"document_retriever\", None)\n        return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/retriever/#dynamiq.nodes.tools.retriever.RetrievalTool.__init__","title":"<code>__init__(connection_manager=None, text_embedder=None, document_retriever=None, **data)</code>","text":"<p>Initialize the RetrievalTool.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Connection manager.</p> <code>None</code> <code>text_embedder</code> <code>ConnectionNode | None</code> <p>Text embedder node.</p> <code>None</code> <code>document_retriever</code> <code>Any | None</code> <p>Document retriever node.</p> <code>None</code> <code>**data</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/tools/retriever.py</code> <pre><code>def __init__(\n    self,\n    connection_manager: ConnectionManager | None = None,\n    text_embedder: ConnectionNode | None = None,\n    document_retriever: Any | None = None,\n    **data,\n):\n    \"\"\"Initialize the RetrievalTool.\n\n    Args:\n        connection_manager (ConnectionManager | None): Connection manager.\n        text_embedder (ConnectionNode | None): Text embedder node.\n        document_retriever (Any | None): Document retriever node.\n        **data: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**data)\n    self.connection_manager = connection_manager\n    self.text_embedder = text_embedder\n    self.document_retriever = document_retriever\n</code></pre>"},{"location":"dynamiq/nodes/tools/retriever/#dynamiq.nodes.tools.retriever.RetrievalTool.execute","title":"<code>execute(input_data, **_)</code>","text":"<p>Execute the retrieval tool.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data for the tool.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Result of the retrieval.</p> Source code in <code>dynamiq/nodes/tools/retriever.py</code> <pre><code>def execute(self, input_data: RetrievalInputSchema, **_) -&gt; dict[str, Any]:\n    \"\"\"Execute the retrieval tool.\n\n    Args:\n        input_data (dict[str, Any]): Input data for the tool.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: Result of the retrieval.\n    \"\"\"\n\n    logger.debug(f\"Tool {self.name} - {self.id}: started with query '{input_data.model_dump()}'\")\n\n    if not self.text_embedder:\n        raise ValueError(f\"{self.name}: Text embedder is not initialized.\")\n    if not self.document_retriever:\n        raise ValueError(f\"{self.name}: Document retriever is not initialized.\")\n\n    try:\n        text_embedder_output = self.text_embedder.run(input_data={\"query\": input_data.query})\n        embedding = text_embedder_output.output.get(\"embedding\")\n\n        document_retriever_output = self.document_retriever.run(input_data={\"embedding\": embedding})\n        retrieved_documents = document_retriever_output.output.get(\"documents\", [])\n        logger.info(f\"Tool {self.name} - {self.id}: retrieved {len(retrieved_documents)} documents\")\n\n        formatted_content = self.format_content(retrieved_documents)\n        logger.debug(f\"Tool {self.name} - {self.id}: finished retrieval. Content: {formatted_content[:100]}...\")\n\n        return {\"content\": formatted_content}\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: execution error: {str(e)}\", exc_info=True)\n        raise\n</code></pre>"},{"location":"dynamiq/nodes/tools/retriever/#dynamiq.nodes.tools.retriever.RetrievalTool.format_content","title":"<code>format_content(documents, metadata_fields=['title', 'url'])</code>","text":"<p>Format the retrieved documents' metadata and content.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of retrieved documents.</p> required <code>metadata_fields</code> <code>list[str]</code> <p>Metadata fields to include.</p> <code>['title', 'url']</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted content of the documents.</p> Source code in <code>dynamiq/nodes/tools/retriever.py</code> <pre><code>def format_content(self, documents: list[Document], metadata_fields: list[str] = [\"title\", \"url\"]) -&gt; str:\n    \"\"\"Format the retrieved documents' metadata and content.\n\n    Args:\n        documents (list[Document]): List of retrieved documents.\n        metadata_fields (list[str]): Metadata fields to include.\n\n    Returns:\n        str: Formatted content of the documents.\n    \"\"\"\n    formatted_docs = []\n    for i, doc in enumerate(documents):\n        metadata = doc.metadata\n        formatted_doc = f\"Source {i + 1}\\n\"\n        for field in metadata_fields:\n            if field in metadata:\n                formatted_doc += f\"{field.capitalize()}: {metadata[field]}\\n\"\n        formatted_doc += f\"Content: {doc.content}\\n\"\n        formatted_docs.append(formatted_doc)\n    return \"\\n\\n\".join(formatted_docs)\n</code></pre>"},{"location":"dynamiq/nodes/tools/retriever/#dynamiq.nodes.tools.retriever.RetrievalTool.init_components","title":"<code>init_components(connection_manager=ConnectionManager())</code>","text":"<p>Initialize components with the connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>connection_manager</code> <code>ConnectionManager</code> <p>Connection manager.</p> <code>ConnectionManager()</code> Source code in <code>dynamiq/nodes/tools/retriever.py</code> <pre><code>def init_components(self, connection_manager: ConnectionManager = ConnectionManager()):\n    \"\"\"Initialize components with the connection manager.\n\n    Args:\n        connection_manager (ConnectionManager): Connection manager.\n    \"\"\"\n    super().init_components(connection_manager)\n    if (\n        hasattr(self.text_embedder, \"is_postponed_component_init\")\n        and self.text_embedder.is_postponed_component_init\n    ):\n        self.text_embedder.init_components(connection_manager)\n    if (\n        hasattr(self.document_retriever, \"is_postponed_component_init\")\n        and self.document_retriever.is_postponed_component_init\n    ):\n        self.document_retriever.init_components(connection_manager)\n</code></pre>"},{"location":"dynamiq/nodes/tools/retriever/#dynamiq.nodes.tools.retriever.RetrievalTool.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the RetrievalTool object to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of the object.</p> Source code in <code>dynamiq/nodes/tools/retriever.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert the RetrievalTool object to a dictionary.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: Dictionary representation of the object.\n    \"\"\"\n    data = super().to_dict(**kwargs)\n    data.pop(\"text_embedder\", None)\n    data.pop(\"document_retriever\", None)\n    return data\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/","title":"Scale serp","text":""},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpInputSchema","title":"<code>ScaleSerpInputSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>class ScaleSerpInputSchema(BaseModel):\n    query: str = Field(default=\"\", description=\"Parameter to provide a search query.\")\n    url: str = Field(default=\"\", description=\"Parameter to provide a search url.\")\n    limit: str = Field(\n        default=\"\", description=\"Parameter to specify the number of results to return, by default is set to 10.\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_query_url(self):\n        \"\"\"Validate that either query or url is specified\"\"\"\n        if not self.url and not self.query:\n            raise ValueError(\"Either 'query' or 'url' has to be specified.\")\n        return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpInputSchema.validate_query_url","title":"<code>validate_query_url()</code>","text":"<p>Validate that either query or url is specified</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_query_url(self):\n    \"\"\"Validate that either query or url is specified\"\"\"\n    if not self.url and not self.query:\n        raise ValueError(\"Either 'query' or 'url' has to be specified.\")\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpTool","title":"<code>ScaleSerpTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for performing web searches using the Scale SERP API.</p> <p>This tool accepts a query or URL and returns search results based on the specified search type (organic, news, images, videos). The results include titles, links, and snippets.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool.</p> <code>connection</code> <code>ScaleSerp</code> <p>The connection instance for the Scale SERP API.</p> <code>limit</code> <code>int</code> <p>The default number of search results to return.</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>class ScaleSerpTool(ConnectionNode):\n    \"\"\"\n    A tool for performing web searches using the Scale SERP API.\n\n    This tool accepts a query or URL and returns search results based on the specified\n    search type (organic, news, images, videos). The results include titles, links, and snippets.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The group to which this tool belongs.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool.\n        connection (ScaleSerp): The connection instance for the Scale SERP API.\n        limit (int): The default number of search results to return.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Scale Serp Search Tool\"\n    description: str = (\n        \"A tool for searching the web, powered by Scale SERP. \"\n        \"You can use this tool to search the web for information.\"\n    )\n    connection: ScaleSerp\n    limit: int = Field(\n        default=10,\n        ge=1,\n        le=100,\n        description=\"The default number of search results to return\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    input_schema: ClassVar[type[ScaleSerpInputSchema]] = ScaleSerpInputSchema\n\n    def _format_search_results(self, results: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Formats the search results into a human-readable string.\n\n        Args:\n            results (dict[str, Any]): The raw search results.\n\n        Returns:\n            str: A formatted string containing the search results.\n        \"\"\"\n        formatted_results = []\n        content_results = results.get(\"organic_results\", [])\n\n        if self.connection.search_type == \"news\":\n            content_results = results.get(\"news_results\", [])\n        elif self.connection.search_type == \"images\":\n            content_results = results.get(\"image_results\", [])\n        elif self.connection.search_type == \"videos\":\n            content_results = results.get(\"video_results\", [])\n\n        for result in content_results:\n            formatted_results.extend(\n                [\n                    f\"Title: {result.get('title')}\",\n                    f\"Link: {result.get('link')}\",\n                    f\"Snippet: {result.get('snippet', 'N/A')}\",\n                    \"\",\n                ]\n            )\n\n        return \"\\n\".join(formatted_results).strip()\n\n    def execute(\n        self, input_data: ScaleSerpInputSchema, config: RunnableConfig | None = None, **kwargs\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the search using the Scale SERP API and returns the formatted results.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the search query or URL.\n            config (RunnableConfig | None, optional): Optional configuration for the execution.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the search results and metadata.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query: str | None = input_data.query\n        url: str | None = input_data.url\n        limit: int = input_data.limit or self.limit\n\n        if not query and not url:\n            return {\n                \"content\": \"Error: Either 'input' (for query) or 'url' must be provided.\"\n            }\n\n        search_params = self.connection.get_params(query=query, url=url, num=limit)\n\n        connection_url = urljoin(self.connection.url, \"/search\")\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                params=search_params,\n            )\n            response.raise_for_status()\n            search_result = response.json()\n        except Exception as e:\n            logger.error(\n                f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\"\n            )\n            raise\n\n        logger.debug(f\"Tool {self.name} - {self.id}: search result {search_result}\")\n\n        formatted_results = self._format_search_results(search_result)\n\n        content_results = search_result.get(\"organic_results\", [])\n        if self.connection.search_type == \"news\":\n            content_results = search_result.get(\"news_results\", [])\n        elif self.connection.search_type == \"images\":\n            content_results = search_result.get(\"image_results\", [])\n        elif self.connection.search_type == \"videos\":\n            content_results = search_result.get(\"video_results\", [])\n\n        sources_with_url = [\n            f\"{result.get('title')}: ({result.get('link')})\"\n            for result in content_results\n        ]\n\n        if self.is_optimized_for_agents:\n            result = (\n                \"&lt;Sources with URLs&gt;\\n\"\n                + \"\\n\".join(sources_with_url)\n                + \"&lt;/Sources with URLs&gt;\\n\\n&lt;Search results&gt;\"\n                + formatted_results\n                + \"&lt;/Search results&gt;\"\n            )\n        else:\n            urls = [result.get(\"link\") for result in content_results]\n\n            result = {\n                \"result\": formatted_results,\n                \"sources_with_url\": sources_with_url,\n                \"urls\": urls,\n                \"raw_response\": search_result,\n            }\n\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/scale_serp/#dynamiq.nodes.tools.scale_serp.ScaleSerpTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the search using the Scale SERP API and returns the formatted results.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the search query or URL.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional configuration for the execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the search results and metadata.</p> Source code in <code>dynamiq/nodes/tools/scale_serp.py</code> <pre><code>def execute(\n    self, input_data: ScaleSerpInputSchema, config: RunnableConfig | None = None, **kwargs\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the search using the Scale SERP API and returns the formatted results.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the search query or URL.\n        config (RunnableConfig | None, optional): Optional configuration for the execution.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the search results and metadata.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query: str | None = input_data.query\n    url: str | None = input_data.url\n    limit: int = input_data.limit or self.limit\n\n    if not query and not url:\n        return {\n            \"content\": \"Error: Either 'input' (for query) or 'url' must be provided.\"\n        }\n\n    search_params = self.connection.get_params(query=query, url=url, num=limit)\n\n    connection_url = urljoin(self.connection.url, \"/search\")\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            params=search_params,\n        )\n        response.raise_for_status()\n        search_result = response.json()\n    except Exception as e:\n        logger.error(\n            f\"Tool {self.name} - {self.id}: failed to get results. Error: {str(e)}\"\n        )\n        raise\n\n    logger.debug(f\"Tool {self.name} - {self.id}: search result {search_result}\")\n\n    formatted_results = self._format_search_results(search_result)\n\n    content_results = search_result.get(\"organic_results\", [])\n    if self.connection.search_type == \"news\":\n        content_results = search_result.get(\"news_results\", [])\n    elif self.connection.search_type == \"images\":\n        content_results = search_result.get(\"image_results\", [])\n    elif self.connection.search_type == \"videos\":\n        content_results = search_result.get(\"video_results\", [])\n\n    sources_with_url = [\n        f\"{result.get('title')}: ({result.get('link')})\"\n        for result in content_results\n    ]\n\n    if self.is_optimized_for_agents:\n        result = (\n            \"&lt;Sources with URLs&gt;\\n\"\n            + \"\\n\".join(sources_with_url)\n            + \"&lt;/Sources with URLs&gt;\\n\\n&lt;Search results&gt;\"\n            + formatted_results\n            + \"&lt;/Search results&gt;\"\n        )\n    else:\n        urls = [result.get(\"link\") for result in content_results]\n\n        result = {\n            \"result\": formatted_results,\n            \"sources_with_url\": sources_with_url,\n            \"urls\": urls,\n            \"raw_response\": search_result,\n        }\n\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/tavily/","title":"Tavily","text":""},{"location":"dynamiq/nodes/tools/tavily/#dynamiq.nodes.tools.tavily.TavilyTool","title":"<code>TavilyTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>TavilyTool is a ConnectionNode that interfaces with the Tavily search service.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[TOOLS]</code> <p>The node group to which this tool belongs.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>A brief description of the tool's functionality.</p> <code>connection</code> <code>Tavily</code> <p>The connection object for interacting with Tavily.</p> <code>search_depth</code> <code>str</code> <p>The depth of the search, default is 'basic'.</p> <code>topic</code> <code>str</code> <p>The topic to search for, default is 'general'.</p> <code>max_results</code> <code>int</code> <p>Maximum number of search results to return, default is 5.</p> <code>include_images</code> <code>bool</code> <p>Flag to include images in search results, default is False.</p> <code>include_answer</code> <code>bool</code> <p>Flag to include an answer in search results, default is False.</p> <code>include_raw_content</code> <code>bool</code> <p>Flag to include raw content in search results, default is False.</p> <code>include_domains</code> <code>list[str]</code> <p>Domains to include in search results.</p> <code>exclude_domains</code> <code>list[str]</code> <p>Domains to exclude from search results.</p> <code>use_cache</code> <code>bool</code> <p>Flag to use cache for search results, default is True.</p> Source code in <code>dynamiq/nodes/tools/tavily.py</code> <pre><code>class TavilyTool(ConnectionNode):\n    \"\"\"\n    TavilyTool is a ConnectionNode that interfaces with the Tavily search service.\n\n    Attributes:\n        group (Literal[NodeGroup.TOOLS]): The node group to which this tool belongs.\n        name (str): The name of the tool.\n        description (str): A brief description of the tool's functionality.\n        connection (Tavily): The connection object for interacting with Tavily.\n        search_depth (str): The depth of the search, default is 'basic'.\n        topic (str): The topic to search for, default is 'general'.\n        max_results (int): Maximum number of search results to return, default is 5.\n        include_images (bool): Flag to include images in search results, default is False.\n        include_answer (bool): Flag to include an answer in search results, default is False.\n        include_raw_content (bool): Flag to include raw content in search results, default is False.\n        include_domains (list[str]): Domains to include in search results.\n        exclude_domains (list[str]): Domains to exclude from search results.\n        use_cache (bool): Flag to use cache for search results, default is True.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Tavily Search Tool\"\n    description: str = (\n        \"A tool for searching the web, powered by Tavily. \"\n    )\n    connection: Tavily\n\n    search_depth: str = Field(default=\"basic\", description=\"The search depth to use.\")\n    topic: str = Field(default=\"general\", description=\"The topic to search for.\")\n    max_results: int = Field(\n        default=5,\n        ge=1,\n        le=100,\n        description=\"The maximum number of search results to return.\",\n    )\n    include_images: bool = Field(\n        default=False, description=\"Include images in search results.\"\n    )\n    include_answer: bool = Field(\n        default=False, description=\"Include answer in search results.\"\n    )\n    include_raw_content: bool = Field(\n        default=False, description=\"Include raw content in search results.\"\n    )\n    include_domains: list[str] = Field(\n        default_factory=list, description=\"The domains to include in search results.\"\n    )\n    exclude_domains: list[str] = Field(\n        default_factory=list, description=\"The domains to exclude from search results.\"\n    )\n    use_cache: bool = Field(default=True, description=\"Use cache for search results.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    input_schema: ClassVar[type[TavilyInputSchema]] = TavilyInputSchema\n\n    def _format_search_results(self, results: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Formats the search results into a readable string format.\n\n        Args:\n            results (dict[str, Any]): The raw search results from Tavily.\n\n        Returns:\n            str: The formatted search results as a string.\n        \"\"\"\n        formatted_results = []\n        for result in results.get(\"results\", []):\n            formatted_results.append(f\"Source: {result.get('url')}\")\n            formatted_results.append(f\"Title: {result.get('title')}\")\n            formatted_results.append(f\"Content: {result.get('content')}\")\n            if result.get(\"raw_content\"):\n                formatted_results.append(f\"Full Content: {result.get('raw_content')}\")\n            formatted_results.append(f\"Relevance Score: {result.get('score')}\")\n            formatted_results.append(\"\")  # Blank line between results\n\n        return \"\\n\".join(formatted_results).strip()\n\n    def execute(self, input_data: TavilyInputSchema, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the search operation using the provided input data.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the search query.\n            config (RunnableConfig | None): Optional configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict[str, Any]: The result of the search operation.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        query = input_data.query\n        search_data = {\n            \"query\": query,\n            \"search_depth\": self.search_depth,\n            \"topic\": self.topic,\n            \"max_results\": self.max_results,\n            \"include_images\": self.include_images,\n            \"include_answer\": self.include_answer,\n            \"include_raw_content\": self.include_raw_content,\n            \"include_domains\": self.include_domains,\n            \"exclude_domains\": self.exclude_domains,\n            \"use_cache\": self.use_cache,\n        }\n\n        connection_url = urljoin(self.connection.url, \"/search\")\n\n        try:\n            response = self.client.request(\n                method=self.connection.method,\n                url=connection_url,\n                json={**self.connection.data, **search_data},\n            )\n            response.raise_for_status()\n            search_result = response.json()\n        except Exception as e:\n            logger.error(\n                f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\"\n            )\n            raise\n\n        logger.debug(f\"Tool {self.name} - {self.id}: search result {search_result}\")\n\n        formatted_results = self._format_search_results(search_result)\n\n        sources_with_url = [\n            f\"[{result.get('title')}]({result.get('url')})\"\n            for result in search_result.get(\"results\", [])\n        ]\n        if self.is_optimized_for_agents:\n            result = (\n                \"&lt;Sources with URLs&gt;\\n\"\n                + \"\\n\".join(sources_with_url)\n                + f\"\\n&lt;\\\\Sources with URLs&gt;\\n\\n&lt;Search results for query {query}&gt;\\n\"\n                + formatted_results\n                + f\"\\n&lt;\\\\Search results for query {query}&gt;\"\n            )\n            if search_result.get(\"answer\", \"\") != \"\":\n                result += f\"\\n\\n&lt;Answer&gt;\\n{search_result.get('answer')}\\n&lt;\\\\Answer&gt;\"\n\n        else:\n            result = {\n                \"result\": formatted_results,\n                \"sources_with_url\": sources_with_url,\n                \"raw_response\": search_result,\n                \"images\": search_result.get(\"images\", []),\n                \"answer\": search_result.get(\"answer\", \"\"),\n                \"query\": search_result.get(\"query\", \"\"),\n                \"response_time\": search_result.get(\"response_time\", 0),\n            }\n\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/tavily/#dynamiq.nodes.tools.tavily.TavilyTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the search operation using the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the search query.</p> required <code>config</code> <code>RunnableConfig | None</code> <p>Optional configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The result of the search operation.</p> Source code in <code>dynamiq/nodes/tools/tavily.py</code> <pre><code>def execute(self, input_data: TavilyInputSchema, config: RunnableConfig | None = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the search operation using the provided input data.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the search query.\n        config (RunnableConfig | None): Optional configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict[str, Any]: The result of the search operation.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    query = input_data.query\n    search_data = {\n        \"query\": query,\n        \"search_depth\": self.search_depth,\n        \"topic\": self.topic,\n        \"max_results\": self.max_results,\n        \"include_images\": self.include_images,\n        \"include_answer\": self.include_answer,\n        \"include_raw_content\": self.include_raw_content,\n        \"include_domains\": self.include_domains,\n        \"exclude_domains\": self.exclude_domains,\n        \"use_cache\": self.use_cache,\n    }\n\n    connection_url = urljoin(self.connection.url, \"/search\")\n\n    try:\n        response = self.client.request(\n            method=self.connection.method,\n            url=connection_url,\n            json={**self.connection.data, **search_data},\n        )\n        response.raise_for_status()\n        search_result = response.json()\n    except Exception as e:\n        logger.error(\n            f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\"\n        )\n        raise\n\n    logger.debug(f\"Tool {self.name} - {self.id}: search result {search_result}\")\n\n    formatted_results = self._format_search_results(search_result)\n\n    sources_with_url = [\n        f\"[{result.get('title')}]({result.get('url')})\"\n        for result in search_result.get(\"results\", [])\n    ]\n    if self.is_optimized_for_agents:\n        result = (\n            \"&lt;Sources with URLs&gt;\\n\"\n            + \"\\n\".join(sources_with_url)\n            + f\"\\n&lt;\\\\Sources with URLs&gt;\\n\\n&lt;Search results for query {query}&gt;\\n\"\n            + formatted_results\n            + f\"\\n&lt;\\\\Search results for query {query}&gt;\"\n        )\n        if search_result.get(\"answer\", \"\") != \"\":\n            result += f\"\\n\\n&lt;Answer&gt;\\n{search_result.get('answer')}\\n&lt;\\\\Answer&gt;\"\n\n    else:\n        result = {\n            \"result\": formatted_results,\n            \"sources_with_url\": sources_with_url,\n            \"raw_response\": search_result,\n            \"images\": search_result.get(\"images\", []),\n            \"answer\": search_result.get(\"answer\", \"\"),\n            \"query\": search_result.get(\"query\", \"\"),\n            \"response_time\": search_result.get(\"response_time\", 0),\n        }\n\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/zenrows/","title":"Zenrows","text":""},{"location":"dynamiq/nodes/tools/zenrows/#dynamiq.nodes.tools.zenrows.ZenRowsTool","title":"<code>ZenRowsTool</code>","text":"<p>               Bases: <code>ConnectionNode</code></p> <p>A tool for scraping web pages, powered by ZenRows.</p> <p>This class is responsible for scraping the content of a web page using ZenRows.</p> Source code in <code>dynamiq/nodes/tools/zenrows.py</code> <pre><code>class ZenRowsTool(ConnectionNode):\n    \"\"\"\n    A tool for scraping web pages, powered by ZenRows.\n\n    This class is responsible for scraping the content of a web page using ZenRows.\n    \"\"\"\n\n    group: Literal[NodeGroup.TOOLS] = NodeGroup.TOOLS\n    name: str = \"Zenrows Scraper Tool\"\n    description: str = (\n        \"A tool for scraping web pages, powered by ZenRows. \"\n        \"You can use this tool to scrape the content of a web page.\"\n    )\n    connection: ZenRows\n    url: str | None = None\n    markdown_response: bool = Field(\n        default=True,\n        description=\"If True, the content will be parsed as Markdown instead of HTML.\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    input_schema: ClassVar[type[ZenRowsInputSchema]] = ZenRowsInputSchema\n\n    def execute(self, input_data: ZenRowsInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Executes the web scraping process.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing 'input' key with the URL to scrape.\n            config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n            **kwargs: Additional arguments passed to the execution context.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the URL and the scraped content.\n        \"\"\"\n        logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n        # Ensure the config is set up correctly\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        params = {\n            \"url\": input_data.url,\n            \"markdown_response\": str(self.markdown_response).lower(),\n        }\n\n        try:\n            # Perform the HTTP request using the ZenRows connection\n            response = self.client.request(\n                method=self.connection.method,\n                url=self.connection.url,\n                params={**self.connection.params, **params},\n            )\n            response.raise_for_status()\n            scrape_result = response.text\n        except Exception as e:\n            logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\")\n\n            raise\n\n        if self.is_optimized_for_agents:\n            result = (\n                f\"&lt;Source URL&gt;\\n{input_data.url}\\n&lt;\\\\Source URL&gt;\"\n                f\"\\n&lt;Scraped result&gt;\\n{scrape_result}\\n&lt;\\\\Scraped result&gt;\"\n            )\n        else:\n            result = {\"url\": input_data.url, \"content\": scrape_result}\n        logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n        return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/tools/zenrows/#dynamiq.nodes.tools.zenrows.ZenRowsTool.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the web scraping process.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing 'input' key with the URL to scrape.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the runnable, including callbacks.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the execution context.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the URL and the scraped content.</p> Source code in <code>dynamiq/nodes/tools/zenrows.py</code> <pre><code>def execute(self, input_data: ZenRowsInputSchema, config: RunnableConfig = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Executes the web scraping process.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing 'input' key with the URL to scrape.\n        config (RunnableConfig, optional): Configuration for the runnable, including callbacks.\n        **kwargs: Additional arguments passed to the execution context.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the URL and the scraped content.\n    \"\"\"\n    logger.debug(f\"Tool {self.name} - {self.id}: started with input data {input_data.model_dump()}\")\n\n    # Ensure the config is set up correctly\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    params = {\n        \"url\": input_data.url,\n        \"markdown_response\": str(self.markdown_response).lower(),\n    }\n\n    try:\n        # Perform the HTTP request using the ZenRows connection\n        response = self.client.request(\n            method=self.connection.method,\n            url=self.connection.url,\n            params={**self.connection.params, **params},\n        )\n        response.raise_for_status()\n        scrape_result = response.text\n    except Exception as e:\n        logger.error(f\"Tool {self.name} - {self.id}: failed to get results. Error: {e}\")\n\n        raise\n\n    if self.is_optimized_for_agents:\n        result = (\n            f\"&lt;Source URL&gt;\\n{input_data.url}\\n&lt;\\\\Source URL&gt;\"\n            f\"\\n&lt;Scraped result&gt;\\n{scrape_result}\\n&lt;\\\\Scraped result&gt;\"\n        )\n    else:\n        result = {\"url\": input_data.url, \"content\": scrape_result}\n    logger.debug(f\"Tool {self.name} - {self.id}: finished with result {str(result)[:200]}...\")\n\n    return {\"content\": result}\n</code></pre>"},{"location":"dynamiq/nodes/utils/utils/","title":"Utils","text":""},{"location":"dynamiq/nodes/utils/utils/#dynamiq.nodes.utils.utils.Input","title":"<code>Input</code>","text":"<p>               Bases: <code>Pass</code></p> <p>A utility node representing the input of workflow.</p> <p>This class inherits from the Pass operator and is used to mark the beginning of a sequence of operations. It is typically used in workflow definitions or process models.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[UTILS]</code> <p>The group the node belongs to, set to UTILS.</p> <code>schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the input data.</p> Source code in <code>dynamiq/nodes/utils/utils.py</code> <pre><code>class Input(Pass):\n    \"\"\"\n    A utility node representing the input of workflow.\n\n    This class inherits from the Pass operator and is used to mark the beginning of a sequence of\n    operations. It is typically used in workflow definitions or process models.\n\n    Attributes:\n        group (Literal[NodeGroup.UTILS]): The group the node belongs to, set to UTILS.\n        schema (dict[str, Any] | None): The JSON schema for the input data.\n    \"\"\"\n\n    name: str | None = \"Start\"\n    group: Literal[NodeGroup.UTILS] = NodeGroup.UTILS\n    schema: dict[str, Any] | None = None\n</code></pre>"},{"location":"dynamiq/nodes/utils/utils/#dynamiq.nodes.utils.utils.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>Pass</code></p> <p>A utility node representing the output of workflow.</p> <p>This class inherits from the Pass operator and is used to mark the conclusion of a sequence of operations. It is typically used in workflow definitions or process models.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[UTILS]</code> <p>The group the node belongs to, set to UTILS.</p> <code>schema</code> <code>dict[str, Any] | None</code> <p>The JSON schema for the output data.</p> Source code in <code>dynamiq/nodes/utils/utils.py</code> <pre><code>class Output(Pass):\n    \"\"\"\n    A utility node representing the output of workflow.\n\n    This class inherits from the Pass operator and is used to mark the conclusion of a sequence of\n    operations. It is typically used in workflow definitions or process models.\n\n    Attributes:\n        group (Literal[NodeGroup.UTILS]): The group the node belongs to, set to UTILS.\n        schema (dict[str, Any] | None): The JSON schema for the output data.\n    \"\"\"\n\n    name: str | None = \"End\"\n    group: Literal[NodeGroup.UTILS] = NodeGroup.UTILS\n    schema: dict[str, Any] | None = None\n</code></pre>"},{"location":"dynamiq/nodes/validators/base/","title":"Base","text":""},{"location":"dynamiq/nodes/validators/base/#dynamiq.nodes.validators.base.BaseValidator","title":"<code>BaseValidator</code>","text":"<p>               Bases: <code>Node</code></p> Source code in <code>dynamiq/nodes/validators/base.py</code> <pre><code>class BaseValidator(Node):\n    group: Literal[NodeGroup.VALIDATORS] = NodeGroup.VALIDATORS\n    name: str | None = \"Validator\"\n    behavior: Behavior | None = Behavior.RETURN\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"Executes the validation process for a given value.\n\n        Args:\n            input_data (dict[str, Any]): The input data containing the value to check under \"content\" key.\n            config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            input_data: A dictionary with the following key if behavior is return:\n                - \"valid\" (bool): boolean indicating if the value is valid.\n                - \"content\" (Any): passed value if everything is correct.\n            bool\n\n        Raises:\n            ValueError: If the value is not valid and behavior equal raise type.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n        try:\n            self.validate(input_data[\"content\"])\n        except Exception as error:\n            if self.behavior == Behavior.RETURN:\n                return {\"valid\": False, \"content\": input_data[\"content\"]}\n            raise ValueError(str(error))\n        return {\"valid\": True, \"content\": input_data[\"content\"]}\n\n    @abstractmethod\n    def validate(self, content):\n        pass\n</code></pre>"},{"location":"dynamiq/nodes/validators/base/#dynamiq.nodes.validators.base.BaseValidator.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Executes the validation process for a given value.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>The input data containing the value to check under \"content\" key.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>input_data</code> <p>A dictionary with the following key if behavior is return: - \"valid\" (bool): boolean indicating if the value is valid. - \"content\" (Any): passed value if everything is correct.</p> <p>bool</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not valid and behavior equal raise type.</p> Source code in <code>dynamiq/nodes/validators/base.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"Executes the validation process for a given value.\n\n    Args:\n        input_data (dict[str, Any]): The input data containing the value to check under \"content\" key.\n        config (RunnableConfig, optional): Configuration for the execution. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        input_data: A dictionary with the following key if behavior is return:\n            - \"valid\" (bool): boolean indicating if the value is valid.\n            - \"content\" (Any): passed value if everything is correct.\n        bool\n\n    Raises:\n        ValueError: If the value is not valid and behavior equal raise type.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n    try:\n        self.validate(input_data[\"content\"])\n    except Exception as error:\n        if self.behavior == Behavior.RETURN:\n            return {\"valid\": False, \"content\": input_data[\"content\"]}\n        raise ValueError(str(error))\n    return {\"valid\": True, \"content\": input_data[\"content\"]}\n</code></pre>"},{"location":"dynamiq/nodes/validators/regex_match/","title":"Regex match","text":""},{"location":"dynamiq/nodes/validators/regex_match/#dynamiq.nodes.validators.regex_match.RegexMatch","title":"<code>RegexMatch</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Validates that a value matches a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <p>A regular expression pattern.</p> required <code>match_type</code> <p>Match type to check input value for a regex search or full-match option.</p> required Source code in <code>dynamiq/nodes/validators/regex_match.py</code> <pre><code>class RegexMatch(BaseValidator):\n    \"\"\"\n    Validates that a value matches a regular expression.\n\n    Args:\n        regex: A regular expression pattern.\n        match_type: Match type to check input value for a regex search or full-match option.\n    \"\"\"\n\n    regex: str\n    match_type: MatchType | None = MatchType.FULL_MATCH\n\n    def validate(self, content: str):\n        \"\"\"\n        Validates if the provided value matches the given regular expression pattern.\n\n        Args:\n            content (str): The value to validate.\n\n        Raises:\n            ValueError: If the provided value does not match the given pattern.\n        \"\"\"\n        compiled_pattern = re.compile(self.regex)\n        match_method = getattr(compiled_pattern, self.match_type)\n        if not match_method(content):\n            raise ValueError(\n                f\"Value does not match the valid pattern. Value: '{content}'. Pattern: '{self.regex}'\",\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/regex_match/#dynamiq.nodes.validators.regex_match.RegexMatch.validate","title":"<code>validate(content)</code>","text":"<p>Validates if the provided value matches the given regular expression pattern.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided value does not match the given pattern.</p> Source code in <code>dynamiq/nodes/validators/regex_match.py</code> <pre><code>def validate(self, content: str):\n    \"\"\"\n    Validates if the provided value matches the given regular expression pattern.\n\n    Args:\n        content (str): The value to validate.\n\n    Raises:\n        ValueError: If the provided value does not match the given pattern.\n    \"\"\"\n    compiled_pattern = re.compile(self.regex)\n    match_method = getattr(compiled_pattern, self.match_type)\n    if not match_method(content):\n        raise ValueError(\n            f\"Value does not match the valid pattern. Value: '{content}'. Pattern: '{self.regex}'\",\n        )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_choices/","title":"Valid choices","text":""},{"location":"dynamiq/nodes/validators/valid_choices/#dynamiq.nodes.validators.valid_choices.ValidChoices","title":"<code>ValidChoices</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Class that provides functionality to check if the provided value is within the list of valid choices.</p> <p>Parameters:</p> Name Type Description Default <code>choices(List[Any])</code> <p>A list of values representing the acceptable choices.</p> required Source code in <code>dynamiq/nodes/validators/valid_choices.py</code> <pre><code>class ValidChoices(BaseValidator):\n    \"\"\"\n    Class that provides functionality to check if the provided value is within the list of valid choices.\n\n    Args:\n        choices(List[Any]): A list of values representing the acceptable choices.\n\n    \"\"\"\n\n    choices: list[Any] = None\n\n    def validate(self, content: Any):\n        \"\"\"\n        Validates if the provided value is among the acceptable choices.\n\n        Args:\n            content(Any): The value to validate.\n\n        Raises:\n            ValueError: If the provided value is not in valid choices.\n        \"\"\"\n        if isinstance(content, str):\n            content = content.strip()\n\n        if content not in self.choices:\n            raise ValueError(\n                f\"Value is not in valid choices. Value: '{content}'. Choices: '{self.choices}'.\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_choices/#dynamiq.nodes.validators.valid_choices.ValidChoices.validate","title":"<code>validate(content)</code>","text":"<p>Validates if the provided value is among the acceptable choices.</p> <p>Parameters:</p> Name Type Description Default <code>content(Any)</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided value is not in valid choices.</p> Source code in <code>dynamiq/nodes/validators/valid_choices.py</code> <pre><code>def validate(self, content: Any):\n    \"\"\"\n    Validates if the provided value is among the acceptable choices.\n\n    Args:\n        content(Any): The value to validate.\n\n    Raises:\n        ValueError: If the provided value is not in valid choices.\n    \"\"\"\n    if isinstance(content, str):\n        content = content.strip()\n\n    if content not in self.choices:\n        raise ValueError(\n            f\"Value is not in valid choices. Value: '{content}'. Choices: '{self.choices}'.\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_json/","title":"Valid json","text":""},{"location":"dynamiq/nodes/validators/valid_json/#dynamiq.nodes.validators.valid_json.ValidJSON","title":"<code>ValidJSON</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Class that provides functionality to check if a value matches a basic JSON structure.</p> Source code in <code>dynamiq/nodes/validators/valid_json.py</code> <pre><code>class ValidJSON(BaseValidator):\n    \"\"\"\n    Class that provides functionality to check if a value matches a basic JSON structure.\n    \"\"\"\n\n    def validate(self, content: str | dict):\n        \"\"\"\n        Validates if the provided string is a properly formatted JSON.\n\n        Args:\n            content(str): The value to check.\n\n        Raises:\n            ValueError: If the value is not a properly formatted JSON.\n\n        \"\"\"\n        try:\n            if not isinstance(content, str):\n                content = json.dumps(content)\n\n            json.loads(content)\n        except (json.decoder.JSONDecodeError, TypeError) as error:\n            raise ValueError(\n                f\"Value is not valid JSON. Value: '{content}'. Error details: {str(error)}\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_json/#dynamiq.nodes.validators.valid_json.ValidJSON.validate","title":"<code>validate(content)</code>","text":"<p>Validates if the provided string is a properly formatted JSON.</p> <p>Parameters:</p> Name Type Description Default <code>content(str)</code> <p>The value to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not a properly formatted JSON.</p> Source code in <code>dynamiq/nodes/validators/valid_json.py</code> <pre><code>def validate(self, content: str | dict):\n    \"\"\"\n    Validates if the provided string is a properly formatted JSON.\n\n    Args:\n        content(str): The value to check.\n\n    Raises:\n        ValueError: If the value is not a properly formatted JSON.\n\n    \"\"\"\n    try:\n        if not isinstance(content, str):\n            content = json.dumps(content)\n\n        json.loads(content)\n    except (json.decoder.JSONDecodeError, TypeError) as error:\n        raise ValueError(\n            f\"Value is not valid JSON. Value: '{content}'. Error details: {str(error)}\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_python/","title":"Valid python","text":""},{"location":"dynamiq/nodes/validators/valid_python/#dynamiq.nodes.validators.valid_python.ValidPython","title":"<code>ValidPython</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Class that provides functionality to check if a value matches a basic Python code standards.</p> Source code in <code>dynamiq/nodes/validators/valid_python.py</code> <pre><code>class ValidPython(BaseValidator):\n    \"\"\"\n    Class that provides functionality to check if a value matches a basic Python code standards.\n    \"\"\"\n\n    def validate(self, content: str):\n        \"\"\"\n        Validates the provided Python code to determine if it is syntactically correct.\n\n        Args:\n            content (str): The Python code to validate.\n\n        Raises:\n            ValueError: Raised if the provided value is not syntactically correct Python code.\n        \"\"\"\n        try:\n            ast.parse(content)\n        except SyntaxError as e:\n            raise ValueError(\n                f\"Value is not valid python code. Value: '{content}'. Error details: {e.msg}\"\n            )\n</code></pre>"},{"location":"dynamiq/nodes/validators/valid_python/#dynamiq.nodes.validators.valid_python.ValidPython.validate","title":"<code>validate(content)</code>","text":"<p>Validates the provided Python code to determine if it is syntactically correct.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The Python code to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if the provided value is not syntactically correct Python code.</p> Source code in <code>dynamiq/nodes/validators/valid_python.py</code> <pre><code>def validate(self, content: str):\n    \"\"\"\n    Validates the provided Python code to determine if it is syntactically correct.\n\n    Args:\n        content (str): The Python code to validate.\n\n    Raises:\n        ValueError: Raised if the provided value is not syntactically correct Python code.\n    \"\"\"\n    try:\n        ast.parse(content)\n    except SyntaxError as e:\n        raise ValueError(\n            f\"Value is not valid python code. Value: '{content}'. Error details: {e.msg}\"\n        )\n</code></pre>"},{"location":"dynamiq/nodes/writers/chroma/","title":"Chroma","text":""},{"location":"dynamiq/nodes/writers/chroma/#dynamiq.nodes.writers.chroma.ChromaDocumentWriter","title":"<code>ChromaDocumentWriter</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>BaseWriterVectorStoreParams</code></p> <p>Document Writer Node using Chroma Vector Store.</p> <p>This class represents a node for writing documents to a Chroma Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Chroma | None</code> <p>The connection to the Chroma Vector Store.</p> <code>vector_store</code> <code>ChromaVectorStore | None</code> <p>The Chroma Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/chroma.py</code> <pre><code>class ChromaDocumentWriter(VectorStoreNode, BaseWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Chroma Vector Store.\n\n    This class represents a node for writing documents to a Chroma Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Chroma | None): The connection to the Chroma Vector Store.\n        vector_store (ChromaVectorStore | None): The Chroma Vector Store instance.\n    \"\"\"\n\n    group: Literal[NodeGroup.WRITERS] = NodeGroup.WRITERS\n    name: str = \"ChromaDocumentWriter\"\n    connection: Chroma | None = None\n    vector_store: ChromaVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ChromaDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Chroma()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return ChromaVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(BaseWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the documents provided in the input_data to the Chroma Vector Store.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data.\n                Expected to have a 'documents' key with the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n\n        output = self.vector_store.write_documents(documents)\n        return {\n            \"upserted_count\": output,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/chroma/#dynamiq.nodes.writers.chroma.ChromaDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ChromaDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/chroma.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ChromaDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Chroma connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Chroma()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/chroma/#dynamiq.nodes.writers.chroma.ChromaDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the documents provided in the input_data to the Chroma Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/chroma.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the documents provided in the input_data to the Chroma Vector Store.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data.\n            Expected to have a 'documents' key with the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n\n    output = self.vector_store.write_documents(documents)\n    return {\n        \"upserted_count\": output,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/milvus/","title":"Milvus","text":""},{"location":"dynamiq/nodes/writers/milvus/#dynamiq.nodes.writers.milvus.MilvusDocumentWriter","title":"<code>MilvusDocumentWriter</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>BaseWriterVectorStoreParams</code></p> <p>Document Writer Node using Milvus Vector Store.</p> <p>This class represents a node for writing documents to a Milvus Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Chroma | None</code> <p>The connection to the Chroma Vector Store.</p> <code>vector_store</code> <code>ChromaVectorStore | None</code> <p>The Chroma Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/milvus.py</code> <pre><code>class MilvusDocumentWriter(VectorStoreNode, BaseWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Milvus Vector Store.\n\n    This class represents a node for writing documents to a Milvus Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Chroma | None): The connection to the Chroma Vector Store.\n        vector_store (ChromaVectorStore | None): The Chroma Vector Store instance.\n    \"\"\"\n\n    group: Literal[NodeGroup.WRITERS] = NodeGroup.WRITERS\n    name: str = \"MilvusDocumentWriter\"\n    connection: Milvus | None = None\n    vector_store: MilvusVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the MilvusDocumentWriter.\n\n        If no vector_store or connection is provided in kwargs, a default Milvus connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Milvus()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return MilvusVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(BaseWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing process.\n\n        This method writes the input documents to the Milvus Vector Store.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data.\n                Expected to have a 'documents' key with the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the number of upserted documents.\n\n        Raises:\n            Any exceptions raised by the vector store's write_documents method.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n\n        # Write documents to Milvus\n        upserted_count = self.vector_store.write_documents(documents)\n        logger.debug(f\"Upserted {upserted_count} documents to Milvus Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/milvus/#dynamiq.nodes.writers.milvus.MilvusDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the MilvusDocumentWriter.</p> <p>If no vector_store or connection is provided in kwargs, a default Milvus connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/milvus.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the MilvusDocumentWriter.\n\n    If no vector_store or connection is provided in kwargs, a default Milvus connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Milvus()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/milvus/#dynamiq.nodes.writers.milvus.MilvusDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing process.</p> <p>This method writes the input documents to the Milvus Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the number of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/milvus.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing process.\n\n    This method writes the input documents to the Milvus Vector Store.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data.\n            Expected to have a 'documents' key with the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the number of upserted documents.\n\n    Raises:\n        Any exceptions raised by the vector store's write_documents method.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n\n    # Write documents to Milvus\n    upserted_count = self.vector_store.write_documents(documents)\n    logger.debug(f\"Upserted {upserted_count} documents to Milvus Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter","title":"<code>PineconeDocumentWriter</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>PineconeWriterVectorStoreParams</code></p> <p>Document Writer Node using Pinecone Vector Store.</p> <p>This class represents a node for writing documents to a Pinecone Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Pinecone | None</code> <p>The Pinecone connection object.</p> <code>vector_store</code> <code>PineconeVectorStore | None</code> <p>The Pinecone Vector Store object.</p> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>class PineconeDocumentWriter(VectorStoreNode, PineconeWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Pinecone Vector Store.\n\n    This class represents a node for writing documents to a Pinecone Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Pinecone | None): The Pinecone connection object.\n        vector_store (PineconeVectorStore | None): The Pinecone Vector Store object.\n    \"\"\"\n\n    group: Literal[NodeGroup.WRITERS] = NodeGroup.WRITERS\n    name: str = \"PineconeDocumentWriter\"\n    connection: Pinecone | None = None\n    vector_store: PineconeVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the PineconeDocumentWriter.\n\n        If no vector_store or connection is provided in kwargs, a default Pinecone connection will be created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Pinecone()\n        super().__init__(**kwargs)\n\n    @model_validator(mode=\"after\")\n    def check_required_params(self) -&gt; \"PineconeDocumentWriter\":\n        \"\"\"\n        Validate required parameters\n\n        Returns:\n            self: The updated instance.\n        \"\"\"\n        if self.vector_store is None:\n            if self.create_if_not_exist and self.index_type is None:\n                raise ValueError(\"Index type 'pod' or 'serverless' must be specified when creating an index\")\n\n            if self.index_type == PineconeIndexType.POD and (self.environment is None or self.pod_type is None):\n                raise ValueError(\"'environment' and 'pod_type' must be specified for 'pod' index\")\n\n            if self.index_type == PineconeIndexType.SERVERLESS and (self.cloud is None or self.region is None):\n                raise ValueError(\"'cloud' and 'region' must be specified for 'serverless' index\")\n\n        return self\n\n    @property\n    def vector_store_cls(self):\n        return PineconeVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(PineconeWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the document writing process.\n\n        This method writes the input documents to the Pinecone Vector Store.\n\n        Args:\n            input_data (dict[str, Any]): A dictionary containing the input data.\n                Expected to have a 'documents' key with the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the number of upserted documents.\n\n        Raises:\n            Any exceptions raised by the vector store's write_documents method.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n\n        upserted_count = self.vector_store.write_documents(documents)\n        logger.debug(f\"Upserted {upserted_count} documents to Pinecone Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PineconeDocumentWriter.</p> <p>If no vector_store or connection is provided in kwargs, a default Pinecone connection will be created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the PineconeDocumentWriter.\n\n    If no vector_store or connection is provided in kwargs, a default Pinecone connection will be created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Pinecone()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter.check_required_params","title":"<code>check_required_params()</code>","text":"<p>Validate required parameters</p> <p>Returns:</p> Name Type Description <code>self</code> <code>PineconeDocumentWriter</code> <p>The updated instance.</p> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_required_params(self) -&gt; \"PineconeDocumentWriter\":\n    \"\"\"\n    Validate required parameters\n\n    Returns:\n        self: The updated instance.\n    \"\"\"\n    if self.vector_store is None:\n        if self.create_if_not_exist and self.index_type is None:\n            raise ValueError(\"Index type 'pod' or 'serverless' must be specified when creating an index\")\n\n        if self.index_type == PineconeIndexType.POD and (self.environment is None or self.pod_type is None):\n            raise ValueError(\"'environment' and 'pod_type' must be specified for 'pod' index\")\n\n        if self.index_type == PineconeIndexType.SERVERLESS and (self.cloud is None or self.region is None):\n            raise ValueError(\"'cloud' and 'region' must be specified for 'serverless' index\")\n\n    return self\n</code></pre>"},{"location":"dynamiq/nodes/writers/pinecone/#dynamiq.nodes.writers.pinecone.PineconeDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing process.</p> <p>This method writes the input documents to the Pinecone Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the input data. Expected to have a 'documents' key with the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the number of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/pinecone.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the document writing process.\n\n    This method writes the input documents to the Pinecone Vector Store.\n\n    Args:\n        input_data (dict[str, Any]): A dictionary containing the input data.\n            Expected to have a 'documents' key with the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the number of upserted documents.\n\n    Raises:\n        Any exceptions raised by the vector store's write_documents method.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n\n    upserted_count = self.vector_store.write_documents(documents)\n    logger.debug(f\"Upserted {upserted_count} documents to Pinecone Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/nodes/writers/qdrant/#dynamiq.nodes.writers.qdrant.QdrantDocumentWriter","title":"<code>QdrantDocumentWriter</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>QdrantWriterVectorStoreParams</code></p> <p>Document Writer Node using Qdrant Vector Store.</p> <p>This class represents a node for writing documents to a Weaviate Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Qdrant | None</code> <p>The Qdrant connection.</p> <code>vector_store</code> <code>QdrantVectorStore | None</code> <p>The Qdrant Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/qdrant.py</code> <pre><code>class QdrantDocumentWriter(VectorStoreNode, QdrantWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Qdrant Vector Store.\n\n    This class represents a node for writing documents to a Weaviate Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Qdrant | None): The Qdrant connection.\n        vector_store (QdrantVectorStore | None): The Qdrant Vector Store instance.\n    \"\"\"\n\n    group: Literal[NodeGroup.WRITERS] = NodeGroup.WRITERS\n    name: str = \"QdrantDocumentWriter\"\n    connection: QdrantConnection | None = None\n    vector_store: QdrantVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the QdrantDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Qdrant connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = QdrantConnection()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return QdrantVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(QdrantWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the Qdrant Vector Store.\n\n        Args:\n            input_data (dict[str, Any]): Input data containing the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n\n        upserted_count = self.vector_store.write_documents(documents)\n        logger.debug(f\"Upserted {upserted_count} documents to Qdrant Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/qdrant/#dynamiq.nodes.writers.qdrant.QdrantDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the QdrantDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Qdrant connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/qdrant.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the QdrantDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Qdrant connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = QdrantConnection()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/qdrant/#dynamiq.nodes.writers.qdrant.QdrantDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the Qdrant Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/qdrant.py</code> <pre><code>def execute(self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the Qdrant Vector Store.\n\n    Args:\n        input_data (dict[str, Any]): Input data containing the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n\n    upserted_count = self.vector_store.write_documents(documents)\n    logger.debug(f\"Upserted {upserted_count} documents to Qdrant Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/nodes/writers/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/nodes/writers/weaviate/#dynamiq.nodes.writers.weaviate.WeaviateDocumentWriter","title":"<code>WeaviateDocumentWriter</code>","text":"<p>               Bases: <code>VectorStoreNode</code>, <code>BaseWriterVectorStoreParams</code></p> <p>Document Writer Node using Weaviate Vector Store.</p> <p>This class represents a node for writing documents to a Weaviate Vector Store.</p> <p>Attributes:</p> Name Type Description <code>group</code> <code>Literal[WRITERS]</code> <p>The group the node belongs to.</p> <code>name</code> <code>str</code> <p>The name of the node.</p> <code>connection</code> <code>Weaviate | None</code> <p>The Weaviate connection.</p> <code>vector_store</code> <code>WeaviateVectorStore | None</code> <p>The Weaviate Vector Store instance.</p> Source code in <code>dynamiq/nodes/writers/weaviate.py</code> <pre><code>class WeaviateDocumentWriter(VectorStoreNode, BaseWriterVectorStoreParams):\n    \"\"\"\n    Document Writer Node using Weaviate Vector Store.\n\n    This class represents a node for writing documents to a Weaviate Vector Store.\n\n    Attributes:\n        group (Literal[NodeGroup.WRITERS]): The group the node belongs to.\n        name (str): The name of the node.\n        connection (Weaviate | None): The Weaviate connection.\n        vector_store (WeaviateVectorStore | None): The Weaviate Vector Store instance.\n    \"\"\"\n\n    group: Literal[NodeGroup.WRITERS] = NodeGroup.WRITERS\n    name: str = \"WeaviateDocumentWriter\"\n    connection: Weaviate | None = None\n    vector_store: WeaviateVectorStore | None = None\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the WeaviateDocumentWriter.\n\n        If neither vector_store nor connection is provided in kwargs, a default Weaviate connection is created.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n            kwargs[\"connection\"] = Weaviate()\n        super().__init__(**kwargs)\n\n    @property\n    def vector_store_cls(self):\n        return WeaviateVectorStore\n\n    @property\n    def vector_store_params(self):\n        return self.model_dump(include=set(BaseWriterVectorStoreParams.model_fields)) | {\n            \"connection\": self.connection,\n            \"client\": self.client,\n        }\n\n    def execute(\n        self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n    ):\n        \"\"\"\n        Execute the document writing operation.\n\n        This method writes the input documents to the Weaviate Vector Store.\n\n        Args:\n            input_data (dict[str, Any]): Input data containing the documents to be written.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing the count of upserted documents.\n        \"\"\"\n        config = ensure_config(config)\n        self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n        documents = input_data[\"documents\"]\n\n        upserted_count = self.vector_store.write_documents(documents)\n        logger.debug(f\"Upserted {upserted_count} documents to Weaviate Vector Store.\")\n\n        return {\n            \"upserted_count\": upserted_count,\n        }\n</code></pre>"},{"location":"dynamiq/nodes/writers/weaviate/#dynamiq.nodes.writers.weaviate.WeaviateDocumentWriter.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the WeaviateDocumentWriter.</p> <p>If neither vector_store nor connection is provided in kwargs, a default Weaviate connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/nodes/writers/weaviate.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the WeaviateDocumentWriter.\n\n    If neither vector_store nor connection is provided in kwargs, a default Weaviate connection is created.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    if kwargs.get(\"vector_store\") is None and kwargs.get(\"connection\") is None:\n        kwargs[\"connection\"] = Weaviate()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"dynamiq/nodes/writers/weaviate/#dynamiq.nodes.writers.weaviate.WeaviateDocumentWriter.execute","title":"<code>execute(input_data, config=None, **kwargs)</code>","text":"<p>Execute the document writing operation.</p> <p>This method writes the input documents to the Weaviate Vector Store.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict[str, Any]</code> <p>Input data containing the documents to be written.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the count of upserted documents.</p> Source code in <code>dynamiq/nodes/writers/weaviate.py</code> <pre><code>def execute(\n    self, input_data: dict[str, Any], config: RunnableConfig = None, **kwargs\n):\n    \"\"\"\n    Execute the document writing operation.\n\n    This method writes the input documents to the Weaviate Vector Store.\n\n    Args:\n        input_data (dict[str, Any]): Input data containing the documents to be written.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the count of upserted documents.\n    \"\"\"\n    config = ensure_config(config)\n    self.run_on_node_execute_run(config.callbacks, **kwargs)\n\n    documents = input_data[\"documents\"]\n\n    upserted_count = self.vector_store.write_documents(documents)\n    logger.debug(f\"Upserted {upserted_count} documents to Weaviate Vector Store.\")\n\n    return {\n        \"upserted_count\": upserted_count,\n    }\n</code></pre>"},{"location":"dynamiq/prompts/prompts/","title":"Prompts","text":""},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for prompts.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the prompt, generated using generate_uuid by default.</p> <code>version</code> <code>str | None</code> <p>Version of the prompt, optional.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class BasePrompt(ABC, BaseModel):\n    \"\"\"\n    Abstract base class for prompts.\n\n    Attributes:\n        id (str): Unique identifier for the prompt, generated using generate_uuid by default.\n        version (str | None): Version of the prompt, optional.\n    \"\"\"\n\n    id: str = Field(default_factory=generate_uuid)\n    version: str | None = None\n\n    @abstractmethod\n    def format_messages(self, **kwargs) -&gt; list[dict]:\n        \"\"\"\n        Abstract method to format messages.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            list[dict]: A list of formatted messages as dictionaries.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def format_tools(self, **kwargs) -&gt; list[dict] | None:\n        \"\"\"\n        Abstract method to format tools.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            list[dict]: A list of formatted tools as dictionaries.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.BasePrompt.format_messages","title":"<code>format_messages(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format messages.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of formatted messages as dictionaries.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>@abstractmethod\ndef format_messages(self, **kwargs) -&gt; list[dict]:\n    \"\"\"\n    Abstract method to format messages.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        list[dict]: A list of formatted messages as dictionaries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.BasePrompt.format_tools","title":"<code>format_tools(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to format tools.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict] | None</code> <p>list[dict]: A list of formatted tools as dictionaries.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>@abstractmethod\ndef format_tools(self, **kwargs) -&gt; list[dict] | None:\n    \"\"\"\n    Abstract method to format tools.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        list[dict]: A list of formatted tools as dictionaries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a message in a conversation. Attributes:     content (str): The content of the message.     role (MessageRole): The role of the message sender.     metadata (dict | None): Additional metadata for the message, default is None.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    Represents a message in a conversation.\n    Attributes:\n        content (str): The content of the message.\n        role (MessageRole): The role of the message sender.\n        metadata (dict | None): Additional metadata for the message, default is None.\n    \"\"\"\n\n    content: str\n    role: MessageRole = MessageRole.USER\n    metadata: dict | None = None\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Prompt","title":"<code>Prompt</code>","text":"<p>               Bases: <code>BasePrompt</code></p> <p>Concrete implementation of BasePrompt for handling both text and vision messages.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[Message | VisionMessage]</code> <p>List of Message or VisionMessage objects</p> <code>tools</code> <code>list[dict[str, Any]]</code> <p>List of functions for which the model may generate JSON inputs.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class Prompt(BasePrompt):\n    \"\"\"\n    Concrete implementation of BasePrompt for handling both text and vision messages.\n\n    Attributes:\n        messages (list[Message | VisionMessage]): List of Message or VisionMessage objects\n        representing the prompt.\n        tools (list[dict[str, Any]]): List of functions for which the model may generate JSON inputs.\n    \"\"\"\n\n    messages: list[Message | VisionMessage]\n    tools: list[Tool] | None = None\n    _Template: Any = PrivateAttr()\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        # Import and initialize Jinja2 Template here\n        from jinja2 import Template\n\n        self._Template = Template\n\n    def format_messages(self, **kwargs) -&gt; list[dict]:\n        \"\"\"\n        Formats the messages in the prompt, rendering any templates.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments used for template rendering.\n\n        Returns:\n            list[dict]: A list of formatted messages as dictionaries.\n        \"\"\"\n        out: list[dict] = []\n        for msg in self.messages:\n            if isinstance(msg, Message):\n                out.append(\n                    Message(\n                        role=msg.role,\n                        content=self._Template(msg.content).render(**kwargs),\n                    ).model_dump(exclude={\"metadata\"})\n                )\n            elif isinstance(msg, VisionMessage):\n                out_msg_content = []\n                for content in msg.content:\n                    if isinstance(content, VisionMessageTextContent):\n                        out_msg_content.append(\n                            VisionMessageTextContent(\n                                text=self._Template(content.text).render(**kwargs),\n                            ).model_dump()\n                        )\n                    elif isinstance(content, VisionMessageImageContent):\n                        out_msg_content.append(\n                            VisionMessageImageContent(\n                                image_url=VisionMessageImageURL(\n                                    url=self._Template(content.image_url.url).render(**kwargs),\n                                    detail=content.image_url.detail,\n                                )\n                            ).model_dump()\n                        )\n                    else:\n                        raise ValueError(f\"Invalid content type: {content.type}\")\n                out_msg = {\n                    \"content\": out_msg_content,\n                    \"role\": msg.role,\n                }\n                out.append(out_msg)\n            else:\n                raise ValueError(f\"Invalid message type: {type(msg)}\")\n\n        return out\n\n    def format_tools(self, **kwargs) -&gt; list[dict] | None:\n        out = None\n        if self.tools:\n            out = [tool.model_dump() for tool in self.tools]\n        return out\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.Prompt.format_messages","title":"<code>format_messages(**kwargs)</code>","text":"<p>Formats the messages in the prompt, rendering any templates.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments used for template rendering.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of formatted messages as dictionaries.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; list[dict]:\n    \"\"\"\n    Formats the messages in the prompt, rendering any templates.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments used for template rendering.\n\n    Returns:\n        list[dict]: A list of formatted messages as dictionaries.\n    \"\"\"\n    out: list[dict] = []\n    for msg in self.messages:\n        if isinstance(msg, Message):\n            out.append(\n                Message(\n                    role=msg.role,\n                    content=self._Template(msg.content).render(**kwargs),\n                ).model_dump(exclude={\"metadata\"})\n            )\n        elif isinstance(msg, VisionMessage):\n            out_msg_content = []\n            for content in msg.content:\n                if isinstance(content, VisionMessageTextContent):\n                    out_msg_content.append(\n                        VisionMessageTextContent(\n                            text=self._Template(content.text).render(**kwargs),\n                        ).model_dump()\n                    )\n                elif isinstance(content, VisionMessageImageContent):\n                    out_msg_content.append(\n                        VisionMessageImageContent(\n                            image_url=VisionMessageImageURL(\n                                url=self._Template(content.image_url.url).render(**kwargs),\n                                detail=content.image_url.detail,\n                            )\n                        ).model_dump()\n                    )\n                else:\n                    raise ValueError(f\"Invalid content type: {content.type}\")\n            out_msg = {\n                \"content\": out_msg_content,\n                \"role\": msg.role,\n            }\n            out.append(out_msg)\n        else:\n            raise ValueError(f\"Invalid message type: {type(msg)}\")\n\n    return out\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessage","title":"<code>VisionMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a vision message in a conversation.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>list[VisionTextMessage | VisionImageMessage]</code> <p>The content of the message.</p> <code>role</code> <code>MessageRole</code> <p>The role of the message sender.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessage(BaseModel):\n    \"\"\"\n    Represents a vision message in a conversation.\n\n    Attributes:\n        content (list[VisionTextMessage | VisionImageMessage]): The content of the message.\n        role (MessageRole): The role of the message sender.\n    \"\"\"\n\n    content: list[VisionMessageTextContent | VisionMessageImageContent]\n    role: MessageRole = MessageRole.USER\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"\n        Converts the message to a dictionary.\n\n        Returns:\n            dict: The message as a dictionary.\n        \"\"\"\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessage.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Converts the message to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The message as a dictionary.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Converts the message to a dictionary.\n\n    Returns:\n        dict: The message as a dictionary.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageImageContent","title":"<code>VisionMessageImageContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an image message in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>VisionMessageType</code> <p>The type of the message, default is \"image_url\".</p> <code>image_url</code> <code>VisionMessageImageURL</code> <p>The image URL class.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageImageContent(BaseModel):\n    \"\"\"\n    Represents an image message in a vision conversation.\n\n    Attributes:\n        type (VisionMessageType): The type of the message, default is \"image_url\".\n        image_url (VisionMessageImageURL): The image URL class.\n    \"\"\"\n    type: VisionMessageType = VisionMessageType.IMAGE_URL\n    image_url: VisionMessageImageURL\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageImageURL","title":"<code>VisionMessageImageURL</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an image URL in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the image.</p> <code>detail</code> <code>VisionDetail</code> <p>The detail level of the image, default is \"auto\".</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageImageURL(BaseModel):\n    \"\"\"\n    Represents an image URL in a vision conversation.\n\n    Attributes:\n        url (str): The URL of the image.\n        detail (VisionDetail): The detail level of the image, default is \"auto\".\n    \"\"\"\n\n    url: str\n    detail: VisionDetail = VisionDetail.AUTO\n</code></pre>"},{"location":"dynamiq/prompts/prompts/#dynamiq.prompts.prompts.VisionMessageTextContent","title":"<code>VisionMessageTextContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a text message in a vision conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>VisionMessageType</code> <p>The type of the message, default is \"text\".</p> <code>text</code> <code>str</code> <p>The text content of the message.</p> Source code in <code>dynamiq/prompts/prompts.py</code> <pre><code>class VisionMessageTextContent(BaseModel):\n    \"\"\"\n    Represents a text message in a vision conversation.\n\n    Attributes:\n        type (VisionMessageType): The type of the message, default is \"text\".\n        text (str): The text content of the message.\n    \"\"\"\n\n    type: VisionMessageType = VisionMessageType.TEXT\n    text: str\n</code></pre>"},{"location":"dynamiq/runnables/base/","title":"Base","text":""},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.Runnable","title":"<code>Runnable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for runnable objects.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class Runnable(ABC):\n    \"\"\"\n    Abstract base class for runnable objects.\n    \"\"\"\n\n    @abstractmethod\n    def run(\n        self, input_data: Any, config: RunnableConfig = None, **kwargs\n    ) -&gt; RunnableResult:\n        \"\"\"\n        Abstract method to run the Runnable object.\n\n        Args:\n            input_data (Any): The input data for the execution.\n            config (RunnableConfig, optional): Configuration for the execution.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: The result of the execution.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.Runnable.run","title":"<code>run(input_data, config=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to run the Runnable object.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data for the execution.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the execution.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>The result of the execution.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>@abstractmethod\ndef run(\n    self, input_data: Any, config: RunnableConfig = None, **kwargs\n) -&gt; RunnableResult:\n    \"\"\"\n    Abstract method to run the Runnable object.\n\n    Args:\n        input_data (Any): The input data for the execution.\n        config (RunnableConfig, optional): Configuration for the execution.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: The result of the execution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableConfig","title":"<code>RunnableConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for Runnable objects.</p> <p>Attributes:</p> Name Type Description <code>callbacks</code> <code>list[BaseCallbackHandler]</code> <p>List of callback handlers.</p> <code>cache</code> <code>CacheConfig | None</code> <p>Cache configuration.</p> <code>max_node_workers</code> <code>int | None</code> <p>Maximum number of node workers.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableConfig(BaseModel):\n    \"\"\"\n    Configuration class for Runnable objects.\n\n    Attributes:\n        callbacks (list[BaseCallbackHandler]): List of callback handlers.\n        cache (CacheConfig | None): Cache configuration.\n        max_node_workers (int | None): Maximum number of node workers.\n    \"\"\"\n\n    run_id: str | None = Field(default_factory=generate_uuid)\n    callbacks: list[BaseCallbackHandler] = []\n    cache: CacheConfig | None = None\n    max_node_workers: int | None = None\n    nodes_override: dict[str, NodeRunnableConfig] = {}\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult","title":"<code>RunnableResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass representing the result of a Runnable execution.</p> <p>Attributes:</p> Name Type Description <code>status</code> <code>RunnableStatus</code> <p>The status of the execution.</p> <code>input</code> <code>Any</code> <p>The input data of the execution.</p> <code>output</code> <code>Any</code> <p>The output data of the execution.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableResult(BaseModel):\n    \"\"\"\n    Dataclass representing the result of a Runnable execution.\n\n    Attributes:\n        status (RunnableStatus): The status of the execution.\n        input (Any): The input data of the execution.\n        output (Any): The output data of the execution.\n    \"\"\"\n\n    status: RunnableStatus\n    input: Any = None\n    output: Any = None\n\n    def to_depend_dict(\n        self,\n        skip_format_types: set | None = None,\n        force_format_types: set | None = None,\n        **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResult instance to a dictionary that used as dependency context.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResult.\n        \"\"\"\n        if skip_format_types is None:\n            skip_format_types = set()\n        skip_format_types.update({BytesIO, BaseModel, bytes})\n\n        if force_format_types is None:\n            force_format_types = set()\n        force_format_types.add(RunnableResult)\n\n        return self.to_dict(skip_format_types, force_format_types, **kwargs)\n\n    def to_tracing_depend_dict(\n        self, skip_format_types: set | None = None, force_format_types: set | None = None, **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResult instance to a dictionary that used as dependency context in tracing.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResult.\n        \"\"\"\n\n        depend_dict = self.to_depend_dict(skip_format_types, force_format_types, **kwargs)\n        depend_dict.pop(\"input\", None)\n\n        return depend_dict\n\n    def to_dict(\n        self,\n        skip_format_types: set | None = None,\n        force_format_types: set | None = None,\n        **kwargs\n    ) -&gt; dict:\n        \"\"\"\n        Convert the RunnableResult instance to a dictionary.\n\n        Returns:\n            dict: A dictionary representation of the RunnableResult.\n        \"\"\"\n\n        return {\n            \"status\": self.status.value,\n            \"input\": format_value(self.input, skip_format_types, force_format_types),\n            \"output\": format_value(self.output, skip_format_types, force_format_types),\n        }\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult.to_depend_dict","title":"<code>to_depend_dict(skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Convert the RunnableResult instance to a dictionary that used as dependency context.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResult.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_depend_dict(\n    self,\n    skip_format_types: set | None = None,\n    force_format_types: set | None = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResult instance to a dictionary that used as dependency context.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResult.\n    \"\"\"\n    if skip_format_types is None:\n        skip_format_types = set()\n    skip_format_types.update({BytesIO, BaseModel, bytes})\n\n    if force_format_types is None:\n        force_format_types = set()\n    force_format_types.add(RunnableResult)\n\n    return self.to_dict(skip_format_types, force_format_types, **kwargs)\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult.to_dict","title":"<code>to_dict(skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Convert the RunnableResult instance to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResult.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_dict(\n    self,\n    skip_format_types: set | None = None,\n    force_format_types: set | None = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResult instance to a dictionary.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResult.\n    \"\"\"\n\n    return {\n        \"status\": self.status.value,\n        \"input\": format_value(self.input, skip_format_types, force_format_types),\n        \"output\": format_value(self.output, skip_format_types, force_format_types),\n    }\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableResult.to_tracing_depend_dict","title":"<code>to_tracing_depend_dict(skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Convert the RunnableResult instance to a dictionary that used as dependency context in tracing.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the RunnableResult.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>def to_tracing_depend_dict(\n    self, skip_format_types: set | None = None, force_format_types: set | None = None, **kwargs\n) -&gt; dict:\n    \"\"\"\n    Convert the RunnableResult instance to a dictionary that used as dependency context in tracing.\n\n    Returns:\n        dict: A dictionary representation of the RunnableResult.\n    \"\"\"\n\n    depend_dict = self.to_depend_dict(skip_format_types, force_format_types, **kwargs)\n    depend_dict.pop(\"input\", None)\n\n    return depend_dict\n</code></pre>"},{"location":"dynamiq/runnables/base/#dynamiq.runnables.base.RunnableStatus","title":"<code>RunnableStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of possible statuses for a Runnable object.</p> <p>Attributes:</p> Name Type Description <code>UNDEFINED</code> <p>Undefined status.</p> <code>FAILURE</code> <p>Failure status.</p> <code>SUCCESS</code> <p>Success status.</p> <code>SKIP</code> <p>Skip status.</p> Source code in <code>dynamiq/runnables/base.py</code> <pre><code>class RunnableStatus(Enum):\n    \"\"\"\n    Enumeration of possible statuses for a Runnable object.\n\n    Attributes:\n        UNDEFINED: Undefined status.\n        FAILURE: Failure status.\n        SUCCESS: Success status.\n        SKIP: Skip status.\n    \"\"\"\n\n    UNDEFINED = \"undefined\"\n    FAILURE = \"failure\"\n    SUCCESS = \"success\"\n    SKIP = \"skip\"\n</code></pre>"},{"location":"dynamiq/storages/vector/base/","title":"Base","text":""},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseVectorStoreParams","title":"<code>BaseVectorStoreParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base parameters for vector store.</p> <p>Attributes:</p> Name Type Description <code>index_name</code> <code>str</code> <p>Name of the index. Defaults to \"default\".</p> Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>class BaseVectorStoreParams(BaseModel):\n    \"\"\"Base parameters for vector store.\n\n    Attributes:\n        index_name (str): Name of the index. Defaults to \"default\".\n    \"\"\"\n    index_name: str = \"default\"\n</code></pre>"},{"location":"dynamiq/storages/vector/base/#dynamiq.storages.vector.base.BaseWriterVectorStoreParams","title":"<code>BaseWriterVectorStoreParams</code>","text":"<p>               Bases: <code>BaseVectorStoreParams</code></p> <p>Parameters for writer vector store.</p> <p>Attributes:</p> Name Type Description <code>create_if_not_exist</code> <code>bool</code> <p>Flag to create index if it does not exist. Defaults to True.</p> Source code in <code>dynamiq/storages/vector/base.py</code> <pre><code>class BaseWriterVectorStoreParams(BaseVectorStoreParams):\n    \"\"\"Parameters for writer vector store.\n\n    Attributes:\n        create_if_not_exist (bool): Flag to create index if it does not exist. Defaults to True.\n    \"\"\"\n\n    create_if_not_exist: bool = False\n</code></pre>"},{"location":"dynamiq/storages/vector/exceptions/","title":"Exceptions","text":""},{"location":"dynamiq/storages/vector/exceptions/#dynamiq.storages.vector.exceptions.VectorStoreDuplicateDocumentException","title":"<code>VectorStoreDuplicateDocumentException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when attempting to add a duplicate document to the vector store.</p> <p>This exception is thrown when a document with the same identifier or content is already present in the vector store and an attempt is made to add it again.</p> Source code in <code>dynamiq/storages/vector/exceptions.py</code> <pre><code>class VectorStoreDuplicateDocumentException(Exception):\n    \"\"\"\n    Exception raised when attempting to add a duplicate document to the vector store.\n\n    This exception is thrown when a document with the same identifier or content is already present\n    in the vector store and an attempt is made to add it again.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/exceptions/#dynamiq.storages.vector.exceptions.VectorStoreException","title":"<code>VectorStoreException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for vector store related errors.</p> <p>This exception is raised when a general error occurs in the vector store operations.</p> Source code in <code>dynamiq/storages/vector/exceptions.py</code> <pre><code>class VectorStoreException(Exception):\n    \"\"\"\n    Base exception class for vector store related errors.\n\n    This exception is raised when a general error occurs in the vector store operations.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/exceptions/#dynamiq.storages.vector.exceptions.VectorStoreFilterException","title":"<code>VectorStoreFilterException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when there's an error in filtering operations on the vector store.</p> <p>This exception is thrown when an invalid filter is applied or when there's an issue with the filtering process in the vector store.</p> Source code in <code>dynamiq/storages/vector/exceptions.py</code> <pre><code>class VectorStoreFilterException(Exception):\n    \"\"\"\n    Exception raised when there's an error in filtering operations on the vector store.\n\n    This exception is thrown when an invalid filter is applied or when there's an issue with the\n    filtering process in the vector store.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"dynamiq/storages/vector/policies/","title":"Policies","text":""},{"location":"dynamiq/storages/vector/policies/#dynamiq.storages.vector.policies.DuplicatePolicy","title":"<code>DuplicatePolicy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of policies for handling duplicate items.</p> <p>Attributes:</p> Name Type Description <code>NONE</code> <code>str</code> <p>No specific policy for handling duplicates.</p> <code>SKIP</code> <code>str</code> <p>Skip duplicate items without modifying existing ones.</p> <code>OVERWRITE</code> <code>str</code> <p>Overwrite existing items with duplicate entries.</p> <code>FAIL</code> <code>str</code> <p>Raise an error when encountering duplicate items.</p> Source code in <code>dynamiq/storages/vector/policies.py</code> <pre><code>class DuplicatePolicy(Enum):\n    \"\"\"\n    Enumeration of policies for handling duplicate items.\n\n    Attributes:\n        NONE (str): No specific policy for handling duplicates.\n        SKIP (str): Skip duplicate items without modifying existing ones.\n        OVERWRITE (str): Overwrite existing items with duplicate entries.\n        FAIL (str): Raise an error when encountering duplicate items.\n    \"\"\"\n\n    NONE = \"none\"\n    SKIP = \"skip\"\n    OVERWRITE = \"overwrite\"\n    FAIL = \"fail\"\n</code></pre>"},{"location":"dynamiq/storages/vector/utils/","title":"Utils","text":""},{"location":"dynamiq/storages/vector/utils/#dynamiq.storages.vector.utils.create_file_id_filter","title":"<code>create_file_id_filter(file_id)</code>","text":"<p>Create filters for Pinecone query based on file_id.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The filter conditions.</p> Source code in <code>dynamiq/storages/vector/utils.py</code> <pre><code>def create_file_id_filter(file_id: str) -&gt; dict:\n    \"\"\"\n    Create filters for Pinecone query based on file_id.\n\n    Args:\n        file_id (str): The file ID to filter by.\n\n    Returns:\n        dict: The filter conditions.\n    \"\"\"\n    return {\n        \"operator\": \"AND\",\n        \"conditions\": [\n            {\"field\": \"file_id\", \"operator\": \"==\", \"value\": file_id},\n        ],\n    }\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/","title":"Chroma","text":""},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore","title":"<code>ChromaVectorStore</code>","text":"<p>Vector store using Chroma.</p> <p>This class provides an interface to interact with a Chroma vector store for document storage and retrieval.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>ClientAPI</code> <p>The Chroma client API instance.</p> <code>index_name</code> <code>str</code> <p>The name of the index or collection in the vector store.</p> <code>_collection</code> <p>The Chroma collection object.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>class ChromaVectorStore:\n    \"\"\"\n    Vector store using Chroma.\n\n    This class provides an interface to interact with a Chroma vector store for document storage and\n    retrieval.\n\n    Attributes:\n        client (ClientAPI): The Chroma client API instance.\n        index_name (str): The name of the index or collection in the vector store.\n        _collection: The Chroma collection object.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection: Chroma | None = None,\n        client: Optional[\"ClientAPI\"] = None,\n        index_name: str = \"default\",\n        create_if_not_exist: bool = False,\n    ):\n        \"\"\"\n        Initialize the ChromaVectorStore.\n\n        Args:\n            connection (Chroma | None): A Chroma connection object. Defaults to None.\n            client (Optional[ClientAPI]): A Chroma client API instance. Defaults to None.\n            index_name (str): The name of the index or collection. Defaults to \"default\".\n        \"\"\"\n        self.client = client\n        if self.client is None:\n            connection = connection or Chroma()\n            self.client = connection.connect()\n        self.index_name = index_name\n        if create_if_not_exist:\n            self._collection = self.client.get_or_create_collection(name=index_name)\n        else:\n            self._collection = self.client.get_collection(name=index_name)\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Get the number of documents in the collection.\n\n        Returns:\n            int: The number of documents in the collection.\n        \"\"\"\n        return self._collection.count()\n\n    def write_documents(self, documents: list[Document]) -&gt; int:\n        \"\"\"\n        Write (or overwrite) documents into the store.\n\n        This method processes a list of documents and writes them into the vector store.\n\n        Args:\n            documents (list[Document]): A list of Document objects to be written into the document\n                store.\n\n        Raises:\n            ValueError: If an item in the documents list is not an instance of the Document class.\n\n        Returns:\n            int: The number of documents successfully written to the document store.\n        \"\"\"\n        for doc in documents:\n            if not isinstance(doc, Document):\n                msg = (\n                    \"param 'documents' must contain a list of objects of type Document\"\n                )\n                raise ValueError(msg)\n\n            data = {\"ids\": [doc.id], \"documents\": [doc.content]}\n\n            if doc.metadata:\n                data[\"metadatas\"] = [doc.metadata]\n\n            if doc.embedding:\n                data[\"embeddings\"] = [doc.embedding]\n\n            self._collection.add(**data)\n\n        return len(documents)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on their IDs.\n\n        Args:\n            document_ids (list[str]): A list containing the IDs of documents to be deleted from the store.\n            delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n        \"\"\"\n\n        if delete_all and self._collection is not None:\n            self.client.delete_collection(name=self.index_name)\n            self._collection = self.client.get_or_create_collection(\n                name=self.index_name\n            )\n        else:\n            if not document_ids:\n                logger.warning(\n                    \"No document IDs provided. No documents will be deleted.\"\n                )\n            else:\n                self._collection.delete(ids=document_ids)\n\n    def delete_documents_by_filters(\n        self, filters: dict[str, Any] | None = None\n    ) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided filters.\n\n        Args:\n            filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n                None.\n        \"\"\"\n        if filters is None:\n            raise ValueError(\"No filters provided. No documents will be deleted.\")\n        else:\n            ids, where, where_document = self._normalize_filters(filters)\n            self._collection.delete(ids=ids, where=where, where_document=where_document)\n\n    def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided file ID.\n            file_id should be located in the metadata of the document.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def search_embeddings(\n        self,\n        query_embeddings: list[list[float]],\n        top_k: int,\n        filters: dict[str, Any] | None = None,\n    ) -&gt; list[list[Document]]:\n        \"\"\"\n        Perform vector search on the stored documents using query embeddings.\n\n        Args:\n            query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n            top_k (int): The maximum number of documents to retrieve.\n            filters (dict[str, Any] | None): A dictionary of filters to apply to the search.\n                Defaults to None.\n\n        Returns:\n            list[list[Document]]: A list of lists containing documents that match the given filters,\n                for each query embedding provided.\n        \"\"\"\n        if filters is None:\n            results = self._collection.query(\n                query_embeddings=query_embeddings,\n                n_results=top_k,\n                include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n            )\n        else:\n            chroma_filters = self._normalize_filters(filters=filters)\n            results = self._collection.query(\n                query_embeddings=query_embeddings,\n                n_results=top_k,\n                where=chroma_filters[1],\n                where_document=chroma_filters[2],\n                include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n            )\n\n        return self._query_result_to_documents(results)\n\n    def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents that match the provided filters.\n\n        Filters can be defined in two formats:\n        1. Old format: Nested dictionaries with logical operators and comparison operators.\n        2. New format: Nested dictionaries of Comparison and Logic types.\n\n        For the new format:\n        Comparison dictionaries must contain the following keys:\n        - 'field': The name of one of the metadata fields of a document (e.g., 'metadata.years').\n        - 'operator': One of '==', '!=', '&gt;', '&gt;=', '&lt;', '&lt;=', 'in', 'not in'.\n        - 'value': A single value or (for 'in' and 'not in') a list of values.\n\n        Logic dictionaries must contain the following keys:\n        - 'operator': One of 'NOT', 'OR', 'AND'.\n        - 'conditions': A list of Comparison or Logic dictionaries.\n\n        Example of new format:\n        {\n            \"operator\": \"AND\",\n            \"conditions\": [\n              {\n                \"field\": \"metadata.years\",\n                \"operator\": \"==\",\n                \"value\": \"2019\"\n              },\n              {\n                \"field\": \"metadata.companies\",\n                \"operator\": \"in\",\n                \"value\": [\"BMW\", \"Mercedes\"]\n              }\n            ]\n        }\n\n        Args:\n            filters (Dict[str, Any] | None): The filters to apply to the document list.\n            filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n                None.\n\n        Returns:\n            list[Document]: A list of Document instances that match the given filters.\n        \"\"\"\n        if filters:\n            ids, where, where_document = self._normalize_filters(filters)\n            kwargs: dict[str, Any] = {\"where\": where}\n\n            if ids:\n                kwargs[\"ids\"] = ids\n            if where_document:\n                kwargs[\"where_document\"] = where_document\n\n            result = self._collection.get(**kwargs)\n        else:\n            raise ValueError(\n                \"No filters provided. No documents will be retrieved with filters.\"\n            )\n\n        return self._get_result_to_documents(result)\n\n    def list_documents(self) -&gt; list[Document]:\n        \"\"\"\n        List all documents in the collection.\n\n        Returns:\n            list[Document]: A list of Document instances representing all documents in the collection.\n        \"\"\"\n        result = self._collection.get()\n        return self._get_result_to_documents(result)\n\n    @staticmethod\n    def _normalize_filters(\n        filters: dict[str, Any]\n    ) -&gt; tuple[list[str], dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Translate filters to Chroma filters.\n\n        Args:\n            filters (Dict[str, Any]): The filters to normalize.\n\n        Returns:\n            Tuple[List[str], Dict[str, Any], Dict[str, Any]]: A tuple containing:\n                - A list of document IDs\n                - A dictionary of 'where' conditions\n                - A dictionary of 'where_document' conditions\n\n        Raises:\n            TypeError: If the 'filters' parameter is not a dictionary.\n            ValueError: If the filter structure is invalid or contains unsupported operators.\n        \"\"\"\n        if not isinstance(filters, dict):\n            raise TypeError(\"'filters' parameter must be a dictionary\")\n\n        # Check if it's the new format\n        if \"operator\" in filters or \"conditions\" in filters:\n            processed_filters = ChromaVectorStore._process_filter_node(filters)\n        else:\n            # It's the old format, use the old processing method\n            return ChromaVectorStore._process_old_filters(filters)\n\n        ids = []\n        where_document = {}\n\n        # Extract 'id' and 'content' filters if present\n        if \"metadata.id\" in processed_filters:\n            ids = processed_filters[\"metadata.id\"].get(\"$eq\", [])\n            del processed_filters[\"metadata.id\"]\n\n        if \"content\" in processed_filters:\n            where_document[\"$contains\"] = processed_filters[\"content\"].get(\"$eq\", \"\")\n            del processed_filters[\"content\"]\n\n        where = processed_filters\n\n        if \"$and\" in where and \"$or\" not in where:\n            and_conditions = where[\"$and\"]\n            if len(and_conditions) == 1:\n                where = and_conditions[0]\n        if \"$or\" in where and \"$and\" not in where:\n            or_conditions = where[\"$or\"]\n            if len(or_conditions) == 1:\n                where = or_conditions[0]\n\n        try:\n            if where:\n                from chromadb.api.types import validate_where\n\n                validate_where(where)\n            if where_document:\n                from chromadb.api.types import validate_where_document\n\n                validate_where_document(where_document)\n        except ValueError as e:\n            raise ValueError(e) from e\n\n        return ids, where, where_document\n\n    @staticmethod\n    def _process_old_filters(\n        filters: dict[str, Any]\n    ) -&gt; tuple[list[str], dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Process filters in the old format.\n        \"\"\"\n        ids = []\n        where = defaultdict(list)\n        where_document = defaultdict(list)\n        keys_to_remove = []\n\n        for field, value in filters.items():\n            if field == \"content\":\n                keys_to_remove.append(field)\n                where_document[\"$contains\"] = value\n            elif field == \"id\":\n                keys_to_remove.append(field)\n                ids.append(value)\n            elif isinstance(value, (list, tuple)):\n                keys_to_remove.append(field)\n                if len(value) == 0:\n                    continue\n                if len(value) == 1:\n                    where[field] = value[0]\n                    continue\n                for v in value:\n                    where[\"$or\"].append({field: v})\n\n        for k in keys_to_remove:\n            del filters[k]\n\n        final_where = dict(filters)\n        final_where.update(dict(where))\n\n        return ids, final_where, dict(where_document)\n\n    @staticmethod\n    def _process_filter_node(node: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process a single node in the filter structure.\n\n        Args:\n            node (Dict[str, Any]): A dictionary representing a filter node.\n\n        Returns:\n            Dict[str, Any]: The processed filter node.\n\n        Raises:\n            ValueError: If the node structure is invalid.\n        \"\"\"\n        if \"operator\" in node and \"conditions\" in node:  # Logic node\n            return ChromaVectorStore._process_logic_node(node)\n        elif (\n            \"field\" in node and \"operator\" in node and \"value\" in node\n        ):  # Comparison node\n            return ChromaVectorStore._process_comparison_node(node)\n        else:\n            raise ValueError(\"Invalid filter node structure\")\n\n    @staticmethod\n    def _process_logic_node(node: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process a logic node in the filter structure.\n\n        Args:\n            node (Dict[str, Any]): A dictionary representing a logic node.\n\n        Returns:\n            Dict[str, Any]: The processed logic node.\n        \"\"\"\n        operator = node[\"operator\"].lower()\n        conditions = [\n            ChromaVectorStore._process_filter_node(condition)\n            for condition in node[\"conditions\"]\n        ]\n        return {f\"${operator}\": conditions}\n\n    @staticmethod\n    def _process_comparison_node(node: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Process a comparison node in the filter structure.\n\n        Args:\n            node (Dict[str, Any]): A dictionary representing a comparison node.\n\n        Returns:\n            Dict[str, Any]: The processed comparison node.\n\n        Raises:\n            ValueError: If the operator is unsupported.\n        \"\"\"\n        field = node[\"field\"]\n        operator = node[\"operator\"]\n        value = node[\"value\"]\n\n        chroma_operator = CHROMA_OPERATOR_MAPPING.get(operator)\n\n        if chroma_operator is None:\n            raise ValueError(f\"Unsupported operator: {operator}\")\n\n        return {field: {chroma_operator: value}}\n\n    @staticmethod\n    def _query_result_to_documents(result: \"QueryResult\") -&gt; list[list[Document]]:\n        \"\"\"\n        Convert Chroma query results into Dynamiq Documents.\n\n        Args:\n            result (QueryResult): The result from a Chroma query operation.\n\n        Returns:\n            list[list[Document]]: A list of lists containing Document objects created from the\n                Chroma query result.\n        \"\"\"\n        return_value: list[list[Document]] = []\n        documents = result.get(\"documents\")\n        if documents is None:\n            return return_value\n\n        for i, answers in enumerate(documents):\n            converted_answers = []\n            for j in range(len(answers)):\n                document_dict: dict[str, Any] = {\n                    \"id\": result[\"ids\"][i][j],\n                    \"content\": documents[i][j],\n                }\n\n                if metadatas := result.get(\"metadatas\"):\n                    document_dict[\"metadata\"] = dict(metadatas[i][j])\n\n                if embeddings := result.get(\"embeddings\"):\n                    document_dict[\"embedding\"] = embeddings[i][j]\n\n                if distances := result.get(\"distances\"):\n                    document_dict[\"score\"] = distances[i][j]\n\n                converted_answers.append(Document(**document_dict))\n            return_value.append(converted_answers)\n\n        return return_value\n\n    @staticmethod\n    def _get_result_to_documents(result: \"QueryResult\") -&gt; list[Document]:\n        \"\"\"\n        Convert Chroma get result into Dynamiq Documents.\n\n        Args:\n            result (GetResult): The result from a Chroma get operation.\n\n        Returns:\n            list[Document]: A list containing Document objects created from the\n                Chroma get result.\n        \"\"\"\n        return_value: list[Document] = []\n        documents = result.get(\"documents\")\n        if documents is None:\n            return return_value\n\n        for i, content in enumerate(documents):\n            document_dict: dict[str, Any] = {\n                \"id\": result[\"ids\"][i],\n                \"content\": content,\n            }\n\n            if metadatas := result.get(\"metadatas\"):\n                document_dict[\"metadata\"] = dict(metadatas[i])\n\n            if embeddings := result.get(\"embeddings\"):\n                document_dict[\"embedding\"] = embeddings[i]\n\n            if distances := result.get(\"distances\"):\n                document_dict[\"score\"] = distances[i]\n\n            return_value.append(Document(**document_dict))\n        return return_value\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', create_if_not_exist=False)</code>","text":"<p>Initialize the ChromaVectorStore.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Chroma | None</code> <p>A Chroma connection object. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[ClientAPI]</code> <p>A Chroma client API instance. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>The name of the index or collection. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def __init__(\n    self,\n    connection: Chroma | None = None,\n    client: Optional[\"ClientAPI\"] = None,\n    index_name: str = \"default\",\n    create_if_not_exist: bool = False,\n):\n    \"\"\"\n    Initialize the ChromaVectorStore.\n\n    Args:\n        connection (Chroma | None): A Chroma connection object. Defaults to None.\n        client (Optional[ClientAPI]): A Chroma client API instance. Defaults to None.\n        index_name (str): The name of the index or collection. Defaults to \"default\".\n    \"\"\"\n    self.client = client\n    if self.client is None:\n        connection = connection or Chroma()\n        self.client = connection.connect()\n    self.index_name = index_name\n    if create_if_not_exist:\n        self._collection = self.client.get_or_create_collection(name=index_name)\n    else:\n        self._collection = self.client.get_collection(name=index_name)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Get the number of documents in the collection.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the collection.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Get the number of documents in the collection.\n\n    Returns:\n        int: The number of documents in the collection.\n    \"\"\"\n    return self._collection.count()\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the vector store based on their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>A list containing the IDs of documents to be deleted from the store.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>A flag to delete all documents from the store. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on their IDs.\n\n    Args:\n        document_ids (list[str]): A list containing the IDs of documents to be deleted from the store.\n        delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n    \"\"\"\n\n    if delete_all and self._collection is not None:\n        self.client.delete_collection(name=self.index_name)\n        self._collection = self.client.get_or_create_collection(\n            name=self.index_name\n        )\n    else:\n        if not document_ids:\n            logger.warning(\n                \"No document IDs provided. No documents will be deleted.\"\n            )\n        else:\n            self._collection.delete(ids=document_ids)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the vector store based on the provided file ID.     file_id should be located in the metadata of the document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided file ID.\n        file_id should be located in the metadata of the document.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters=None)</code>","text":"<p>Delete documents from the vector store based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | None</code> <p>The filters to apply to the document list. Defaults to None.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def delete_documents_by_filters(\n    self, filters: dict[str, Any] | None = None\n) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided filters.\n\n    Args:\n        filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n            None.\n    \"\"\"\n    if filters is None:\n        raise ValueError(\"No filters provided. No documents will be deleted.\")\n    else:\n        ids, where, where_document = self._normalize_filters(filters)\n        self._collection.delete(ids=ids, where=where, where_document=where_document)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.filter_documents","title":"<code>filter_documents(filters=None)</code>","text":"<p>Retrieve documents that match the provided filters.</p> <p>Filters can be defined in two formats: 1. Old format: Nested dictionaries with logical operators and comparison operators. 2. New format: Nested dictionaries of Comparison and Logic types.</p> <p>For the new format: Comparison dictionaries must contain the following keys: - 'field': The name of one of the metadata fields of a document (e.g., 'metadata.years'). - 'operator': One of '==', '!=', '&gt;', '&gt;=', '&lt;', '&lt;=', 'in', 'not in'. - 'value': A single value or (for 'in' and 'not in') a list of values.</p> <p>Logic dictionaries must contain the following keys: - 'operator': One of 'NOT', 'OR', 'AND'. - 'conditions': A list of Comparison or Logic dictionaries.</p> <p>Example of new format: {     \"operator\": \"AND\",     \"conditions\": [       {         \"field\": \"metadata.years\",         \"operator\": \"==\",         \"value\": \"2019\"       },       {         \"field\": \"metadata.companies\",         \"operator\": \"in\",         \"value\": [\"BMW\", \"Mercedes\"]       }     ] }</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict[str, Any] | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <code>filters</code> <code>dict[str, Any] | None</code> <p>The filters to apply to the document list. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Document instances that match the given filters.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n    \"\"\"\n    Retrieve documents that match the provided filters.\n\n    Filters can be defined in two formats:\n    1. Old format: Nested dictionaries with logical operators and comparison operators.\n    2. New format: Nested dictionaries of Comparison and Logic types.\n\n    For the new format:\n    Comparison dictionaries must contain the following keys:\n    - 'field': The name of one of the metadata fields of a document (e.g., 'metadata.years').\n    - 'operator': One of '==', '!=', '&gt;', '&gt;=', '&lt;', '&lt;=', 'in', 'not in'.\n    - 'value': A single value or (for 'in' and 'not in') a list of values.\n\n    Logic dictionaries must contain the following keys:\n    - 'operator': One of 'NOT', 'OR', 'AND'.\n    - 'conditions': A list of Comparison or Logic dictionaries.\n\n    Example of new format:\n    {\n        \"operator\": \"AND\",\n        \"conditions\": [\n          {\n            \"field\": \"metadata.years\",\n            \"operator\": \"==\",\n            \"value\": \"2019\"\n          },\n          {\n            \"field\": \"metadata.companies\",\n            \"operator\": \"in\",\n            \"value\": [\"BMW\", \"Mercedes\"]\n          }\n        ]\n    }\n\n    Args:\n        filters (Dict[str, Any] | None): The filters to apply to the document list.\n        filters (dict[str, Any] | None): The filters to apply to the document list. Defaults to\n            None.\n\n    Returns:\n        list[Document]: A list of Document instances that match the given filters.\n    \"\"\"\n    if filters:\n        ids, where, where_document = self._normalize_filters(filters)\n        kwargs: dict[str, Any] = {\"where\": where}\n\n        if ids:\n            kwargs[\"ids\"] = ids\n        if where_document:\n            kwargs[\"where_document\"] = where_document\n\n        result = self._collection.get(**kwargs)\n    else:\n        raise ValueError(\n            \"No filters provided. No documents will be retrieved with filters.\"\n        )\n\n    return self._get_result_to_documents(result)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.list_documents","title":"<code>list_documents()</code>","text":"<p>List all documents in the collection.</p> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Document instances representing all documents in the collection.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def list_documents(self) -&gt; list[Document]:\n    \"\"\"\n    List all documents in the collection.\n\n    Returns:\n        list[Document]: A list of Document instances representing all documents in the collection.\n    \"\"\"\n    result = self._collection.get()\n    return self._get_result_to_documents(result)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.search_embeddings","title":"<code>search_embeddings(query_embeddings, top_k, filters=None)</code>","text":"<p>Perform vector search on the stored documents using query embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>query_embeddings</code> <code>list[list[float]]</code> <p>A list of embeddings to use as queries.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> required <code>filters</code> <code>dict[str, Any] | None</code> <p>A dictionary of filters to apply to the search. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Document]]</code> <p>list[list[Document]]: A list of lists containing documents that match the given filters, for each query embedding provided.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def search_embeddings(\n    self,\n    query_embeddings: list[list[float]],\n    top_k: int,\n    filters: dict[str, Any] | None = None,\n) -&gt; list[list[Document]]:\n    \"\"\"\n    Perform vector search on the stored documents using query embeddings.\n\n    Args:\n        query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n        top_k (int): The maximum number of documents to retrieve.\n        filters (dict[str, Any] | None): A dictionary of filters to apply to the search.\n            Defaults to None.\n\n    Returns:\n        list[list[Document]]: A list of lists containing documents that match the given filters,\n            for each query embedding provided.\n    \"\"\"\n    if filters is None:\n        results = self._collection.query(\n            query_embeddings=query_embeddings,\n            n_results=top_k,\n            include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n        )\n    else:\n        chroma_filters = self._normalize_filters(filters=filters)\n        results = self._collection.query(\n            query_embeddings=query_embeddings,\n            n_results=top_k,\n            where=chroma_filters[1],\n            where_document=chroma_filters[2],\n            include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"],\n        )\n\n    return self._query_result_to_documents(results)\n</code></pre>"},{"location":"dynamiq/storages/vector/chroma/chroma/#dynamiq.storages.vector.chroma.chroma.ChromaVectorStore.write_documents","title":"<code>write_documents(documents)</code>","text":"<p>Write (or overwrite) documents into the store.</p> <p>This method processes a list of documents and writes them into the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>A list of Document objects to be written into the document store.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an item in the documents list is not an instance of the Document class.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents successfully written to the document store.</p> Source code in <code>dynamiq/storages/vector/chroma/chroma.py</code> <pre><code>def write_documents(self, documents: list[Document]) -&gt; int:\n    \"\"\"\n    Write (or overwrite) documents into the store.\n\n    This method processes a list of documents and writes them into the vector store.\n\n    Args:\n        documents (list[Document]): A list of Document objects to be written into the document\n            store.\n\n    Raises:\n        ValueError: If an item in the documents list is not an instance of the Document class.\n\n    Returns:\n        int: The number of documents successfully written to the document store.\n    \"\"\"\n    for doc in documents:\n        if not isinstance(doc, Document):\n            msg = (\n                \"param 'documents' must contain a list of objects of type Document\"\n            )\n            raise ValueError(msg)\n\n        data = {\"ids\": [doc.id], \"documents\": [doc.content]}\n\n        if doc.metadata:\n            data[\"metadatas\"] = [doc.metadata]\n\n        if doc.embedding:\n            data[\"embeddings\"] = [doc.embedding]\n\n        self._collection.add(**data)\n\n    return len(documents)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/","title":"Filter","text":""},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter","title":"<code>Filter</code>","text":"Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>class Filter:\n    LOGICAL_OPERATORS = {\"AND\": \" and \", \"OR\": \" or \", \"NOT\": \"not \"}\n    COMPARISON_OPERATORS = {\n        \"==\": \"==\",\n        \"!=\": \"!=\",\n        \"&gt;\": \"&gt;\",\n        \"&gt;=\": \"&gt;=\",\n        \"&lt;\": \"&lt;\",\n        \"&lt;=\": \"&lt;=\",\n        \"in\": \"in\",\n        \"not in\": \"not in\",\n    }\n\n    def __init__(self, filter_criteria: dict[str, Any]):\n        \"\"\"\n        Initializes the Filter object with filter criteria.\n\n        Args:\n            filter_criteria (Dict[str, Any]): The filters to apply.\n        \"\"\"\n        self.filter_criteria = filter_criteria\n\n    def build_filter_expression(self) -&gt; str:\n        \"\"\"\n        Builds the filter expression string from the filter criteria.\n\n        Returns:\n            str: The constructed filter expression string compatible with the database.\n        \"\"\"\n        return self._parse_filter(self.filter_criteria)\n\n    def _parse_filter(self, filter_term: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Recursively parse the filter criteria to build a Milvus-compatible filter expression.\n\n        Args:\n            filter_term (Dict[str, Any]): The filter dictionary to parse.\n\n        Returns:\n            str: A Milvus-compatible filter expression.\n        \"\"\"\n        # Handle logical operators with nested conditions\n        if \"operator\" in filter_term and \"conditions\" in filter_term:\n            operator = filter_term[\"operator\"]\n            if operator not in self.LOGICAL_OPERATORS:\n                raise ValueError(f\"Unsupported logical operator: {operator}\")\n\n            # Process each condition recursively\n            sub_expressions = [self._parse_filter(cond) for cond in filter_term[\"conditions\"]]\n            return f\"({self.LOGICAL_OPERATORS[operator].join(sub_expressions)})\"\n\n        # Handle comparison conditions\n        elif \"field\" in filter_term and \"operator\" in filter_term and \"value\" in filter_term:\n            field = filter_term[\"field\"]\n            operator = filter_term[\"operator\"]\n            value = filter_term[\"value\"]\n\n            # Build comparison expression\n            return self._build_comparison_expression(field, operator, value)\n\n        else:\n            raise ValueError(\"Invalid filter structure\")\n\n    def _build_comparison_expression(self, field: str, operator: str, value: Any) -&gt; str:\n        \"\"\"\n        Constructs a comparison expression based on field, operator, and value.\n\n        Args:\n            field (str): The field to filter on.\n            operator (str): The comparison operator.\n            value (Any): The value to compare against.\n\n        Returns:\n            str: A Milvus-compatible comparison expression.\n        \"\"\"\n        if operator not in self.COMPARISON_OPERATORS:\n            raise ValueError(f\"Unsupported comparison operator: {operator}\")\n\n        if operator == \"in\" and isinstance(value, list):\n            return f\"{field} in {value}\"\n        elif operator == \"not in\" and isinstance(value, list):\n            return f\"{field} not in {value}\"\n        elif isinstance(value, str):\n            return f'{field} {self.COMPARISON_OPERATORS[operator]} \"{value}\"'\n        else:\n            return f\"{field} {self.COMPARISON_OPERATORS[operator]} {value}\"\n\n    @staticmethod\n    def from_dict(filter_dict: dict[str, Any]) -&gt; \"Filter\":\n        \"\"\"\n        Creates a Filter instance from a dictionary.\n\n        Args:\n            filter_dict (Dict[str, Any]): Dictionary defining filter criteria.\n\n        Returns:\n            Filter: The Filter instance.\n        \"\"\"\n        return Filter(filter_dict)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter.__init__","title":"<code>__init__(filter_criteria)</code>","text":"<p>Initializes the Filter object with filter criteria.</p> <p>Parameters:</p> Name Type Description Default <code>filter_criteria</code> <code>Dict[str, Any]</code> <p>The filters to apply.</p> required Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>def __init__(self, filter_criteria: dict[str, Any]):\n    \"\"\"\n    Initializes the Filter object with filter criteria.\n\n    Args:\n        filter_criteria (Dict[str, Any]): The filters to apply.\n    \"\"\"\n    self.filter_criteria = filter_criteria\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter.build_filter_expression","title":"<code>build_filter_expression()</code>","text":"<p>Builds the filter expression string from the filter criteria.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The constructed filter expression string compatible with the database.</p> Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>def build_filter_expression(self) -&gt; str:\n    \"\"\"\n    Builds the filter expression string from the filter criteria.\n\n    Returns:\n        str: The constructed filter expression string compatible with the database.\n    \"\"\"\n    return self._parse_filter(self.filter_criteria)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/filter/#dynamiq.storages.vector.milvus.filter.Filter.from_dict","title":"<code>from_dict(filter_dict)</code>  <code>staticmethod</code>","text":"<p>Creates a Filter instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>filter_dict</code> <code>Dict[str, Any]</code> <p>Dictionary defining filter criteria.</p> required <p>Returns:</p> Name Type Description <code>Filter</code> <code>Filter</code> <p>The Filter instance.</p> Source code in <code>dynamiq/storages/vector/milvus/filter.py</code> <pre><code>@staticmethod\ndef from_dict(filter_dict: dict[str, Any]) -&gt; \"Filter\":\n    \"\"\"\n    Creates a Filter instance from a dictionary.\n\n    Args:\n        filter_dict (Dict[str, Any]): Dictionary defining filter criteria.\n\n    Returns:\n        Filter: The Filter instance.\n    \"\"\"\n    return Filter(filter_dict)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/","title":"Milvus","text":""},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore","title":"<code>MilvusVectorStore</code>","text":"<p>Vector store using Milvus.</p> <p>This class can be used with Zilliz Cloud Services or self-hosted instances.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>class MilvusVectorStore:\n    \"\"\"\n    Vector store using Milvus.\n\n    This class can be used with Zilliz Cloud Services or self-hosted instances.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        connection: Milvus | None = None,\n        client: Optional[\"MilvusClient\"] = None,\n        index_name: str = \"default\",\n        metric_type: str = \"COSINE\",\n        index_type: str = \"AUTOINDEX\",\n        dimension: int = 1536,\n        create_if_not_exist: bool = False,\n    ):\n        self.client = client\n        if self.client is None:\n            connection = connection or Milvus()\n            self.client = connection.connect()\n        self.index_name = index_name\n        self.metric_type = metric_type\n        self.index_type = index_type\n        self.dimension = dimension\n        self.create_if_not_exist = create_if_not_exist\n        self.schema = self.client.create_schema(\n            auto_id=False,\n            enable_dynamic_field=True,\n        )\n        self.schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, is_primary=True, max_length=65_535)\n        self.schema.add_field(field_name=\"content\", datatype=DataType.VARCHAR, max_length=65_535)\n        self.schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=self.dimension)\n\n        self.index_params = self.client.prepare_index_params()\n        self.index_params.add_index(field_name=\"id\")\n        self.index_params.add_index(field_name=\"vector\", index_type=self.index_type, metric_type=self.metric_type)\n\n        if not self.client.has_collection(self.index_name):\n            if self.create_if_not_exist:\n                logger.info(f\"Collection {self.index_name} does not exist. Creating a new collection.\")\n                self.client.create_collection(\n                    collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n                )\n            else:\n                raise ValueError(\n                    f\"Collection {self.index_name} does not exist. Set 'create_if_not_exist' to True to create it.\"\n                )\n        else:\n            logger.info(f\"Collection {self.index_name} already exists. Skipping creation.\")\n\n        self.client.load_collection(self.index_name)\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Get the number of documents in the collection.\n\n        Returns:\n            int: The number of documents in the collection.\n        \"\"\"\n        return self.client.get_collection_stats(self.index_name)[\"row_count\"]\n\n    def write_documents(self, documents: list[Document]) -&gt; int:\n        \"\"\"\n        Write (or overwrite) documents into the Milvus store.\n\n        This method processes a list of Document objects and writes them into the vector store.\n\n        Args:\n            documents (List[Document]): A list of Document objects to be written into the document store.\n\n        Raises:\n            ValueError: If an item in the documents list is not an instance of the Document class.\n\n        Returns:\n            int: The number of documents successfully written to the document store.\n        \"\"\"\n        data_to_upsert = []\n\n        for doc in documents:\n            if not isinstance(doc, Document):\n                raise ValueError(\"All items in 'documents' must be of type Document.\")\n\n            document_data = {\n                \"id\": doc.id,\n                \"vector\": doc.embedding,\n                \"content\": doc.content,\n            }\n\n            if doc.metadata:\n                document_data.update(doc.metadata)\n\n            data_to_upsert.append(document_data)\n\n        response = self.client.upsert(\n            collection_name=self.index_name,\n            data=data_to_upsert,\n        )\n        return response[\"upsert_count\"]\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the Milvus vector store based on their IDs.\n\n        Args:\n            document_ids (List[str]): A list containing the IDs of documents to be deleted from the store.\n            delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n\n        Raises:\n            ValueError: If neither document_ids nor delete_all is provided.\n        \"\"\"\n        if delete_all:\n            self.client.drop_collection(collection_name=self.index_name)\n            self.client.create_collection(\n                collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n            )\n            self.client.load_collection(self.index_name)\n            logger.info(f\"All documents in the collection {self.index_name} have been deleted.\")\n        elif document_ids:\n            response = self.client.delete(collection_name=self.index_name, ids=document_ids)\n            logger.info(f\"Deleted {len(response)} documents from collection {self.index_name}.\")\n        else:\n            raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents based on filters.\n\n        Args:\n            filters (Dict[str, Any]): Filter criteria for deleting documents.\n        \"\"\"\n        if not filters:\n            raise ValueError(\"Filters must be provided to delete documents.\")\n\n        filter_expression = Filter(filters).build_filter_expression()\n\n        delete_result = self.client.delete(collection_name=self.index_name, filter=filter_expression)\n\n        logger.info(f\"Deleted {len(delete_result)} entities from collection {self.index_name} based on filters.\")\n\n    def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n        \"\"\"\n        Delete documents from the vector store based on the provided file ID.\n            file_id should be located in the metadata of the document.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def list_documents(self, limit: int = 1000) -&gt; list[Document]:\n        \"\"\"\n        List all documents in the collection up to a specified limit.\n\n        Args:\n            limit (int): Maximum number of documents to retrieve. Defaults to 1000.\n\n        Returns:\n            List[Document]: A list of Document instances representing all documents in the collection.\n        \"\"\"\n        if not self.client.has_collection(self.index_name):\n            raise ValueError(f\"Collection '{self.index_name}' does not exist.\")\n\n        collection_info = self.client.describe_collection(collection_name=self.index_name)\n\n        output_fields = [field[\"name\"] for field in collection_info[\"fields\"]]\n\n        result = self.client.query(collection_name=self.index_name, filter=\"\", output_fields=output_fields, limit=limit)\n\n        return self._get_result_to_documents(result)\n\n    def search_embeddings(\n        self, query_embeddings: list[list[float]], top_k: int, filters: dict[str, Any] | None = None\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform vector search on the stored documents using query embeddings.\n\n        Args:\n            query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n            top_k (int): The maximum number of documents to retrieve.\n            filters (dict[str, Any] | None): A dictionary of filters to apply to the search. Defaults to None.\n\n        Returns:\n            List[Document]: A list of Document objects containing the retrieved documents.\n        \"\"\"\n        search_params = {\"metric_type\": self.metric_type, \"params\": {}}\n\n        filter_expression = Filter(filters).build_filter_expression() if filters else \"\"\n\n        results = self.client.search(\n            collection_name=self.index_name,\n            data=query_embeddings,\n            limit=top_k,\n            filter=filter_expression,\n            output_fields=[\"*\"],\n            search_params=search_params,\n        )\n\n        return self._convert_query_result_to_documents(results[0])\n\n    @staticmethod\n    def _convert_query_result_to_documents(result: list[dict[str, Any]]) -&gt; list[Document]:\n        \"\"\"\n        Convert Milvus search results to Document objects.\n\n        Args:\n            result (List[Dict[str, Any]]): The result from a Milvus search operation.\n\n        Returns:\n            List[Document]: A list of Document instances created from the Milvus search result.\n        \"\"\"\n        documents = []\n\n        for hit in result:\n            entity = hit.get(\"entity\", {})\n            content = entity.get(\"content\", \"\")\n            embedding = entity.get(\"vector\", [])\n            metadata = {k: v for k, v in entity.items() if k not in (\"content\", \"vector\")}\n\n            doc = Document(\n                id=str(hit.get(\"id\", \"\")),\n                content=content,\n                metadata=metadata,\n                embedding=embedding,\n                score=hit.get(\"distance\", None),\n            )\n            documents.append(doc)\n\n        return documents\n\n    def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents that match the provided filters.\n\n        Args:\n            filters (Dict[str, Any] | None): The filters to apply to the document list.\n\n        Returns:\n            list[Document]: A list of Document instances that match the given filters.\n\n        Raises:\n            ValueError: If no filters are provided.\n        \"\"\"\n        if not filters:\n            raise ValueError(\"No filters provided. No documents will be retrieved with filters.\")\n\n        filter_expression = Filter(filters).build_filter_expression()\n\n        result = self.client.query(\n            collection_name=self.index_name,\n            filter=filter_expression,\n            output_fields=[\"*\"],\n        )\n\n        return self._get_result_to_documents(result)\n\n    @staticmethod\n    def _get_result_to_documents(result: list[dict[str, Any]]) -&gt; list[Document]:\n        \"\"\"\n        Convert Milvus query result into Documents.\n\n        Args:\n            result (List[Dict[str, Any]]): The result from a Milvus query operation.\n\n        Returns:\n            List[Document]: A list containing Document objects created from the Milvus query result.\n        \"\"\"\n        documents = []\n\n        for entry in result:\n            document_dict: dict[str, Any] = {\n                \"id\": str(entry.get(\"id\", \"\")),\n                \"content\": entry.get(\"content\", \"\"),\n                \"embedding\": entry.get(\"vector\", []),\n            }\n            metadata = {k: v for k, v in entry.items() if k not in (\"id\", \"content\", \"vector\")}\n\n            if metadata:\n                document_dict[\"metadata\"] = metadata\n\n            try:\n                documents.append(Document(**document_dict))\n            except Exception as e:\n                logger.error(f\"Error creating Document: {e}, data: {document_dict}\")\n\n        return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Get the number of documents in the collection.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the collection.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Get the number of documents in the collection.\n\n    Returns:\n        int: The number of documents in the collection.\n    \"\"\"\n    return self.client.get_collection_stats(self.index_name)[\"row_count\"]\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the Milvus vector store based on their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>List[str]</code> <p>A list containing the IDs of documents to be deleted from the store.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>A flag to delete all documents from the store. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither document_ids nor delete_all is provided.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the Milvus vector store based on their IDs.\n\n    Args:\n        document_ids (List[str]): A list containing the IDs of documents to be deleted from the store.\n        delete_all (bool): A flag to delete all documents from the store. Defaults to False.\n\n    Raises:\n        ValueError: If neither document_ids nor delete_all is provided.\n    \"\"\"\n    if delete_all:\n        self.client.drop_collection(collection_name=self.index_name)\n        self.client.create_collection(\n            collection_name=self.index_name, schema=self.schema, index_params=self.index_params\n        )\n        self.client.load_collection(self.index_name)\n        logger.info(f\"All documents in the collection {self.index_name} have been deleted.\")\n    elif document_ids:\n        response = self.client.delete(collection_name=self.index_name, ids=document_ids)\n        logger.info(f\"Deleted {len(response)} documents from collection {self.index_name}.\")\n    else:\n        raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the vector store based on the provided file ID.     file_id should be located in the metadata of the document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n    \"\"\"\n    Delete documents from the vector store based on the provided file ID.\n        file_id should be located in the metadata of the document.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents based on filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict[str, Any]</code> <p>Filter criteria for deleting documents.</p> required Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents based on filters.\n\n    Args:\n        filters (Dict[str, Any]): Filter criteria for deleting documents.\n    \"\"\"\n    if not filters:\n        raise ValueError(\"Filters must be provided to delete documents.\")\n\n    filter_expression = Filter(filters).build_filter_expression()\n\n    delete_result = self.client.delete(collection_name=self.index_name, filter=filter_expression)\n\n    logger.info(f\"Deleted {len(delete_result)} entities from collection {self.index_name} based on filters.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.filter_documents","title":"<code>filter_documents(filters=None)</code>","text":"<p>Retrieve documents that match the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Dict[str, Any] | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Document instances that match the given filters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no filters are provided.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n    \"\"\"\n    Retrieve documents that match the provided filters.\n\n    Args:\n        filters (Dict[str, Any] | None): The filters to apply to the document list.\n\n    Returns:\n        list[Document]: A list of Document instances that match the given filters.\n\n    Raises:\n        ValueError: If no filters are provided.\n    \"\"\"\n    if not filters:\n        raise ValueError(\"No filters provided. No documents will be retrieved with filters.\")\n\n    filter_expression = Filter(filters).build_filter_expression()\n\n    result = self.client.query(\n        collection_name=self.index_name,\n        filter=filter_expression,\n        output_fields=[\"*\"],\n    )\n\n    return self._get_result_to_documents(result)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.list_documents","title":"<code>list_documents(limit=1000)</code>","text":"<p>List all documents in the collection up to a specified limit.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of documents to retrieve. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: A list of Document instances representing all documents in the collection.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def list_documents(self, limit: int = 1000) -&gt; list[Document]:\n    \"\"\"\n    List all documents in the collection up to a specified limit.\n\n    Args:\n        limit (int): Maximum number of documents to retrieve. Defaults to 1000.\n\n    Returns:\n        List[Document]: A list of Document instances representing all documents in the collection.\n    \"\"\"\n    if not self.client.has_collection(self.index_name):\n        raise ValueError(f\"Collection '{self.index_name}' does not exist.\")\n\n    collection_info = self.client.describe_collection(collection_name=self.index_name)\n\n    output_fields = [field[\"name\"] for field in collection_info[\"fields\"]]\n\n    result = self.client.query(collection_name=self.index_name, filter=\"\", output_fields=output_fields, limit=limit)\n\n    return self._get_result_to_documents(result)\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.search_embeddings","title":"<code>search_embeddings(query_embeddings, top_k, filters=None)</code>","text":"<p>Perform vector search on the stored documents using query embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>query_embeddings</code> <code>list[list[float]]</code> <p>A list of embeddings to use as queries.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of documents to retrieve.</p> required <code>filters</code> <code>dict[str, Any] | None</code> <p>A dictionary of filters to apply to the search. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>List[Document]: A list of Document objects containing the retrieved documents.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def search_embeddings(\n    self, query_embeddings: list[list[float]], top_k: int, filters: dict[str, Any] | None = None\n) -&gt; list[Document]:\n    \"\"\"\n    Perform vector search on the stored documents using query embeddings.\n\n    Args:\n        query_embeddings (list[list[float]]): A list of embeddings to use as queries.\n        top_k (int): The maximum number of documents to retrieve.\n        filters (dict[str, Any] | None): A dictionary of filters to apply to the search. Defaults to None.\n\n    Returns:\n        List[Document]: A list of Document objects containing the retrieved documents.\n    \"\"\"\n    search_params = {\"metric_type\": self.metric_type, \"params\": {}}\n\n    filter_expression = Filter(filters).build_filter_expression() if filters else \"\"\n\n    results = self.client.search(\n        collection_name=self.index_name,\n        data=query_embeddings,\n        limit=top_k,\n        filter=filter_expression,\n        output_fields=[\"*\"],\n        search_params=search_params,\n    )\n\n    return self._convert_query_result_to_documents(results[0])\n</code></pre>"},{"location":"dynamiq/storages/vector/milvus/milvus/#dynamiq.storages.vector.milvus.milvus.MilvusVectorStore.write_documents","title":"<code>write_documents(documents)</code>","text":"<p>Write (or overwrite) documents into the Milvus store.</p> <p>This method processes a list of Document objects and writes them into the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>A list of Document objects to be written into the document store.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an item in the documents list is not an instance of the Document class.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents successfully written to the document store.</p> Source code in <code>dynamiq/storages/vector/milvus/milvus.py</code> <pre><code>def write_documents(self, documents: list[Document]) -&gt; int:\n    \"\"\"\n    Write (or overwrite) documents into the Milvus store.\n\n    This method processes a list of Document objects and writes them into the vector store.\n\n    Args:\n        documents (List[Document]): A list of Document objects to be written into the document store.\n\n    Raises:\n        ValueError: If an item in the documents list is not an instance of the Document class.\n\n    Returns:\n        int: The number of documents successfully written to the document store.\n    \"\"\"\n    data_to_upsert = []\n\n    for doc in documents:\n        if not isinstance(doc, Document):\n            raise ValueError(\"All items in 'documents' must be of type Document.\")\n\n        document_data = {\n            \"id\": doc.id,\n            \"vector\": doc.embedding,\n            \"content\": doc.content,\n        }\n\n        if doc.metadata:\n            document_data.update(doc.metadata)\n\n        data_to_upsert.append(document_data)\n\n    response = self.client.upsert(\n        collection_name=self.index_name,\n        data=data_to_upsert,\n    )\n    return response[\"upsert_count\"]\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/pinecone/pinecone/","title":"Pinecone","text":""},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeIndexType","title":"<code>PineconeIndexType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>This enum defines various index types for different Pinecone deployments.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>class PineconeIndexType(str, enum.Enum):\n    \"\"\"\n    This enum defines various index types for different Pinecone deployments.\n    \"\"\"\n\n    SERVERLESS = \"serverless\"\n    POD = \"pod\"\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore","title":"<code>PineconeVectorStore</code>","text":"<p>Vector store using Pinecone.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>class PineconeVectorStore:\n    \"\"\"Vector store using Pinecone.\"\"\"\n\n    def __init__(\n        self,\n        connection: Pinecone | None = None,\n        client: Optional[\"PineconeClient\"] = None,\n        index_name: str = \"default\",\n        namespace: str = \"default\",\n        batch_size: int = 100,\n        dimension: int = 1536,\n        metric: str = \"cosine\",\n        create_if_not_exist: bool = False,\n        index_type: PineconeIndexType | None = None,\n        cloud: str | None = None,\n        region: str | None = None,\n        environment: str | None = None,\n        pod_type: str | None = None,\n        pods: int = 1,\n        **index_creation_kwargs,\n    ):\n        \"\"\"\n        Initialize a PineconeVectorStore instance.\n\n        Args:\n            connection (Optional[Pinecone]): Pinecone connection instance. Defaults to None.\n            client (Optional[PineconeClient]): Pinecone client instance. Defaults to None.\n            index_name (str): Name of the Pinecone index. Defaults to None.\n            namespace (str): Namespace for the index. Defaults to 'default'.\n            batch_size (int): Size of batches for operations. Defaults to 100.\n            dimension (int): Number of dimensions for vectors. Defaults to 1536.\n            metric (str): Metric for calculating vector similarity. Defaults to 'cosine'.\n            **index_creation_kwargs: Additional arguments for index creation.\n        \"\"\"\n        self.client = client\n        if self.client is None:\n            if connection is None:\n                connection = Pinecone()\n            self.client = connection.connect()\n\n        self.index_name = index_name\n        self.namespace = namespace\n        self.index_type = index_type\n\n        self.create_if_not_exist = create_if_not_exist\n\n        self.batch_size = batch_size\n        self.metric = metric\n        self.dimension = dimension\n        self.cloud = cloud\n        self.region = region\n        self.environment = environment\n        self.pod_type = pod_type\n        self.pods = pods\n\n        self.index_creation_kwargs = index_creation_kwargs\n\n        self._spec = self._get_spec()\n        self._dummy_vector = [-10.0] * dimension\n        self._index = self.connect_to_index()\n        logger.debug(f\"PineconeVectorStore initialized with index {self.index_name} and namespace {self.namespace}.\")\n\n    def _get_spec(self):\n        \"\"\"\n        Returns the serverless or pod specification for the Pinecone service.\n\n        Returns:\n            ServerlessSpec | PodSpec | None: The serverless or pod specification.\n        \"\"\"\n        if self.index_type == PineconeIndexType.SERVERLESS:\n            return self.serverless_spec\n        elif self.index_type == PineconeIndexType.POD:\n            return self.pod_spec\n\n    @property\n    def serverless_spec(self):\n        \"\"\"\n        Returns the serverless specification for the Pinecone service.\n\n        Returns:\n            ServerlessSpec: The serverless specification.\n        \"\"\"\n        # Import in runtime to save memory\n        from pinecone import ServerlessSpec\n\n        if self.cloud is None or self.region is None:\n            raise ValueError(\"'cloud' and 'region' must be specified for 'serverless' index\")\n        return ServerlessSpec(cloud=self.cloud, region=self.region)\n\n    @property\n    def pod_spec(self):\n        \"\"\"\n        Returns the pod specification for the Pinecone service.\n\n        Returns:\n            PodSpec: The pod specification.\n        \"\"\"\n        # Import in runtime to save memory\n        from pinecone import PodSpec\n\n        if self.environment is None or self.pod_type is None:\n            raise ValueError(\"'environment' and 'pod_type' must be specified for 'pod' index\")\n\n        return PodSpec(environment=self.environment, pod_type=self.pod_type, pods=self.pods)\n\n    def connect_to_index(self):\n        \"\"\"\n        Create or connect to an existing Pinecone index.\n\n        Returns:\n            The initialized Pinecone index object.\n        \"\"\"\n        available_indexes = self.client.list_indexes().index_list[\"indexes\"]\n        indexes_names = [index[\"name\"] for index in available_indexes]\n\n        if self.index_name not in indexes_names:\n            if self.create_if_not_exist and self.index_type is not None:\n                logger.debug(f\"Index {self.index_name} does not exist. Creating a new index.\")\n                self.client.create_index(\n                    name=self.index_name,\n                    spec=self._spec,\n                    dimension=self.dimension,\n                    metric=self.metric,\n                    **self.index_creation_kwargs,\n                )\n                return self.client.Index(name=self.index_name)\n            else:\n                raise ValueError(\n                    f\"Index {self.index_name} does not exist.\"\n                    f\"'create_if_not_exist' must be set to True and 'index_type' must be specified.\"\n                )\n        else:\n            logger.debug(f\"Index {self.index_name} already exists. Connecting to it.\")\n            return self.client.Index(name=self.index_name)\n\n    def _set_dimension(self, dimension: int):\n        \"\"\"\n        Set the dimension for the index, with a warning if it differs from the actual dimension.\n\n        Args:\n            dimension (int): The desired dimension.\n\n        Returns:\n            int: The actual dimension of the index.\n        \"\"\"\n        actual_dimension = self._index.describe_index_stats().get(\"dimension\")\n        if actual_dimension and actual_dimension != dimension:\n            logger.warning(\n                f\"Dimension of index {self.index_name} is {actual_dimension}, but {dimension} was specified. \"\n                \"The specified dimension will be ignored. \"\n                \"If you need an index with a different dimension, please create a new one.\"\n            )\n        return actual_dimension or dimension\n\n    def delete_index(self):\n        \"\"\"Delete the entire index.\"\"\"\n        self._index.delete(delete_all=True, namespace=self.namespace)\n        self.client.delete_index(self.index_name)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the Pinecone vector store.\n\n        Args:\n            document_ids (list[str]): List of document IDs to delete. Defaults to None.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n        \"\"\"\n        if delete_all and self._index is not None:\n            self._index.delete(delete_all=True, namespace=self.namespace)\n            self._index = self.connect_to_index()\n        else:\n            if not document_ids:\n                logger.warning(\n                    \"No document IDs provided. No documents will be deleted.\"\n                )\n            else:\n                self._index.delete(ids=document_ids, namespace=self.namespace)\n\n    def delete_documents_by_filters(\n        self, filters: dict[str, Any], top_k: int = 1000\n    ) -&gt; None:\n        \"\"\"\n        Delete documents from the Pinecone vector store using filters.\n\n        Args:\n            filters (dict[str, Any]): Filters to select documents to delete.\n            top_k (int): Maximum number of documents to retrieve for deletion. Defaults to 1000.\n        \"\"\"\n        if self.index_type is None or self.index_type == PineconeIndexType.SERVERLESS:\n            \"\"\"\n            Serverless and Starter indexes do not support deleting with metadata filtering.\n            \"\"\"\n            documents = self._embedding_retrieval(\n                query_embedding=self._dummy_vector,\n                filters=filters,\n                exclude_document_embeddings=True,\n                top_k=top_k,\n            )\n            document_ids = [doc.id for doc in documents]\n            self.delete_documents(document_ids=document_ids)\n        else:\n            filters = _normalize_filters(filters)\n            self._index.delete(filter=filters, namespace=self.namespace)\n\n    def delete_documents_by_file_id(self, file_id: str):\n        \"\"\"\n        Delete documents from the Pinecone vector store by file ID.\n            file_id should be located in the metadata of the document.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def list_documents(self, include_embeddings: bool = False) -&gt; list[Document]:\n        \"\"\"\n        List documents in the Pinecone vector store.\n\n        Args:\n            include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n\n        Returns:\n            list[Document]: List of Document objects retrieved.\n        \"\"\"\n\n        all_documents = []\n        for batch_doc_ids in self._index.list(namespace=self.namespace):\n            response = self._index.fetch(ids=batch_doc_ids, namespace=self.namespace)\n\n            documents = []\n            for pinecone_doc in response[\"vectors\"].values():\n                content = pinecone_doc[\"metadata\"].pop(\"content\", None)\n\n                embedding = None\n                if include_embeddings and pinecone_doc[\"values\"] != self._dummy_vector:\n                    embedding = pinecone_doc[\"values\"]\n\n                doc = Document(\n                    id=pinecone_doc[\"id\"],\n                    content=content,\n                    metadata=pinecone_doc[\"metadata\"],\n                    embedding=embedding,\n                    score=None,\n                )\n                documents.append(doc)\n\n            all_documents.extend(documents)\n        return all_documents\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the store.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n        try:\n            count = self._index.describe_index_stats()[\"namespaces\"][self.namespace][\n                \"vector_count\"\n            ]\n        except KeyError:\n            count = 0\n        return count\n\n    def write_documents(self, documents: list[Document]) -&gt; int:\n        \"\"\"\n        Write documents to the Pinecone vector store.\n\n        Args:\n            documents (list[Document]): List of Document objects to write.\n\n        Returns:\n            int: Number of documents successfully written.\n\n        Raises:\n            ValueError: If documents are not of type Document.\n        \"\"\"\n        if len(documents) &gt; 0 and not isinstance(documents[0], Document):\n            msg = \"param 'documents' must contain a list of objects of type Document\"\n            raise ValueError(msg)\n\n        documents_for_pinecone = self._convert_documents_to_pinecone_format(documents)\n\n        result = self._index.upsert(\n            vectors=documents_for_pinecone,\n            namespace=self.namespace,\n            batch_size=self.batch_size,\n        )\n\n        written_docs = result[\"upserted_count\"]\n        return written_docs\n\n    def _convert_documents_to_pinecone_format(\n        self, documents: list[Document]\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Convert Document objects to Pinecone-compatible format.\n\n        Args:\n            documents (list[Document]): List of Document objects to convert.\n\n        Returns:\n            list[dict[str, Any]]: List of documents in Pinecone-compatible format.\n        \"\"\"\n        documents_for_pinecone = []\n        for document in documents:\n            embedding = copy(document.embedding)\n            if embedding is None:\n                logger.warning(\n                    f\"Document {document.id} has no embedding. A dummy embedding will be used.\"\n                )\n                embedding = self._dummy_vector\n            doc_for_pinecone = {\n                \"id\": document.id,\n                \"values\": embedding,\n                \"metadata\": dict(document.metadata),\n            }\n\n            if document.content is not None:\n                doc_for_pinecone[\"metadata\"][\"content\"] = document.content\n\n            documents_for_pinecone.append(doc_for_pinecone)\n        return documents_for_pinecone\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        *,\n        namespace: str | None = None,\n        filters: dict[str, Any] | None = None,\n        top_k: int = 10,\n        exclude_document_embeddings: bool = True,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Retrieve documents similar to the given query embedding.\n\n        Args:\n            query_embedding (list[float]): The query embedding vector.\n            namespace (str | None): The namespace to query. Defaults to None.\n            filters (dict[str, Any] | None): Filters for the query. Defaults to None.\n            top_k (int): Maximum number of documents to retrieve. Defaults to 10.\n            exclude_document_embeddings (bool): Whether to exclude embeddings in results. Defaults to True.\n\n        Returns:\n            list[Document]: List of retrieved Document objects.\n\n        Raises:\n            ValueError: If query_embedding is empty or filter format is incorrect.\n        \"\"\"\n        if not query_embedding:\n            msg = \"query_embedding must be a non-empty list of floats\"\n            raise ValueError(msg)\n\n        filters = _normalize_filters(filters) if filters else None\n\n        result = self._index.query(\n            vector=query_embedding,\n            top_k=top_k,\n            namespace=namespace or self.namespace,\n            filter=filters,\n            include_values=not exclude_document_embeddings,\n            include_metadata=True,\n        )\n\n        return self._convert_query_result_to_documents(result)\n\n    def _convert_query_result_to_documents(\n        self, query_result: dict[str, Any]\n    ) -&gt; list[Document]:\n        \"\"\"\n        Convert Pinecone query results to Document objects.\n\n        Args:\n            query_result (dict[str, Any]): The query result from Pinecone.\n\n        Returns:\n            list[Document]: List of Document objects created from the query result.\n        \"\"\"\n        pinecone_docs = query_result[\"matches\"]\n        documents = []\n        for pinecone_doc in pinecone_docs:\n            content = pinecone_doc[\"metadata\"].pop(\"content\", None)\n\n            embedding = None\n            if pinecone_doc[\"values\"] != self._dummy_vector:\n                embedding = pinecone_doc[\"values\"]\n\n            doc = Document(\n                id=pinecone_doc[\"id\"],\n                content=content,\n                metadata=pinecone_doc[\"metadata\"],\n                embedding=embedding,\n                score=pinecone_doc[\"score\"],\n            )\n            documents.append(doc)\n\n        return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.pod_spec","title":"<code>pod_spec</code>  <code>property</code>","text":"<p>Returns the pod specification for the Pinecone service.</p> <p>Returns:</p> Name Type Description <code>PodSpec</code> <p>The pod specification.</p>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.serverless_spec","title":"<code>serverless_spec</code>  <code>property</code>","text":"<p>Returns the serverless specification for the Pinecone service.</p> <p>Returns:</p> Name Type Description <code>ServerlessSpec</code> <p>The serverless specification.</p>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', namespace='default', batch_size=100, dimension=1536, metric='cosine', create_if_not_exist=False, index_type=None, cloud=None, region=None, environment=None, pod_type=None, pods=1, **index_creation_kwargs)</code>","text":"<p>Initialize a PineconeVectorStore instance.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Optional[Pinecone]</code> <p>Pinecone connection instance. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[Pinecone]</code> <p>Pinecone client instance. Defaults to None.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>Name of the Pinecone index. Defaults to None.</p> <code>'default'</code> <code>namespace</code> <code>str</code> <p>Namespace for the index. Defaults to 'default'.</p> <code>'default'</code> <code>batch_size</code> <code>int</code> <p>Size of batches for operations. Defaults to 100.</p> <code>100</code> <code>dimension</code> <code>int</code> <p>Number of dimensions for vectors. Defaults to 1536.</p> <code>1536</code> <code>metric</code> <code>str</code> <p>Metric for calculating vector similarity. Defaults to 'cosine'.</p> <code>'cosine'</code> <code>**index_creation_kwargs</code> <p>Additional arguments for index creation.</p> <code>{}</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def __init__(\n    self,\n    connection: Pinecone | None = None,\n    client: Optional[\"PineconeClient\"] = None,\n    index_name: str = \"default\",\n    namespace: str = \"default\",\n    batch_size: int = 100,\n    dimension: int = 1536,\n    metric: str = \"cosine\",\n    create_if_not_exist: bool = False,\n    index_type: PineconeIndexType | None = None,\n    cloud: str | None = None,\n    region: str | None = None,\n    environment: str | None = None,\n    pod_type: str | None = None,\n    pods: int = 1,\n    **index_creation_kwargs,\n):\n    \"\"\"\n    Initialize a PineconeVectorStore instance.\n\n    Args:\n        connection (Optional[Pinecone]): Pinecone connection instance. Defaults to None.\n        client (Optional[PineconeClient]): Pinecone client instance. Defaults to None.\n        index_name (str): Name of the Pinecone index. Defaults to None.\n        namespace (str): Namespace for the index. Defaults to 'default'.\n        batch_size (int): Size of batches for operations. Defaults to 100.\n        dimension (int): Number of dimensions for vectors. Defaults to 1536.\n        metric (str): Metric for calculating vector similarity. Defaults to 'cosine'.\n        **index_creation_kwargs: Additional arguments for index creation.\n    \"\"\"\n    self.client = client\n    if self.client is None:\n        if connection is None:\n            connection = Pinecone()\n        self.client = connection.connect()\n\n    self.index_name = index_name\n    self.namespace = namespace\n    self.index_type = index_type\n\n    self.create_if_not_exist = create_if_not_exist\n\n    self.batch_size = batch_size\n    self.metric = metric\n    self.dimension = dimension\n    self.cloud = cloud\n    self.region = region\n    self.environment = environment\n    self.pod_type = pod_type\n    self.pods = pods\n\n    self.index_creation_kwargs = index_creation_kwargs\n\n    self._spec = self._get_spec()\n    self._dummy_vector = [-10.0] * dimension\n    self._index = self.connect_to_index()\n    logger.debug(f\"PineconeVectorStore initialized with index {self.index_name} and namespace {self.namespace}.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.connect_to_index","title":"<code>connect_to_index()</code>","text":"<p>Create or connect to an existing Pinecone index.</p> <p>Returns:</p> Type Description <p>The initialized Pinecone index object.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def connect_to_index(self):\n    \"\"\"\n    Create or connect to an existing Pinecone index.\n\n    Returns:\n        The initialized Pinecone index object.\n    \"\"\"\n    available_indexes = self.client.list_indexes().index_list[\"indexes\"]\n    indexes_names = [index[\"name\"] for index in available_indexes]\n\n    if self.index_name not in indexes_names:\n        if self.create_if_not_exist and self.index_type is not None:\n            logger.debug(f\"Index {self.index_name} does not exist. Creating a new index.\")\n            self.client.create_index(\n                name=self.index_name,\n                spec=self._spec,\n                dimension=self.dimension,\n                metric=self.metric,\n                **self.index_creation_kwargs,\n            )\n            return self.client.Index(name=self.index_name)\n        else:\n            raise ValueError(\n                f\"Index {self.index_name} does not exist.\"\n                f\"'create_if_not_exist' must be set to True and 'index_type' must be specified.\"\n            )\n    else:\n        logger.debug(f\"Index {self.index_name} already exists. Connecting to it.\")\n        return self.client.Index(name=self.index_name)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the store.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the store.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n    try:\n        count = self._index.describe_index_stats()[\"namespaces\"][self.namespace][\n            \"vector_count\"\n        ]\n    except KeyError:\n        count = 0\n    return count\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>List of document IDs to delete. Defaults to None.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the Pinecone vector store.\n\n    Args:\n        document_ids (list[str]): List of document IDs to delete. Defaults to None.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n    \"\"\"\n    if delete_all and self._index is not None:\n        self._index.delete(delete_all=True, namespace=self.namespace)\n        self._index = self.connect_to_index()\n    else:\n        if not document_ids:\n            logger.warning(\n                \"No document IDs provided. No documents will be deleted.\"\n            )\n        else:\n            self._index.delete(ids=document_ids, namespace=self.namespace)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the Pinecone vector store by file ID.     file_id should be located in the metadata of the document.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str):\n    \"\"\"\n    Delete documents from the Pinecone vector store by file ID.\n        file_id should be located in the metadata of the document.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters, top_k=1000)</code>","text":"<p>Delete documents from the Pinecone vector store using filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Filters to select documents to delete.</p> required <code>top_k</code> <code>int</code> <p>Maximum number of documents to retrieve for deletion. Defaults to 1000.</p> <code>1000</code> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_documents_by_filters(\n    self, filters: dict[str, Any], top_k: int = 1000\n) -&gt; None:\n    \"\"\"\n    Delete documents from the Pinecone vector store using filters.\n\n    Args:\n        filters (dict[str, Any]): Filters to select documents to delete.\n        top_k (int): Maximum number of documents to retrieve for deletion. Defaults to 1000.\n    \"\"\"\n    if self.index_type is None or self.index_type == PineconeIndexType.SERVERLESS:\n        \"\"\"\n        Serverless and Starter indexes do not support deleting with metadata filtering.\n        \"\"\"\n        documents = self._embedding_retrieval(\n            query_embedding=self._dummy_vector,\n            filters=filters,\n            exclude_document_embeddings=True,\n            top_k=top_k,\n        )\n        document_ids = [doc.id for doc in documents]\n        self.delete_documents(document_ids=document_ids)\n    else:\n        filters = _normalize_filters(filters)\n        self._index.delete(filter=filters, namespace=self.namespace)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.delete_index","title":"<code>delete_index()</code>","text":"<p>Delete the entire index.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def delete_index(self):\n    \"\"\"Delete the entire index.\"\"\"\n    self._index.delete(delete_all=True, namespace=self.namespace)\n    self.client.delete_index(self.index_name)\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False)</code>","text":"<p>List documents in the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the results. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: List of Document objects retrieved.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def list_documents(self, include_embeddings: bool = False) -&gt; list[Document]:\n    \"\"\"\n    List documents in the Pinecone vector store.\n\n    Args:\n        include_embeddings (bool): Whether to include embeddings in the results. Defaults to False.\n\n    Returns:\n        list[Document]: List of Document objects retrieved.\n    \"\"\"\n\n    all_documents = []\n    for batch_doc_ids in self._index.list(namespace=self.namespace):\n        response = self._index.fetch(ids=batch_doc_ids, namespace=self.namespace)\n\n        documents = []\n        for pinecone_doc in response[\"vectors\"].values():\n            content = pinecone_doc[\"metadata\"].pop(\"content\", None)\n\n            embedding = None\n            if include_embeddings and pinecone_doc[\"values\"] != self._dummy_vector:\n                embedding = pinecone_doc[\"values\"]\n\n            doc = Document(\n                id=pinecone_doc[\"id\"],\n                content=content,\n                metadata=pinecone_doc[\"metadata\"],\n                embedding=embedding,\n                score=None,\n            )\n            documents.append(doc)\n\n        all_documents.extend(documents)\n    return all_documents\n</code></pre>"},{"location":"dynamiq/storages/vector/pinecone/pinecone/#dynamiq.storages.vector.pinecone.pinecone.PineconeVectorStore.write_documents","title":"<code>write_documents(documents)</code>","text":"<p>Write documents to the Pinecone vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of Document objects to write.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of documents successfully written.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If documents are not of type Document.</p> Source code in <code>dynamiq/storages/vector/pinecone/pinecone.py</code> <pre><code>def write_documents(self, documents: list[Document]) -&gt; int:\n    \"\"\"\n    Write documents to the Pinecone vector store.\n\n    Args:\n        documents (list[Document]): List of Document objects to write.\n\n    Returns:\n        int: Number of documents successfully written.\n\n    Raises:\n        ValueError: If documents are not of type Document.\n    \"\"\"\n    if len(documents) &gt; 0 and not isinstance(documents[0], Document):\n        msg = \"param 'documents' must contain a list of objects of type Document\"\n        raise ValueError(msg)\n\n    documents_for_pinecone = self._convert_documents_to_pinecone_format(documents)\n\n    result = self._index.upsert(\n        vectors=documents_for_pinecone,\n        namespace=self.namespace,\n        batch_size=self.batch_size,\n    )\n\n    written_docs = result[\"upserted_count\"]\n    return written_docs\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/converters/","title":"Converters","text":""},{"location":"dynamiq/storages/vector/qdrant/converters/#dynamiq.storages.vector.qdrant.converters.convert_id","title":"<code>convert_id(_id)</code>","text":"<p>Converts any string into a UUID-like format in a deterministic way.</p> <p>Qdrant does not accept any string as an id, so an internal id has to be generated for each point. This is a deterministic way of doing so.</p> Source code in <code>dynamiq/storages/vector/qdrant/converters.py</code> <pre><code>def convert_id(_id: str) -&gt; str:\n    \"\"\"\n    Converts any string into a UUID-like format in a deterministic way.\n\n    Qdrant does not accept any string as an id, so an internal id has to be\n    generated for each point. This is a deterministic way of doing so.\n    \"\"\"\n    UUID_NAMESPACE = uuid.UUID(\"00000000-0000-0000-0000-000000000000\")\n    return uuid.uuid5(UUID_NAMESPACE, _id).hex\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/qdrant/filters/#dynamiq.storages.vector.qdrant.filters.build_filters_for_repeated_operators","title":"<code>build_filters_for_repeated_operators(must_clauses, should_clauses, must_not_clauses, qdrant_filter)</code>","text":"<p>Flattens the nested lists of clauses by creating separate Filters for each clause of a logical operator.</p> <p>Parameters:</p> Name Type Description Default <code>must_clauses</code> <code>list[Filter]</code> <p>A nested list of must clauses or an empty list.</p> required <code>should_clauses</code> <code>list[Filter]</code> <p>A nested list of should clauses or an empty list.</p> required <code>must_not_clauses</code> <code>list[Filter]</code> <p>A nested list of must_not clauses or an empty list.</p> required <code>qdrant_filter</code> <code>list[Filter]</code> <p>A list where the generated Filter objects will be appended. This list will be modified in-place.</p> required <p>Returns:</p> Type Description <code>list[Filter]</code> <p>The modified <code>qdrant_filter</code> list with appended generated Filter objects.</p> Source code in <code>dynamiq/storages/vector/qdrant/filters.py</code> <pre><code>def build_filters_for_repeated_operators(\n    must_clauses: list[models.Filter],\n    should_clauses: list[models.Filter],\n    must_not_clauses: list[models.Filter],\n    qdrant_filter: list[models.Filter],\n) -&gt; list[models.Filter]:\n    \"\"\"\n    Flattens the nested lists of clauses by creating separate Filters for each clause of a logical\n    operator.\n\n    Args:\n        must_clauses: A nested list of must clauses or an empty list.\n        should_clauses: A nested list of should clauses or an empty list.\n        must_not_clauses: A nested list of must_not clauses or an empty list.\n        qdrant_filter: A list where the generated Filter objects will be appended. This list will be\n            modified in-place.\n\n    Returns:\n        The modified `qdrant_filter` list with appended generated Filter objects.\n    \"\"\"\n\n    if any(isinstance(i, list) for i in must_clauses):\n        for i in must_clauses:\n            qdrant_filter.append(\n                models.Filter(\n                    must=i or None,\n                    should=should_clauses or None,\n                    must_not=must_not_clauses or None,\n                )\n            )\n    if any(isinstance(i, list) for i in should_clauses):\n        for i in should_clauses:\n            qdrant_filter.append(\n                models.Filter(\n                    must=must_clauses or None,\n                    should=i or None,\n                    must_not=must_not_clauses or None,\n                )\n            )\n    if any(isinstance(i, list) for i in must_not_clauses):\n        for i in must_clauses:\n            qdrant_filter.append(\n                models.Filter(\n                    must=must_clauses or None,\n                    should=should_clauses or None,\n                    must_not=i or None,\n                )\n            )\n\n    return qdrant_filter\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/filters/#dynamiq.storages.vector.qdrant.filters.convert_filters_to_qdrant","title":"<code>convert_filters_to_qdrant(filter_term=None, is_parent_call=True)</code>","text":"<p>Converts Dynamiq filters to the format used by Qdrant.</p> <p>Parameters:</p> Name Type Description Default <code>filter_term</code> <code>list[dict] | dict | Filter | None</code> <p>The Dynamiq filter to be converted to Qdrant.</p> <code>None</code> <code>is_parent_call</code> <code>bool</code> <p>Indicates if this is the top-level call to the function. If True, the function returns a single models.Filter object; if False, it may return a list of filters or conditions for further processing.</p> <code>True</code> <p>Returns:</p> Type Description <code>Filter | list[Filter] | list[Condition] | None</code> <p>A single Qdrant Filter in the parent call or a list of such Filters in recursive calls.</p> <p>Raises:</p> Type Description <code>VectorStoreFilterException</code> <p>If the invalid filter criteria is provided or if an unknown operator is encountered.</p> Source code in <code>dynamiq/storages/vector/qdrant/filters.py</code> <pre><code>def convert_filters_to_qdrant(\n    filter_term: list[dict] | dict | models.Filter | None = None, is_parent_call: bool = True\n) -&gt; models.Filter | list[models.Filter] | list[models.Condition] | None:\n    \"\"\"Converts Dynamiq filters to the format used by Qdrant.\n\n    Args:\n        filter_term: The Dynamiq filter to be converted to Qdrant.\n        is_parent_call: Indicates if this is the top-level call to the function. If True, the function\n            returns a single models.Filter object; if False, it may return a list of filters or\n            conditions for further processing.\n\n    Returns:\n        A single Qdrant Filter in the parent call or a list of such Filters in recursive calls.\n\n    Raises:\n        FilterError: If the invalid filter criteria is provided or if an unknown operator is\n            encountered.\n    \"\"\"\n\n    if isinstance(filter_term, models.Filter):\n        return filter_term\n    if not filter_term:\n        return None\n\n    must_clauses: list[models.Filter] = []\n    should_clauses: list[models.Filter] = []\n    must_not_clauses: list[models.Filter] = []\n    # Indicates if there are multiple same LOGICAL OPERATORS on each level\n    # and prevents them from being combined\n    same_operator_flag = False\n    conditions, qdrant_filter, current_level_operators = (\n        [],\n        [],\n        [],\n    )\n\n    if isinstance(filter_term, dict):\n        filter_term = [filter_term]\n\n    # ======== IDENTIFY FILTER ITEMS ON EACH LEVEL ========\n\n    for item in filter_term:\n        operator = item.get(\"operator\")\n\n        # Check for repeated similar operators on each level\n        same_operator_flag = operator in current_level_operators and operator in LOGICAL_OPERATORS\n        if not same_operator_flag:\n            current_level_operators.append(operator)\n\n        if operator is None:\n            msg = \"Operator not found in filters\"\n            raise FilterError(msg)\n\n        if operator in LOGICAL_OPERATORS and \"conditions\" not in item:\n            msg = f\"'conditions' not found for '{operator}'\"\n            raise FilterError(msg)\n\n        if operator in LOGICAL_OPERATORS:\n            # Recursively process nested conditions\n            current_filter = convert_filters_to_qdrant(item.get(\"conditions\", []), is_parent_call=False) or []\n\n            # When same_operator_flag is set to True,\n            # ensure each clause is appended as an independent list to avoid merging distinct clauses.\n            if operator == \"AND\":\n                must_clauses = [must_clauses, current_filter] if same_operator_flag else must_clauses + current_filter\n            elif operator == \"OR\":\n                should_clauses = (\n                    [should_clauses, current_filter] if same_operator_flag else should_clauses + current_filter\n                )\n            elif operator == \"NOT\":\n                must_not_clauses = (\n                    [must_not_clauses, current_filter] if same_operator_flag else must_not_clauses + current_filter\n                )\n\n        elif operator in COMPARISON_OPERATORS:\n            field = item.get(\"field\")\n            if not field.startswith(\"metadata.\"):\n                field = f\"metadata.{field}\"\n            value = item.get(\"value\")\n            if field is None or value is None:\n                msg = f\"'field' or 'value' not found for '{operator}'\"\n                raise FilterError(msg)\n\n            parsed_conditions = _parse_comparison_operation(comparison_operation=operator, key=field, value=value)\n\n            # check if the parsed_conditions are models.Filter or models.Condition\n            for condition in parsed_conditions:\n                if isinstance(condition, models.Filter):\n                    qdrant_filter.append(condition)\n                else:\n                    conditions.append(condition)\n\n        else:\n            msg = f\"Unknown operator {operator} used in filters\"\n            raise FilterError(msg)\n\n    # ======== PROCESS FILTER ITEMS ON EACH LEVEL ========\n\n    # If same logical operators have separate clauses, create separate filters\n    if same_operator_flag:\n        qdrant_filter = build_filters_for_repeated_operators(\n            must_clauses, should_clauses, must_not_clauses, qdrant_filter\n        )\n\n    # else append a single Filter for existing clauses\n    elif must_clauses or should_clauses or must_not_clauses:\n        qdrant_filter.append(\n            models.Filter(\n                must=must_clauses or None,\n                should=should_clauses or None,\n                must_not=must_not_clauses or None,\n            )\n        )\n\n    # In case of parent call, a single Filter is returned\n    if is_parent_call:\n        # If qdrant_filter has just a single Filter in parent call,\n        # then it might be returned instead.\n        if len(qdrant_filter) == 1 and isinstance(qdrant_filter[0], models.Filter):\n            return qdrant_filter[0]\n        else:\n            must_clauses.extend(conditions)\n            return models.Filter(\n                must=must_clauses or None,\n                should=should_clauses or None,\n                must_not=must_not_clauses or None,\n            )\n\n    # Store conditions of each level in output of the loop\n    elif conditions:\n        qdrant_filter.extend(conditions)\n\n    return qdrant_filter\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/","title":"Qdrant","text":""},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore","title":"<code>QdrantVectorStore</code>","text":"<p>QdrantVectorStore a Document Store for Qdrant.</p> <p>Usage example:</p> <pre><code>from dynamiq.types import Document\nfrom dynamiq.storages.vector.qdrant import QdrantVectorStore\n\ndocument_store = QdrantVectorStore(\n        url=\"https://xxxxxx-xxxxx-xxxxx-xxxx-xxxxxxxxx.us-east.aws.cloud.qdrant.io:6333\",\n    api_key=\"&lt;your-api-key&gt;\",\n)\n\ndocument_store.count_documents()\n</code></pre> <p>Attributes:</p> Name Type Description <code>DISTANCE_BY_SIMILARITY</code> <code>ClassVar[dict[QdrantSimilarityMetric, str]]</code> <p>Mapping of metrics to distances.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>class QdrantVectorStore:\n    \"\"\"QdrantVectorStore a Document Store for Qdrant.\n\n    Usage example:\n\n    ```python\n    from dynamiq.types import Document\n    from dynamiq.storages.vector.qdrant import QdrantVectorStore\n\n    document_store = QdrantVectorStore(\n            url=\"https://xxxxxx-xxxxx-xxxxx-xxxx-xxxxxxxxx.us-east.aws.cloud.qdrant.io:6333\",\n        api_key=\"&lt;your-api-key&gt;\",\n    )\n\n    document_store.count_documents()\n    ```\n\n    Attributes:\n        DISTANCE_BY_SIMILARITY (ClassVar[dict[QdrantSimilarityMetric, str]]): Mapping of metrics to distances.\n    \"\"\"\n\n    DISTANCE_BY_SIMILARITY: ClassVar[dict[QdrantSimilarityMetric, str]] = {\n        QdrantSimilarityMetric.COSINE: rest.Distance.COSINE,\n        QdrantSimilarityMetric.DOT_PRODUCT: rest.Distance.DOT,\n        QdrantSimilarityMetric.L2: rest.Distance.EUCLID,\n    }\n\n    def __init__(\n        self,\n        connection: QdrantConnection | None = None,\n        client: Optional[\"QdrantClient\"] = None,\n        location: str | None = None,\n        url: str | None = None,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: bool | None = None,\n        api_key: str | None = None,\n        prefix: str | None = None,\n        timeout: int | None = None,\n        host: str | None = None,\n        path: str | None = None,\n        force_disable_check_same_thread: bool = False,\n        index_name: str = \"Document\",\n        dimension: int = 1536,\n        on_disk: bool = False,\n        use_sparse_embeddings: bool = False,\n        sparse_idf: bool = False,\n        metric: QdrantSimilarityMetric = QdrantSimilarityMetric.COSINE,\n        return_embedding: bool = False,\n        create_if_not_exist: bool = False,\n        recreate_index: bool = False,\n        shard_number: int | None = None,\n        replication_factor: int | None = None,\n        write_consistency_factor: int | None = None,\n        on_disk_payload: bool | None = None,\n        hnsw_config: dict | None = None,\n        optimizers_config: dict | None = None,\n        wal_config: dict | None = None,\n        quantization_config: dict | None = None,\n        init_from: dict | None = None,\n        wait_result_from_api: bool = True,\n        metadata: dict | None = None,\n        write_batch_size: int = 100,\n        scroll_size: int = 10_000,\n        payload_fields_to_index: list[dict] | None = None,\n    ):\n        \"\"\"Initializes the QdrantDocumentStore.\n\n        Args:\n            location: If `memory` - use in-memory Qdrant instance. If `str` - use it as a URL parameter. If `None` - use\n                default values for host and port.\n            url: Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.\n            port: Port of the REST API interface.\n            grpc_port: Port of the gRPC interface.\n            prefer_grpc: If `True` - use gRPC interface whenever possible in custom methods.\n            https: If `True` - use HTTPS(SSL) protocol.\n            api_key: API key for authentication in Qdrant Cloud.\n            prefix: If not `None` - add prefix to the REST URL path. Example: service/v1 will result in\n                http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.\n            timeout: Timeout for REST and gRPC API requests.\n            host: Host name of Qdrant service. If `url` and `host` are `None`, set to `localhost`.\n            path: Persistence path for QdrantLocal.\n            force_disable_check_same_thread: For QdrantLocal, force disable check_same_thread. Only use this if you can\n                guarantee that you can resolve the thread safety outside QdrantClient.\n            index_name: Name of the index.\n            dimension: Dimension of the embeddings.\n            on_disk: Whether to store the collection on disk.\n            use_sparse_embedding: If set to `True`, enables support for sparse embeddings.\n            sparse_idf: If set to `True`, computes the Inverse Document Frequency (IDF) when using sparse embeddings. It\n                is required to use techniques like BM42. It is ignored if `use_sparse_embeddings` is `False`.\n            metric: The similarity metric to use.\n            return_embedding: Whether to return embeddings in the search results.\n            recreate_index: Whether to recreate the index.\n            shard_number: Number of shards in the collection.\n            replication_factor: Replication factor for the collection. Defines how many copies of each shard will be\n                created. Effective only in distributed mode.\n            write_consistency_factor: Write consistency factor for the collection. Minimum value is 1. Defines how many\n                replicas should apply to the operation for it to be considered successful. Increasing this number makes\n                the collection more resilient to inconsistencies but will cause failures if not enough replicas are\n                available. Effective only in distributed mode.\n            on_disk_payload: If `True`, the point's payload will not be stored in memory and will be read from the disk\n                every time it is requested. This setting saves RAM by slightly increasing response time. Note: indexed\n                payload values remain in RAM.\n            hnsw_config: Params for HNSW index.\n            optimizers_config: Params for optimizer.\n            wal_config: Params for Write-Ahead-Log.\n            quantization_config: Params for quantization. If `None`, quantization will be disabled.\n            init_from: Use data stored in another collection to initialize this collection.\n            wait_result_from_api: Whether to wait for the result from the API after each request.\n            metadata: Additional metadata to include with the documents.\n            write_batch_size: The batch size for writing documents.\n            scroll_size: The scroll size for reading documents.\n            payload_fields_to_index: List of payload fields to index.\n        \"\"\"\n\n        self._client = client\n        if self._client is None:\n            connection = connection or QdrantConnection()\n            self._client = connection.connect()\n\n        # Store the Qdrant client specific attributes\n        self.location = location\n        self.url = url\n        self.port = port\n        self.grpc_port = grpc_port\n        self.prefer_grpc = prefer_grpc\n        self.https = https\n        self.api_key = api_key\n        self.prefix = prefix\n        self.timeout = timeout\n        self.host = host\n        self.path = path\n        self.force_disable_check_same_thread = force_disable_check_same_thread\n        self.metadata = metadata or {}\n        self.api_key = api_key\n\n        # Store the Qdrant collection specific attributes\n        self.shard_number = shard_number\n        self.replication_factor = replication_factor\n        self.write_consistency_factor = write_consistency_factor\n        self.on_disk_payload = on_disk_payload\n        self.hnsw_config = hnsw_config\n        self.optimizers_config = optimizers_config\n        self.wal_config = wal_config\n        self.quantization_config = quantization_config\n        self.init_from = init_from\n        self.wait_result_from_api = wait_result_from_api\n        self.create_if_not_exist = create_if_not_exist\n        self.recreate_index = recreate_index\n        self.payload_fields_to_index = payload_fields_to_index\n        self.use_sparse_embeddings = use_sparse_embeddings\n        self.sparse_idf = use_sparse_embeddings and sparse_idf\n        self.dimension = dimension\n        self.on_disk = on_disk\n        self.metric = metric\n        self.index_name = index_name\n        self.return_embedding = return_embedding\n        self.write_batch_size = write_batch_size\n        self.scroll_size = scroll_size\n\n    @property\n    def client(self):\n        if not self._client:\n            self._client = qdrant_client.QdrantClient(\n                location=self.location,\n                url=self.url,\n                port=self.port,\n                grpc_port=self.grpc_port,\n                prefer_grpc=self.prefer_grpc,\n                https=self.https,\n                api_key=self.api_key.resolve_value() if self.api_key else None,\n                prefix=self.prefix,\n                timeout=self.timeout,\n                host=self.host,\n                path=self.path,\n                metadata=self.metadata,\n                force_disable_check_same_thread=self.force_disable_check_same_thread,\n            )\n            # Make sure the collection is properly set up\n            self._set_up_collection(\n                collection_name=self.index_name,\n                embedding_dim=self.dimension,\n                create_if_not_exist=self.create_if_not_exist,\n                recreate_collection=self.recreate_index,\n                similarity=self.metric,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n                sparse_idf=self.sparse_idf,\n                on_disk=self.on_disk,\n                payload_fields_to_index=self.payload_fields_to_index,\n            )\n        return self._client\n\n    def count_documents(self) -&gt; int:\n        \"\"\"Returns the number of documents present in the Document Store.\n\n        Returns:\n            The number of documents in the Document Store.\n        \"\"\"\n        try:\n            response = self.client.count(\n                collection_name=self.index_name,\n            )\n            return response.count\n        except (UnexpectedResponse, ValueError):\n            # Qdrant local raises ValueError if the collection is not found, but\n            # with the remote server UnexpectedResponse is raised. Until that's unified,\n            # we need to catch both.\n            return 0\n\n    def filter_documents(\n        self,\n        filters: dict[str, Any] | rest.Filter | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Returns the documents that match the provided filters.\n\n        For a detailed specification of the filters, refer to the\n        [documentation](https://docs.dynamiq.deepset.ai/docs/metadata-filtering)\n\n        Args:\n            filters: The filters to apply to the document list.\n\n        Returns:\n            A list of documents that match the given filters.\n        \"\"\"\n        if filters and not isinstance(filters, dict) and not isinstance(filters, rest.Filter):\n            msg = \"Filter must be a dictionary or an instance of `qdrant_client.http.models.Filter`\"\n            raise ValueError(msg)\n\n        if filters and not isinstance(filters, rest.Filter) and \"operator\" not in filters:\n            raise ValueError(\"Filter must contain an 'operator' key\")\n\n        return list(\n            self.get_documents_generator(\n                filters,\n            )\n        )\n\n    def write_documents(\n        self,\n        documents: list[Document],\n        policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n    ) -&gt; int:\n        \"\"\"Writes documents to Qdrant using the specified policy.\n\n        The QdrantDocumentStore can handle duplicate documents based on the given policy. The available policies are:\n        - `FAIL`: The operation will raise an error if any document already exists.\n        - `OVERWRITE`: Existing documents will be overwritten with the new ones.\n        - `SKIP`: Existing documents will be skipped, and only new documents will be added.\n\n        Args:\n            documents: A list of Document objects to write to Qdrant.\n            policy: The policy for handling duplicate documents.\n\n        Returns:\n            The number of documents written to the document store.\n        \"\"\"\n        for doc in documents:\n            if not isinstance(doc, Document):\n                msg = f\"DocumentStore.write_documents() expects a list of Documents but got an element of {type(doc)}.\"\n                raise ValueError(msg)\n\n        if len(documents) == 0:\n            logger.warning(\"Calling QdrantDocumentStore.write_documents() with empty list\")\n            return 0\n\n        document_objects = self._handle_duplicate_documents(\n            documents=documents,\n            index=self.index_name,\n            policy=policy,\n        )\n\n        batched_documents = get_batches_from_generator(document_objects, self.write_batch_size)\n        for document_batch in batched_documents:\n            batch = convert_dynamiq_documents_to_qdrant_points(\n                document_batch,\n                use_sparse_embeddings=self.use_sparse_embeddings,\n            )\n\n            self.client.upsert(\n                collection_name=self.index_name,\n                points=batch,\n                wait=self.wait_result_from_api,\n            )\n\n        return len(document_objects)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"Deletes documents that match the provided `document_ids` from the document store.\n\n        Args:\n            document_ids: The document ids to delete.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n        \"\"\"\n        if delete_all:\n            self.client.delete_collection(collection_name=self.index_name)\n        elif document_ids:\n            ids = [convert_id(_id) for _id in document_ids]\n            try:\n                self.client.delete(\n                    collection_name=self.index_name,\n                    points_selector=ids,\n                    wait=self.wait_result_from_api,\n                )\n            except KeyError:\n                logger.warning(\n                    \"Called QdrantDocumentStore.delete_documents() on a non-existing ID\",\n                )\n        else:\n            raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore based on the provided filters.\n\n        Args:\n            filters (dict[str, Any]): The filters to apply to the document list.\n        \"\"\"\n        if filters:\n            documents = self.filter_documents(filters=filters)\n            document_ids = [doc.id for doc in documents]\n            self.delete_documents(document_ids=document_ids)\n        else:\n            raise ValueError(\"No filters provided to delete documents.\")\n\n    def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore based on the provided file_id.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def get_documents_generator(\n        self,\n        filters: dict[str, Any] | rest.Filter | None = None,\n        include_embeddings: bool = False,\n    ) -&gt; Generator[Document, None, None]:\n        \"\"\"Returns a generator that yields documents from Qdrant based on the provided filters.\n\n        Args:\n            filters: Filters applied to the retrieved documents.\n            include_embeddings: Whether to include the embeddings of the retrieved documents.\n\n        Returns:\n            A generator that yields documents retrieved from Qdrant.\n        \"\"\"\n\n        index = self.index_name\n        qdrant_filters = convert_filters_to_qdrant(filters)\n\n        next_offset = None\n        stop_scrolling = False\n        while not stop_scrolling:\n            records, next_offset = self.client.scroll(\n                collection_name=index,\n                scroll_filter=qdrant_filters,\n                limit=self.scroll_size,\n                offset=next_offset,\n                with_payload=True,\n                with_vectors=include_embeddings,\n            )\n            stop_scrolling = next_offset is None or (\n                isinstance(next_offset, grpc.PointId) and next_offset.num == 0 and next_offset.uuid == \"\"\n            )\n\n            for record in records:\n                yield convert_qdrant_point_to_dynamiq_document(record, use_sparse_embeddings=self.use_sparse_embeddings)\n\n    def list_documents(self, include_embeddings: bool = False) -&gt; list[Document]:\n        \"\"\"Returns a list of all documents in the Document Store.\n\n        Args:\n            include_embeddings: Whether to include the embeddings of the retrieved documents.\n\n        Returns:\n            A list of all documents in the Document Store.\n        \"\"\"\n        return list(self.get_documents_generator(include_embeddings=include_embeddings))\n\n    def get_documents_by_id(\n        self,\n        ids: list[str],\n        index: str | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Retrieves documents from Qdrant by their IDs.\n\n        Args:\n            ids: A list of document IDs to retrieve.\n            index: The name of the index to retrieve documents from.\n\n        Returns:\n            A list of documents.\n        \"\"\"\n        index = index or self.index_name\n\n        documents: list[Document] = []\n\n        ids = [convert_id(_id) for _id in ids]\n        records = self.client.retrieve(\n            collection_name=index,\n            ids=ids,\n            with_payload=True,\n            with_vectors=True,\n        )\n\n        for record in records:\n            documents.append(\n                convert_qdrant_point_to_dynamiq_document(record, use_sparse_embeddings=self.use_sparse_embeddings)\n            )\n        return documents\n\n    def _query_by_sparse(\n        self,\n        query_sparse_embedding: SparseEmbedding,\n        filters: dict[str, Any] | rest.Filter | None = None,\n        top_k: int = 10,\n        scale_score: bool = False,\n        return_embedding: bool = False,\n        score_threshold: float | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Queries Qdrant using a sparse embedding and returns the most relevant documents.\n\n        Args:\n            query_sparse_embedding: Sparse embedding of the query.\n            filters: Filters applied to the retrieved documents.\n            top_k: Maximum number of documents to return.\n            scale_score: Whether to scale the scores of the retrieved documents.\n            return_embedding: Whether to return the embeddings of the retrieved documents.\n            score_threshold: A minimal score threshold for the result. Score of the returned result might be higher or\n                smaller than the threshold depending on the Distance function used. E.g. for cosine similarity only\n                higher scores will be returned.\n\n        Returns:\n            List of documents that are most similar to `query_sparse_embedding`.\n\n        Raises:\n            QdrantStoreError: If the Document Store was initialized with `use_sparse_embeddings=False`.\n        \"\"\"\n\n        if not self.use_sparse_embeddings:\n            message = (\n                \"You are trying to query using sparse embeddings, but the Document Store \"\n                \"was initialized with `use_sparse_embeddings=False`. \"\n            )\n            raise QdrantStoreError(message)\n\n        qdrant_filters = convert_filters_to_qdrant(filters)\n        query_indices = query_sparse_embedding.indices\n        query_values = query_sparse_embedding.values\n        points = self.client.query_points(\n            collection_name=self.index_name,\n            query=rest.SparseVector(\n                indices=query_indices,\n                values=query_values,\n            ),\n            using=SPARSE_VECTORS_NAME,\n            query_filter=qdrant_filters,\n            limit=top_k,\n            with_vectors=return_embedding,\n            score_threshold=score_threshold,\n        ).points\n        results = [\n            convert_qdrant_point_to_dynamiq_document(point, use_sparse_embeddings=self.use_sparse_embeddings)\n            for point in points\n        ]\n        if scale_score:\n            for document in results:\n                score = document.score\n                score = float(1 / (1 + np.exp(-score / 100)))\n                document.score = score\n        return results\n\n    def _query_by_embedding(\n        self,\n        query_embedding: list[float],\n        filters: dict[str, Any] | rest.Filter | None = None,\n        top_k: int = 10,\n        scale_score: bool = False,\n        return_embedding: bool = False,\n        score_threshold: float | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Queries Qdrant using a dense embedding and returns the most relevant documents.\n\n        Args:\n            query_embedding: Dense embedding of the query.\n            filters: Filters applied to the retrieved documents.\n            top_k: Maximum number of documents to return.\n            scale_score: Whether to scale the scores of the retrieved documents.\n            return_embedding: Whether to return the embeddings of the retrieved documents.\n            score_threshold: A minimal score threshold for the result. Score of the returned result might be higher or\n                smaller than the threshold depending on the Distance function used. E.g. for cosine similarity only\n                higher scores will be returned.\n\n        Returns:\n            List of documents that are most similar to `query_embedding`.\n        \"\"\"\n        qdrant_filters = convert_filters_to_qdrant(filters)\n\n        points = self.client.query_points(\n            collection_name=self.index_name,\n            query=query_embedding,\n            using=DENSE_VECTORS_NAME if self.use_sparse_embeddings else None,\n            query_filter=qdrant_filters,\n            limit=top_k,\n            with_vectors=return_embedding,\n            score_threshold=score_threshold,\n        ).points\n        results = [\n            convert_qdrant_point_to_dynamiq_document(point, use_sparse_embeddings=self.use_sparse_embeddings)\n            for point in points\n        ]\n        if scale_score:\n            for document in results:\n                score = document.score\n                if self.metric == \"cosine\":\n                    score = (score + 1) / 2\n                else:\n                    score = float(1 / (1 + np.exp(-score / 100)))\n                document.score = score\n        return results\n\n    def _query_hybrid(\n        self,\n        query_embedding: list[float],\n        query_sparse_embedding: SparseEmbedding,\n        filters: dict[str, Any] | rest.Filter | None = None,\n        top_k: int = 10,\n        return_embedding: bool = False,\n        score_threshold: float | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"Retrieves documents based on dense and sparse embeddings and fuses the results using Reciprocal Rank Fusion.\n\n        This method is not part of the public interface of `QdrantDocumentStore` and shouldn't be used directly. Use the\n        `QdrantHybridRetriever` instead.\n\n        Args:\n            query_embedding: Dense embedding of the query.\n            query_sparse_embedding: Sparse embedding of the query.\n            filters: Filters applied to the retrieved documents.\n            top_k: Maximum number of documents to return.\n            return_embedding: Whether to return the embeddings of the retrieved documents.\n            score_threshold: A minimal score threshold for the result. Score of the returned result might be higher or\n                smaller than the threshold depending on the Distance function used. E.g. for cosine similarity only\n                higher scores will be returned.\n\n        Returns:\n            List of Document that are most similar to `query_embedding` and `query_sparse_embedding`.\n\n        Raises:\n            QdrantStoreError: If the Document Store was initialized with `use_sparse_embeddings=False`.\n        \"\"\"\n\n        # This implementation is based on the code from the Python Qdrant client:\n        # https://github.com/qdrant/qdrant-client/blob/8e3ea58f781e4110d11c0a6985b5e6bb66b85d33/qdrant_client/qdrant_fastembed.py#L519\n        if not self.use_sparse_embeddings:\n            message = (\n                \"You are trying to query using sparse embeddings, but the Document Store \"\n                \"was initialized with `use_sparse_embeddings=False`. \"\n            )\n            raise QdrantStoreError(message)\n\n        qdrant_filters = convert_filters_to_qdrant(filters)\n\n        try:\n            points = self.client.query_points(\n                collection_name=self.index_name,\n                prefetch=[\n                    rest.Prefetch(\n                        query=rest.SparseVector(\n                            indices=query_sparse_embedding.indices,\n                            values=query_sparse_embedding.values,\n                        ),\n                        using=SPARSE_VECTORS_NAME,\n                        filter=qdrant_filters,\n                    ),\n                    rest.Prefetch(\n                        query=query_embedding,\n                        using=DENSE_VECTORS_NAME,\n                        filter=qdrant_filters,\n                    ),\n                ],\n                query=rest.FusionQuery(fusion=rest.Fusion.RRF),\n                limit=top_k,\n                score_threshold=score_threshold,\n                with_payload=True,\n                with_vectors=return_embedding,\n            ).points\n        except Exception as e:\n            msg = \"Error during hybrid search\"\n            raise QdrantStoreError(msg) from e\n\n        results = [convert_qdrant_point_to_dynamiq_document(point, use_sparse_embeddings=True) for point in points]\n\n        return results\n\n    def get_distance(self, similarity: str) -&gt; rest.Distance:\n        \"\"\"Retrieves the distance metric for the specified similarity measure.\n\n        Args:\n            similarity: The similarity measure to retrieve the distance.\n\n        Returns:\n            The corresponding rest.Distance object.\n\n        Raises:\n            QdrantStoreError: If the provided similarity measure is not supported.\n        \"\"\"\n        try:\n            return self.DISTANCE_BY_SIMILARITY[similarity]\n        except KeyError as ke:\n            msg = (\n                f\"Provided similarity '{similarity}' is not supported by Qdrant \"\n                f\"document store. Please choose one of the options: \"\n                f\"{', '.join(self.DISTANCE_BY_SIMILARITY.keys())}\"\n            )\n            raise QdrantStoreError(msg) from ke\n\n    def _create_payload_index(self, collection_name: str, payload_fields_to_index: list[dict] | None = None):\n        \"\"\"Create payload index for the collection if payload_fields_to_index is provided.\n\n        See: https://qdrant.tech/documentation/concepts/indexing/#payload-index\n\n        Args:\n            collection_name: The name of the collection.\n            payload_fields_to_index: List of payload fields to index.\n        \"\"\"\n        if payload_fields_to_index is not None:\n            for payload_index in payload_fields_to_index:\n                self.client.create_payload_index(\n                    collection_name=collection_name,\n                    field_name=payload_index[\"field_name\"],\n                    field_schema=payload_index[\"field_schema\"],\n                )\n\n    def _set_up_collection(\n        self,\n        collection_name: str,\n        embedding_dim: int,\n        create_if_not_exist: bool,\n        recreate_collection: bool,\n        similarity: str,\n        use_sparse_embeddings: bool,\n        sparse_idf: bool,\n        on_disk: bool = False,\n        payload_fields_to_index: list[dict] | None = None,\n    ):\n        \"\"\"Sets up the Qdrant collection with the specified parameters.\n\n        Args:\n            collection_name: The name of the collection to set up.\n            embedding_dim: The dimension of the embeddings.\n            recreate_collection: Whether to recreate the collection if it already exists.\n            similarity: The similarity measure to use.\n            use_sparse_embeddings: Whether to use sparse embeddings.\n            sparse_idf: Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required\n                for BM42.\n            on_disk: Whether to store the collection on disk.\n            payload_fields_to_index: List of payload fields to index.\n\n        Raises:\n            QdrantStoreError: If the collection exists with incompatible settings.\n            ValueError: If the collection exists with a different similarity measure or embedding dimension.\n        \"\"\"\n        distance = self.get_distance(similarity)\n\n        collection_exists = self.client.collection_exists(collection_name)\n\n        if not create_if_not_exist and not collection_exists:\n            msg = f\"Collection '{collection_name}' does not exist in Qdrant.\"\n            raise QdrantStoreError(msg)\n\n        if recreate_collection or not collection_exists:\n            # There is no need to verify the current configuration of that\n            # collection. It might be just recreated again or does not exist yet.\n            self.recreate_collection(\n                collection_name, distance, embedding_dim, on_disk, use_sparse_embeddings, sparse_idf\n            )\n            # Create Payload index if payload_fields_to_index is provided\n            self._create_payload_index(collection_name, payload_fields_to_index)\n            logger.debug(f\"Index {self.index_name} does not exist. Creating a new index.\")\n            return\n\n        collection_info = self.client.get_collection(collection_name)\n\n        has_named_vectors = (\n            isinstance(collection_info.config.params.vectors, dict)\n            and DENSE_VECTORS_NAME in collection_info.config.params.vectors\n        )\n\n        if self.use_sparse_embeddings and not has_named_vectors:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it has been originally created without sparse embedding vectors. \"\n                f\"If you want to use that collection, you can set `use_sparse_embeddings=False`. \"\n                f\"To use sparse embeddings, you need to recreate the collection or migrate the existing one. \"\n                f\"See `migrate_to_sparse_embeddings_support` function in \"\n                f\"`dynamiq_integrations.document_stores.qdrant`.\"\n            )\n            raise QdrantStoreError(msg)\n\n        elif not self.use_sparse_embeddings and has_named_vectors:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it has been originally created with sparse embedding vectors.\"\n                f\"If you want to use that collection, please set `use_sparse_embeddings=True`.\"\n            )\n            raise QdrantStoreError(msg)\n\n        if self.use_sparse_embeddings:\n            current_distance = collection_info.config.params.vectors[DENSE_VECTORS_NAME].distance\n            current_vector_size = collection_info.config.params.vectors[DENSE_VECTORS_NAME].size\n        else:\n            current_distance = collection_info.config.params.vectors.distance\n            current_vector_size = collection_info.config.params.vectors.size\n\n        if current_distance != distance:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it is configured with a similarity '{current_distance.name}'. \"\n                f\"If you want to use that collection, but with a different \"\n                f\"similarity, please set `recreate_collection=True` argument.\"\n            )\n            raise ValueError(msg)\n\n        if current_vector_size != embedding_dim:\n            msg = (\n                f\"Collection '{collection_name}' already exists in Qdrant, \"\n                f\"but it is configured with a vector size '{current_vector_size}'. \"\n                f\"If you want to use that collection, but with a different \"\n                f\"vector size, please set `recreate_collection=True` argument.\"\n            )\n            raise ValueError(msg)\n\n    def recreate_collection(\n        self,\n        collection_name: str,\n        distance,\n        embedding_dim: int,\n        on_disk: bool | None = None,\n        use_sparse_embeddings: bool | None = None,\n        sparse_idf: bool = False,\n    ):\n        \"\"\"Recreates the Qdrant collection with the specified parameters.\n\n        Args:\n            collection_name: The name of the collection to recreate.\n            distance: The distance metric to use for the collection.\n            embedding_dim: The dimension of the embeddings.\n            on_disk: Whether to store the collection on disk.\n            use_sparse_embeddings: Whether to use sparse embeddings.\n            sparse_idf: Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required\n                for BM42.\n        \"\"\"\n        if on_disk is None:\n            on_disk = self.on_disk\n\n        if use_sparse_embeddings is None:\n            use_sparse_embeddings = self.use_sparse_embeddings\n\n        # dense vectors configuration\n        vectors_config = rest.VectorParams(size=embedding_dim, on_disk=on_disk, distance=distance)\n\n        if use_sparse_embeddings:\n            # in this case, we need to define named vectors\n            vectors_config = {DENSE_VECTORS_NAME: vectors_config}\n\n            sparse_vectors_config = {\n                SPARSE_VECTORS_NAME: rest.SparseVectorParams(\n                    index=rest.SparseIndexParams(\n                        on_disk=on_disk,\n                    ),\n                    modifier=rest.Modifier.IDF if sparse_idf else None,\n                ),\n            }\n\n        if self.client.collection_exists(collection_name):\n            self.client.delete_collection(collection_name)\n\n        self.client.create_collection(\n            collection_name=collection_name,\n            vectors_config=vectors_config,\n            sparse_vectors_config=sparse_vectors_config if use_sparse_embeddings else None,\n            shard_number=self.shard_number,\n            replication_factor=self.replication_factor,\n            write_consistency_factor=self.write_consistency_factor,\n            on_disk_payload=self.on_disk_payload,\n            hnsw_config=self.hnsw_config,\n            optimizers_config=self.optimizers_config,\n            wal_config=self.wal_config,\n            quantization_config=self.quantization_config,\n            init_from=self.init_from,\n        )\n\n    def _handle_duplicate_documents(\n        self,\n        documents: list[Document],\n        index: str | None = None,\n        policy: DuplicatePolicy = None,\n    ):\n        \"\"\"Checks whether any of the passed documents is already existing in the chosen index and returns a list of\n        documents that are not in the index yet.\n\n        Args:\n            documents: A list of Dynamiq Document objects.\n            index: Name of the index.\n            policy: The duplicate policy to use when writing documents.\n\n        Returns:\n            A list of Dynamiq Document objects.\n        \"\"\"\n\n        index = index or self.index_name\n        if policy in (DuplicatePolicy.SKIP, DuplicatePolicy.FAIL):\n            documents = self._drop_duplicate_documents(documents, index)\n            documents_found = self.get_documents_by_id(ids=[doc.id for doc in documents], index=index)\n            ids_exist_in_db: list[str] = [doc.id for doc in documents_found]\n\n            if len(ids_exist_in_db) &gt; 0 and policy == DuplicatePolicy.FAIL:\n                msg = f\"Document with ids '{', '.join(ids_exist_in_db)} already exists in index = '{index}'.\"\n                raise DuplicateDocumentError(msg)\n\n            documents = list(filter(lambda doc: doc.id not in ids_exist_in_db, documents))\n\n        return documents\n\n    def _drop_duplicate_documents(self, documents: list[Document], index: str | None = None) -&gt; list[Document]:\n        \"\"\"Drop duplicate documents based on same hash ID.\n\n        Args:\n            documents: A list of Dynamiq Document objects.\n            index: Name of the index.\n\n        Returns:\n            A list of Dynamiq Document objects.\n        \"\"\"\n        _hash_ids: set = set()\n        _documents: list[Document] = []\n\n        for document in documents:\n            if document.id in _hash_ids:\n                logger.info(\n                    \"Duplicate Documents: Document with id '%s' already exists in index '%s'\",\n                    document.id,\n                    index or self.index_name,\n                )\n                continue\n            _documents.append(document)\n            _hash_ids.add(document.id)\n\n        return _documents\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.__init__","title":"<code>__init__(connection=None, client=None, location=None, url=None, port=6333, grpc_port=6334, prefer_grpc=False, https=None, api_key=None, prefix=None, timeout=None, host=None, path=None, force_disable_check_same_thread=False, index_name='Document', dimension=1536, on_disk=False, use_sparse_embeddings=False, sparse_idf=False, metric=QdrantSimilarityMetric.COSINE, return_embedding=False, create_if_not_exist=False, recreate_index=False, shard_number=None, replication_factor=None, write_consistency_factor=None, on_disk_payload=None, hnsw_config=None, optimizers_config=None, wal_config=None, quantization_config=None, init_from=None, wait_result_from_api=True, metadata=None, write_batch_size=100, scroll_size=10000, payload_fields_to_index=None)</code>","text":"<p>Initializes the QdrantDocumentStore.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str | None</code> <p>If <code>memory</code> - use in-memory Qdrant instance. If <code>str</code> - use it as a URL parameter. If <code>None</code> - use default values for host and port.</p> <code>None</code> <code>url</code> <code>str | None</code> <p>Either host or str of <code>Optional[scheme], host, Optional[port], Optional[prefix]</code>.</p> <code>None</code> <code>port</code> <code>int</code> <p>Port of the REST API interface.</p> <code>6333</code> <code>grpc_port</code> <code>int</code> <p>Port of the gRPC interface.</p> <code>6334</code> <code>prefer_grpc</code> <code>bool</code> <p>If <code>True</code> - use gRPC interface whenever possible in custom methods.</p> <code>False</code> <code>https</code> <code>bool | None</code> <p>If <code>True</code> - use HTTPS(SSL) protocol.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for authentication in Qdrant Cloud.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>If not <code>None</code> - add prefix to the REST URL path. Example: service/v1 will result in http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.</p> <code>None</code> <code>timeout</code> <code>int | None</code> <p>Timeout for REST and gRPC API requests.</p> <code>None</code> <code>host</code> <code>str | None</code> <p>Host name of Qdrant service. If <code>url</code> and <code>host</code> are <code>None</code>, set to <code>localhost</code>.</p> <code>None</code> <code>path</code> <code>str | None</code> <p>Persistence path for QdrantLocal.</p> <code>None</code> <code>force_disable_check_same_thread</code> <code>bool</code> <p>For QdrantLocal, force disable check_same_thread. Only use this if you can guarantee that you can resolve the thread safety outside QdrantClient.</p> <code>False</code> <code>index_name</code> <code>str</code> <p>Name of the index.</p> <code>'Document'</code> <code>dimension</code> <code>int</code> <p>Dimension of the embeddings.</p> <code>1536</code> <code>on_disk</code> <code>bool</code> <p>Whether to store the collection on disk.</p> <code>False</code> <code>use_sparse_embedding</code> <p>If set to <code>True</code>, enables support for sparse embeddings.</p> required <code>sparse_idf</code> <code>bool</code> <p>If set to <code>True</code>, computes the Inverse Document Frequency (IDF) when using sparse embeddings. It is required to use techniques like BM42. It is ignored if <code>use_sparse_embeddings</code> is <code>False</code>.</p> <code>False</code> <code>metric</code> <code>QdrantSimilarityMetric</code> <p>The similarity metric to use.</p> <code>COSINE</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return embeddings in the search results.</p> <code>False</code> <code>recreate_index</code> <code>bool</code> <p>Whether to recreate the index.</p> <code>False</code> <code>shard_number</code> <code>int | None</code> <p>Number of shards in the collection.</p> <code>None</code> <code>replication_factor</code> <code>int | None</code> <p>Replication factor for the collection. Defines how many copies of each shard will be created. Effective only in distributed mode.</p> <code>None</code> <code>write_consistency_factor</code> <code>int | None</code> <p>Write consistency factor for the collection. Minimum value is 1. Defines how many replicas should apply to the operation for it to be considered successful. Increasing this number makes the collection more resilient to inconsistencies but will cause failures if not enough replicas are available. Effective only in distributed mode.</p> <code>None</code> <code>on_disk_payload</code> <code>bool | None</code> <p>If <code>True</code>, the point's payload will not be stored in memory and will be read from the disk every time it is requested. This setting saves RAM by slightly increasing response time. Note: indexed payload values remain in RAM.</p> <code>None</code> <code>hnsw_config</code> <code>dict | None</code> <p>Params for HNSW index.</p> <code>None</code> <code>optimizers_config</code> <code>dict | None</code> <p>Params for optimizer.</p> <code>None</code> <code>wal_config</code> <code>dict | None</code> <p>Params for Write-Ahead-Log.</p> <code>None</code> <code>quantization_config</code> <code>dict | None</code> <p>Params for quantization. If <code>None</code>, quantization will be disabled.</p> <code>None</code> <code>init_from</code> <code>dict | None</code> <p>Use data stored in another collection to initialize this collection.</p> <code>None</code> <code>wait_result_from_api</code> <code>bool</code> <p>Whether to wait for the result from the API after each request.</p> <code>True</code> <code>metadata</code> <code>dict | None</code> <p>Additional metadata to include with the documents.</p> <code>None</code> <code>write_batch_size</code> <code>int</code> <p>The batch size for writing documents.</p> <code>100</code> <code>scroll_size</code> <code>int</code> <p>The scroll size for reading documents.</p> <code>10000</code> <code>payload_fields_to_index</code> <code>list[dict] | None</code> <p>List of payload fields to index.</p> <code>None</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def __init__(\n    self,\n    connection: QdrantConnection | None = None,\n    client: Optional[\"QdrantClient\"] = None,\n    location: str | None = None,\n    url: str | None = None,\n    port: int = 6333,\n    grpc_port: int = 6334,\n    prefer_grpc: bool = False,\n    https: bool | None = None,\n    api_key: str | None = None,\n    prefix: str | None = None,\n    timeout: int | None = None,\n    host: str | None = None,\n    path: str | None = None,\n    force_disable_check_same_thread: bool = False,\n    index_name: str = \"Document\",\n    dimension: int = 1536,\n    on_disk: bool = False,\n    use_sparse_embeddings: bool = False,\n    sparse_idf: bool = False,\n    metric: QdrantSimilarityMetric = QdrantSimilarityMetric.COSINE,\n    return_embedding: bool = False,\n    create_if_not_exist: bool = False,\n    recreate_index: bool = False,\n    shard_number: int | None = None,\n    replication_factor: int | None = None,\n    write_consistency_factor: int | None = None,\n    on_disk_payload: bool | None = None,\n    hnsw_config: dict | None = None,\n    optimizers_config: dict | None = None,\n    wal_config: dict | None = None,\n    quantization_config: dict | None = None,\n    init_from: dict | None = None,\n    wait_result_from_api: bool = True,\n    metadata: dict | None = None,\n    write_batch_size: int = 100,\n    scroll_size: int = 10_000,\n    payload_fields_to_index: list[dict] | None = None,\n):\n    \"\"\"Initializes the QdrantDocumentStore.\n\n    Args:\n        location: If `memory` - use in-memory Qdrant instance. If `str` - use it as a URL parameter. If `None` - use\n            default values for host and port.\n        url: Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.\n        port: Port of the REST API interface.\n        grpc_port: Port of the gRPC interface.\n        prefer_grpc: If `True` - use gRPC interface whenever possible in custom methods.\n        https: If `True` - use HTTPS(SSL) protocol.\n        api_key: API key for authentication in Qdrant Cloud.\n        prefix: If not `None` - add prefix to the REST URL path. Example: service/v1 will result in\n            http://localhost:6333/service/v1/{qdrant-endpoint} for REST API.\n        timeout: Timeout for REST and gRPC API requests.\n        host: Host name of Qdrant service. If `url` and `host` are `None`, set to `localhost`.\n        path: Persistence path for QdrantLocal.\n        force_disable_check_same_thread: For QdrantLocal, force disable check_same_thread. Only use this if you can\n            guarantee that you can resolve the thread safety outside QdrantClient.\n        index_name: Name of the index.\n        dimension: Dimension of the embeddings.\n        on_disk: Whether to store the collection on disk.\n        use_sparse_embedding: If set to `True`, enables support for sparse embeddings.\n        sparse_idf: If set to `True`, computes the Inverse Document Frequency (IDF) when using sparse embeddings. It\n            is required to use techniques like BM42. It is ignored if `use_sparse_embeddings` is `False`.\n        metric: The similarity metric to use.\n        return_embedding: Whether to return embeddings in the search results.\n        recreate_index: Whether to recreate the index.\n        shard_number: Number of shards in the collection.\n        replication_factor: Replication factor for the collection. Defines how many copies of each shard will be\n            created. Effective only in distributed mode.\n        write_consistency_factor: Write consistency factor for the collection. Minimum value is 1. Defines how many\n            replicas should apply to the operation for it to be considered successful. Increasing this number makes\n            the collection more resilient to inconsistencies but will cause failures if not enough replicas are\n            available. Effective only in distributed mode.\n        on_disk_payload: If `True`, the point's payload will not be stored in memory and will be read from the disk\n            every time it is requested. This setting saves RAM by slightly increasing response time. Note: indexed\n            payload values remain in RAM.\n        hnsw_config: Params for HNSW index.\n        optimizers_config: Params for optimizer.\n        wal_config: Params for Write-Ahead-Log.\n        quantization_config: Params for quantization. If `None`, quantization will be disabled.\n        init_from: Use data stored in another collection to initialize this collection.\n        wait_result_from_api: Whether to wait for the result from the API after each request.\n        metadata: Additional metadata to include with the documents.\n        write_batch_size: The batch size for writing documents.\n        scroll_size: The scroll size for reading documents.\n        payload_fields_to_index: List of payload fields to index.\n    \"\"\"\n\n    self._client = client\n    if self._client is None:\n        connection = connection or QdrantConnection()\n        self._client = connection.connect()\n\n    # Store the Qdrant client specific attributes\n    self.location = location\n    self.url = url\n    self.port = port\n    self.grpc_port = grpc_port\n    self.prefer_grpc = prefer_grpc\n    self.https = https\n    self.api_key = api_key\n    self.prefix = prefix\n    self.timeout = timeout\n    self.host = host\n    self.path = path\n    self.force_disable_check_same_thread = force_disable_check_same_thread\n    self.metadata = metadata or {}\n    self.api_key = api_key\n\n    # Store the Qdrant collection specific attributes\n    self.shard_number = shard_number\n    self.replication_factor = replication_factor\n    self.write_consistency_factor = write_consistency_factor\n    self.on_disk_payload = on_disk_payload\n    self.hnsw_config = hnsw_config\n    self.optimizers_config = optimizers_config\n    self.wal_config = wal_config\n    self.quantization_config = quantization_config\n    self.init_from = init_from\n    self.wait_result_from_api = wait_result_from_api\n    self.create_if_not_exist = create_if_not_exist\n    self.recreate_index = recreate_index\n    self.payload_fields_to_index = payload_fields_to_index\n    self.use_sparse_embeddings = use_sparse_embeddings\n    self.sparse_idf = use_sparse_embeddings and sparse_idf\n    self.dimension = dimension\n    self.on_disk = on_disk\n    self.metric = metric\n    self.index_name = index_name\n    self.return_embedding = return_embedding\n    self.write_batch_size = write_batch_size\n    self.scroll_size = scroll_size\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Returns the number of documents present in the Document Store.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of documents in the Document Store.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"Returns the number of documents present in the Document Store.\n\n    Returns:\n        The number of documents in the Document Store.\n    \"\"\"\n    try:\n        response = self.client.count(\n            collection_name=self.index_name,\n        )\n        return response.count\n    except (UnexpectedResponse, ValueError):\n        # Qdrant local raises ValueError if the collection is not found, but\n        # with the remote server UnexpectedResponse is raised. Until that's unified,\n        # we need to catch both.\n        return 0\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Deletes documents that match the provided <code>document_ids</code> from the document store.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str] | None</code> <p>The document ids to delete.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"Deletes documents that match the provided `document_ids` from the document store.\n\n    Args:\n        document_ids: The document ids to delete.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n    \"\"\"\n    if delete_all:\n        self.client.delete_collection(collection_name=self.index_name)\n    elif document_ids:\n        ids = [convert_id(_id) for _id in document_ids]\n        try:\n            self.client.delete(\n                collection_name=self.index_name,\n                points_selector=ids,\n                wait=self.wait_result_from_api,\n            )\n        except KeyError:\n            logger.warning(\n                \"Called QdrantDocumentStore.delete_documents() on a non-existing ID\",\n            )\n    else:\n        raise ValueError(\"Either `document_ids` or `delete_all` must be provided.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the DocumentStore based on the provided file_id.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore based on the provided file_id.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents from the DocumentStore based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>The filters to apply to the document list.</p> required Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore based on the provided filters.\n\n    Args:\n        filters (dict[str, Any]): The filters to apply to the document list.\n    \"\"\"\n    if filters:\n        documents = self.filter_documents(filters=filters)\n        document_ids = [doc.id for doc in documents]\n        self.delete_documents(document_ids=document_ids)\n    else:\n        raise ValueError(\"No filters provided to delete documents.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.filter_documents","title":"<code>filter_documents(filters=None)</code>","text":"<p>Returns the documents that match the provided filters.</p> <p>For a detailed specification of the filters, refer to the documentation</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | Filter | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of documents that match the given filters.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def filter_documents(\n    self,\n    filters: dict[str, Any] | rest.Filter | None = None,\n) -&gt; list[Document]:\n    \"\"\"Returns the documents that match the provided filters.\n\n    For a detailed specification of the filters, refer to the\n    [documentation](https://docs.dynamiq.deepset.ai/docs/metadata-filtering)\n\n    Args:\n        filters: The filters to apply to the document list.\n\n    Returns:\n        A list of documents that match the given filters.\n    \"\"\"\n    if filters and not isinstance(filters, dict) and not isinstance(filters, rest.Filter):\n        msg = \"Filter must be a dictionary or an instance of `qdrant_client.http.models.Filter`\"\n        raise ValueError(msg)\n\n    if filters and not isinstance(filters, rest.Filter) and \"operator\" not in filters:\n        raise ValueError(\"Filter must contain an 'operator' key\")\n\n    return list(\n        self.get_documents_generator(\n            filters,\n        )\n    )\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.get_distance","title":"<code>get_distance(similarity)</code>","text":"<p>Retrieves the distance metric for the specified similarity measure.</p> <p>Parameters:</p> Name Type Description Default <code>similarity</code> <code>str</code> <p>The similarity measure to retrieve the distance.</p> required <p>Returns:</p> Type Description <code>Distance</code> <p>The corresponding rest.Distance object.</p> <p>Raises:</p> Type Description <code>QdrantStoreError</code> <p>If the provided similarity measure is not supported.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_distance(self, similarity: str) -&gt; rest.Distance:\n    \"\"\"Retrieves the distance metric for the specified similarity measure.\n\n    Args:\n        similarity: The similarity measure to retrieve the distance.\n\n    Returns:\n        The corresponding rest.Distance object.\n\n    Raises:\n        QdrantStoreError: If the provided similarity measure is not supported.\n    \"\"\"\n    try:\n        return self.DISTANCE_BY_SIMILARITY[similarity]\n    except KeyError as ke:\n        msg = (\n            f\"Provided similarity '{similarity}' is not supported by Qdrant \"\n            f\"document store. Please choose one of the options: \"\n            f\"{', '.join(self.DISTANCE_BY_SIMILARITY.keys())}\"\n        )\n        raise QdrantStoreError(msg) from ke\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.get_documents_by_id","title":"<code>get_documents_by_id(ids, index=None)</code>","text":"<p>Retrieves documents from Qdrant by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>A list of document IDs to retrieve.</p> required <code>index</code> <code>str | None</code> <p>The name of the index to retrieve documents from.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of documents.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_documents_by_id(\n    self,\n    ids: list[str],\n    index: str | None = None,\n) -&gt; list[Document]:\n    \"\"\"Retrieves documents from Qdrant by their IDs.\n\n    Args:\n        ids: A list of document IDs to retrieve.\n        index: The name of the index to retrieve documents from.\n\n    Returns:\n        A list of documents.\n    \"\"\"\n    index = index or self.index_name\n\n    documents: list[Document] = []\n\n    ids = [convert_id(_id) for _id in ids]\n    records = self.client.retrieve(\n        collection_name=index,\n        ids=ids,\n        with_payload=True,\n        with_vectors=True,\n    )\n\n    for record in records:\n        documents.append(\n            convert_qdrant_point_to_dynamiq_document(record, use_sparse_embeddings=self.use_sparse_embeddings)\n        )\n    return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.get_documents_generator","title":"<code>get_documents_generator(filters=None, include_embeddings=False)</code>","text":"<p>Returns a generator that yields documents from Qdrant based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | Filter | None</code> <p>Filters applied to the retrieved documents.</p> <code>None</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include the embeddings of the retrieved documents.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>A generator that yields documents retrieved from Qdrant.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_documents_generator(\n    self,\n    filters: dict[str, Any] | rest.Filter | None = None,\n    include_embeddings: bool = False,\n) -&gt; Generator[Document, None, None]:\n    \"\"\"Returns a generator that yields documents from Qdrant based on the provided filters.\n\n    Args:\n        filters: Filters applied to the retrieved documents.\n        include_embeddings: Whether to include the embeddings of the retrieved documents.\n\n    Returns:\n        A generator that yields documents retrieved from Qdrant.\n    \"\"\"\n\n    index = self.index_name\n    qdrant_filters = convert_filters_to_qdrant(filters)\n\n    next_offset = None\n    stop_scrolling = False\n    while not stop_scrolling:\n        records, next_offset = self.client.scroll(\n            collection_name=index,\n            scroll_filter=qdrant_filters,\n            limit=self.scroll_size,\n            offset=next_offset,\n            with_payload=True,\n            with_vectors=include_embeddings,\n        )\n        stop_scrolling = next_offset is None or (\n            isinstance(next_offset, grpc.PointId) and next_offset.num == 0 and next_offset.uuid == \"\"\n        )\n\n        for record in records:\n            yield convert_qdrant_point_to_dynamiq_document(record, use_sparse_embeddings=self.use_sparse_embeddings)\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False)</code>","text":"<p>Returns a list of all documents in the Document Store.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include the embeddings of the retrieved documents.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of all documents in the Document Store.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def list_documents(self, include_embeddings: bool = False) -&gt; list[Document]:\n    \"\"\"Returns a list of all documents in the Document Store.\n\n    Args:\n        include_embeddings: Whether to include the embeddings of the retrieved documents.\n\n    Returns:\n        A list of all documents in the Document Store.\n    \"\"\"\n    return list(self.get_documents_generator(include_embeddings=include_embeddings))\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.recreate_collection","title":"<code>recreate_collection(collection_name, distance, embedding_dim, on_disk=None, use_sparse_embeddings=None, sparse_idf=False)</code>","text":"<p>Recreates the Qdrant collection with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to recreate.</p> required <code>distance</code> <p>The distance metric to use for the collection.</p> required <code>embedding_dim</code> <code>int</code> <p>The dimension of the embeddings.</p> required <code>on_disk</code> <code>bool | None</code> <p>Whether to store the collection on disk.</p> <code>None</code> <code>use_sparse_embeddings</code> <code>bool | None</code> <p>Whether to use sparse embeddings.</p> <code>None</code> <code>sparse_idf</code> <code>bool</code> <p>Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required for BM42.</p> <code>False</code> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def recreate_collection(\n    self,\n    collection_name: str,\n    distance,\n    embedding_dim: int,\n    on_disk: bool | None = None,\n    use_sparse_embeddings: bool | None = None,\n    sparse_idf: bool = False,\n):\n    \"\"\"Recreates the Qdrant collection with the specified parameters.\n\n    Args:\n        collection_name: The name of the collection to recreate.\n        distance: The distance metric to use for the collection.\n        embedding_dim: The dimension of the embeddings.\n        on_disk: Whether to store the collection on disk.\n        use_sparse_embeddings: Whether to use sparse embeddings.\n        sparse_idf: Whether to compute the Inverse Document Frequency (IDF) when using sparse embeddings. Required\n            for BM42.\n    \"\"\"\n    if on_disk is None:\n        on_disk = self.on_disk\n\n    if use_sparse_embeddings is None:\n        use_sparse_embeddings = self.use_sparse_embeddings\n\n    # dense vectors configuration\n    vectors_config = rest.VectorParams(size=embedding_dim, on_disk=on_disk, distance=distance)\n\n    if use_sparse_embeddings:\n        # in this case, we need to define named vectors\n        vectors_config = {DENSE_VECTORS_NAME: vectors_config}\n\n        sparse_vectors_config = {\n            SPARSE_VECTORS_NAME: rest.SparseVectorParams(\n                index=rest.SparseIndexParams(\n                    on_disk=on_disk,\n                ),\n                modifier=rest.Modifier.IDF if sparse_idf else None,\n            ),\n        }\n\n    if self.client.collection_exists(collection_name):\n        self.client.delete_collection(collection_name)\n\n    self.client.create_collection(\n        collection_name=collection_name,\n        vectors_config=vectors_config,\n        sparse_vectors_config=sparse_vectors_config if use_sparse_embeddings else None,\n        shard_number=self.shard_number,\n        replication_factor=self.replication_factor,\n        write_consistency_factor=self.write_consistency_factor,\n        on_disk_payload=self.on_disk_payload,\n        hnsw_config=self.hnsw_config,\n        optimizers_config=self.optimizers_config,\n        wal_config=self.wal_config,\n        quantization_config=self.quantization_config,\n        init_from=self.init_from,\n    )\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.QdrantVectorStore.write_documents","title":"<code>write_documents(documents, policy=DuplicatePolicy.FAIL)</code>","text":"<p>Writes documents to Qdrant using the specified policy.</p> <p>The QdrantDocumentStore can handle duplicate documents based on the given policy. The available policies are: - <code>FAIL</code>: The operation will raise an error if any document already exists. - <code>OVERWRITE</code>: Existing documents will be overwritten with the new ones. - <code>SKIP</code>: Existing documents will be skipped, and only new documents will be added.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>A list of Document objects to write to Qdrant.</p> required <code>policy</code> <code>DuplicatePolicy</code> <p>The policy for handling duplicate documents.</p> <code>FAIL</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of documents written to the document store.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def write_documents(\n    self,\n    documents: list[Document],\n    policy: DuplicatePolicy = DuplicatePolicy.FAIL,\n) -&gt; int:\n    \"\"\"Writes documents to Qdrant using the specified policy.\n\n    The QdrantDocumentStore can handle duplicate documents based on the given policy. The available policies are:\n    - `FAIL`: The operation will raise an error if any document already exists.\n    - `OVERWRITE`: Existing documents will be overwritten with the new ones.\n    - `SKIP`: Existing documents will be skipped, and only new documents will be added.\n\n    Args:\n        documents: A list of Document objects to write to Qdrant.\n        policy: The policy for handling duplicate documents.\n\n    Returns:\n        The number of documents written to the document store.\n    \"\"\"\n    for doc in documents:\n        if not isinstance(doc, Document):\n            msg = f\"DocumentStore.write_documents() expects a list of Documents but got an element of {type(doc)}.\"\n            raise ValueError(msg)\n\n    if len(documents) == 0:\n        logger.warning(\"Calling QdrantDocumentStore.write_documents() with empty list\")\n        return 0\n\n    document_objects = self._handle_duplicate_documents(\n        documents=documents,\n        index=self.index_name,\n        policy=policy,\n    )\n\n    batched_documents = get_batches_from_generator(document_objects, self.write_batch_size)\n    for document_batch in batched_documents:\n        batch = convert_dynamiq_documents_to_qdrant_points(\n            document_batch,\n            use_sparse_embeddings=self.use_sparse_embeddings,\n        )\n\n        self.client.upsert(\n            collection_name=self.index_name,\n            points=batch,\n            wait=self.wait_result_from_api,\n        )\n\n    return len(document_objects)\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.SparseEmbedding","title":"<code>SparseEmbedding</code>","text":"<p>Class representing a sparse embedding.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>class SparseEmbedding:\n    \"\"\"Class representing a sparse embedding.\"\"\"\n\n    indices: list[int]\n    values: list[float]\n</code></pre>"},{"location":"dynamiq/storages/vector/qdrant/qdrant/#dynamiq.storages.vector.qdrant.qdrant.get_batches_from_generator","title":"<code>get_batches_from_generator(iterable, n)</code>","text":"<p>Batch elements of an iterable into fixed-length chunks or blocks.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <p>The iterable to batch.</p> required <code>n</code> <p>The size of each batch.</p> required <p>Yields:</p> Type Description <p>Batches of the iterable.</p> Source code in <code>dynamiq/storages/vector/qdrant/qdrant.py</code> <pre><code>def get_batches_from_generator(iterable, n):\n    \"\"\"Batch elements of an iterable into fixed-length chunks or blocks.\n\n    Args:\n        iterable: The iterable to batch.\n        n: The size of each batch.\n\n    Yields:\n        Batches of the iterable.\n    \"\"\"\n    it = iter(iterable)\n    x = tuple(islice(it, n))\n    while x:\n        yield x\n        x = tuple(islice(it, n))\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/filters/","title":"Filters","text":""},{"location":"dynamiq/storages/vector/weaviate/filters/#dynamiq.storages.vector.weaviate.filters.convert_filters","title":"<code>convert_filters(filters)</code>","text":"<p>Convert filters from dynamiq format to Weaviate format.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>Filters in dynamiq format.</p> required <p>Returns:</p> Name Type Description <code>FilterReturn</code> <code>FilterReturn</code> <p>Filters in Weaviate format.</p> <p>Raises:</p> Type Description <code>VectorStoreFilterException</code> <p>If filters are not a dictionary.</p> Source code in <code>dynamiq/storages/vector/weaviate/filters.py</code> <pre><code>def convert_filters(filters: dict[str, Any]) -&gt; FilterReturn:\n    \"\"\"\n    Convert filters from dynamiq format to Weaviate format.\n\n    Args:\n        filters (dict[str, Any]): Filters in dynamiq format.\n\n    Returns:\n        FilterReturn: Filters in Weaviate format.\n\n    Raises:\n        VectorStoreFilterException: If filters are not a dictionary.\n    \"\"\"\n    if not isinstance(filters, dict):\n        msg = \"Filters must be a dictionary\"\n        raise VectorStoreFilterException(msg)\n\n    if \"field\" in filters:\n        return Filter.all_of([_parse_comparison_condition(filters)])\n    return _parse_logical_condition(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/","title":"Weaviate","text":""},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore","title":"<code>WeaviateVectorStore</code>","text":"<p>A Document Store for Weaviate.</p> <p>This class can be used with Weaviate Cloud Services or self-hosted instances.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>class WeaviateVectorStore:\n    \"\"\"\n    A Document Store for Weaviate.\n\n    This class can be used with Weaviate Cloud Services or self-hosted instances.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection: Weaviate | None = None,\n        client: Optional[\"WeaviateClient\"] = None,\n        index_name: str = \"default\",\n        create_if_not_exist: bool = False,\n    ):\n        \"\"\"\n        Initialize a new instance of WeaviateDocumentStore and connect to the Weaviate instance.\n\n        Args:\n            connection (Weaviate | None): A Weaviate connection object. If None, a new one is created.\n            client (Optional[WeaviateClient]): A Weaviate client. If None, one is created from the connection.\n            index_name (str): The name of the index to use. Defaults to \"default\".\n        \"\"\"\n        self.client = client\n        if self.client is None:\n            if connection is None:\n                connection = Weaviate()\n            self.client = connection.connect()\n\n        collection_settings = {\n            \"class\": index_name,\n            \"invertedIndexConfig\": {\"indexNullState\": True},\n        }\n\n        if not self.client.collections.exists(collection_settings[\"class\"]):\n            if create_if_not_exist:\n                self.client.collections.create_from_dict(collection_settings)\n            else:\n                raise ValueError(\n                    f\"Collection '{collection_settings['class']}' does not exist.\"\n                    \" Set 'create_if_not_exist' to True to create it.\"\n                )\n\n        self._collection_settings = collection_settings\n        self._collection = self.client.collections.get(collection_settings[\"class\"])\n\n    def close(self):\n        \"\"\"Close the connection to Weaviate.\"\"\"\n        if self.client:\n            self.client.close()\n\n    def count_documents(self) -&gt; int:\n        \"\"\"\n        Count the number of documents in the DocumentStore.\n\n        Returns:\n            int: The number of documents in the store.\n        \"\"\"\n        total = self._collection.aggregate.over_all(total_count=True).total_count\n        return total if total else 0\n\n    def _to_data_object(self, document: Document) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert a Document to a Weaviate data object ready to be saved.\n\n        Args:\n            document (Document): The document to convert.\n\n        Returns:\n            dict[str, Any]: A dictionary representing the Weaviate data object.\n        \"\"\"\n        data = document.to_dict()\n        data[\"_original_id\"] = data.pop(\"id\")\n        metadata = data.get(\"metadata\", {})\n\n        for key, val in metadata.items():\n            data[key] = val\n\n        del data[\"embedding\"]\n        del data[\"metadata\"]\n\n        return data\n\n    def _to_document(self, data: \"DataObject[dict[str, Any], None]\") -&gt; Document:\n        \"\"\"\n        Convert a data object read from Weaviate into a Document.\n\n        Args:\n            data (DataObject[dict[str, Any], None]): The data object from Weaviate.\n\n        Returns:\n            Document: The converted Document object.\n        \"\"\"\n        document_data = data.properties\n        document_id = document_data.pop(\"_original_id\")\n\n        content = document_data.pop(\"content\")\n\n        if isinstance(data.vector, list):\n            document_data[\"embedding\"] = data.vector\n        elif isinstance(data.vector, dict):\n            document_data[\"embedding\"] = data.vector.get(\"default\")\n        else:\n            document_data[\"embedding\"] = None\n\n        for key, value in document_data.items():\n            if isinstance(value, datetime.datetime):\n                document_data[key] = value.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n        if weaviate_meta := getattr(data, \"metadata\", None):\n            if weaviate_meta.score is not None:\n                document_data[\"score\"] = weaviate_meta.score\n            elif weaviate_meta.certainty is not None:\n                document_data[\"score\"] = weaviate_meta.certainty\n\n        score = document_data.pop(\"score\", None)\n        embedding = document_data.pop(\"embedding\", None)\n\n        data = {\n            \"id\": str(document_id),\n            \"content\": content,\n            \"metadata\": document_data,\n            \"score\": score,\n            \"embedding\": embedding,\n        }\n\n        logger.debug(f\"Document loaded from Weaviate: {data}\")\n\n        return Document(**data)\n\n    def _query(self) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Query all documents from Weaviate.\n\n        Returns:\n            list[dict[str, Any]]: A list of all documents in the store.\n\n        Raises:\n            VectorStoreException: If the query fails.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n        try:\n            result = self._collection.iterator(\n                include_vector=True, return_properties=properties\n            )\n        except WeaviateQueryError as e:\n            msg = f\"Failed to query documents in Weaviate. Error: {e.message}\"\n            raise VectorStoreException(msg) from e\n        return result\n\n    def _query_with_filters(self, filters: dict[str, Any]) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Query documents from Weaviate with filters.\n\n        Args:\n            filters (dict[str, Any]): The filters to apply to the query.\n\n        Returns:\n            list[dict[str, Any]]: A list of documents matching the filters.\n\n        Raises:\n            VectorStoreException: If the query fails.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n\n        offset = 0\n        partial_result = None\n        result = []\n        while (\n            partial_result is None or len(partial_result.objects) == DEFAULT_QUERY_LIMIT\n        ):\n            try:\n                partial_result = self._collection.query.fetch_objects(\n                    filters=convert_filters(filters),\n                    include_vector=True,\n                    limit=DEFAULT_QUERY_LIMIT,\n                    offset=offset,\n                    return_properties=properties,\n                )\n            except WeaviateQueryError as e:\n                msg = f\"Failed to query documents in Weaviate. Error: {e.message}\"\n                raise VectorStoreException(msg) from e\n            result.extend(partial_result.objects)\n            offset += DEFAULT_QUERY_LIMIT\n        return result\n\n    def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n        \"\"\"\n        Filter documents based on the provided filters.\n\n        Args:\n            filters (dict[str, Any] | None): The filters to apply to the document list.\n\n        Returns:\n            list[Document]: A list of Documents that match the given filters.\n        \"\"\"\n        if filters:\n            result = self._query_with_filters(filters)\n        else:\n            result = self._query()\n        return [self._to_document(doc) for doc in result]\n\n    def list_documents(self, include_embeddings: bool = False) -&gt; list[Document]:\n        \"\"\"\n        List all documents in the DocumentStore.\n\n        Args:\n            include_embeddings (bool): Whether to include document embeddings in the result.\n\n        Returns:\n            list[Document]: A list of all documents in the store.\n        \"\"\"\n        documents = []\n        for item in self._collection.iterator(\n            include_vector=include_embeddings\n            # If using named vectors, you can specify ones to include e.g. ['title', 'body'], or True to include all\n        ):\n            document = self._to_document(item)\n            documents.append(document)\n        return documents\n\n    def _batch_write(self, documents: list[Document]) -&gt; int:\n        \"\"\"\n        Write documents to Weaviate in batches.\n\n        Args:\n            documents (list[Document]): The list of documents to write.\n\n        Returns:\n            int: The number of documents written.\n\n        Raises:\n            ValueError: If any of the input is not a Document.\n            VectorStoreException: If the write operation fails.\n        \"\"\"\n        with self.client.batch.dynamic() as batch:\n            for doc in documents:\n                if not isinstance(doc, Document):\n                    msg = f\"Expected a Document, got '{type(doc)}' instead.\"\n                    raise ValueError(msg)\n\n                batch.add_object(\n                    properties=self._to_data_object(doc),\n                    collection=self._collection.name,\n                    uuid=generate_uuid5(doc.id),\n                    vector=doc.embedding,\n                )\n        if failed_objects := self.client.batch.failed_objects:\n            mapped_objects = {}\n            for obj in failed_objects:\n                properties = obj.object_.properties or {}\n                id_ = properties.get(\"_original_id\", obj.object_.uuid)\n                mapped_objects[id_] = obj.data\n\n            msg = \"\\n\".join(\n                [\n                    f\"Failed to write object with id '{id_}'. Error: '{message}'\"\n                    for id_, message in mapped_objects.items()\n                ]\n            )\n            raise VectorStoreException(msg)\n\n        return len(documents)\n\n    def _write(self, documents: list[Document], policy: DuplicatePolicy) -&gt; int:\n        \"\"\"\n        Write documents to Weaviate using the specified policy.\n\n        Args:\n            documents (list[Document]): The list of documents to write.\n            policy (DuplicatePolicy): The policy to use for handling duplicates.\n\n        Returns:\n            int: The number of documents written.\n\n        Raises:\n            ValueError: If any of the input is not a Document.\n            VectorStoreDuplicateDocumentException: If duplicates are found with FAIL policy.\n        \"\"\"\n        written = 0\n        duplicate_errors_ids = []\n        for doc in documents:\n            if not isinstance(doc, Document):\n                msg = f\"Expected a Document, got '{type(doc)}' instead.\"\n                raise ValueError(msg)\n\n            if policy == DuplicatePolicy.SKIP and self._collection.data.exists(\n                uuid=generate_uuid5(doc.id)\n            ):\n                continue\n\n            try:\n                self._collection.data.insert(\n                    uuid=generate_uuid5(doc.id),\n                    properties=self._to_data_object(doc),\n                    vector=doc.embedding,\n                )\n\n                written += 1\n            except UnexpectedStatusCodeError:\n                if policy == DuplicatePolicy.FAIL:\n                    duplicate_errors_ids.append(doc.id)\n        if duplicate_errors_ids:\n            msg = f\"IDs '{', '.join(duplicate_errors_ids)}' already exist in the document store.\"\n            raise VectorStoreDuplicateDocumentException(msg)\n        return written\n\n    def write_documents(\n        self, documents: list[Document], policy: DuplicatePolicy = DuplicatePolicy.NONE\n    ) -&gt; int:\n        \"\"\"\n        Write documents to Weaviate using the specified policy.\n\n        Args:\n            documents (list[Document]): The list of documents to write.\n            policy (DuplicatePolicy): The policy to use for handling duplicates.\n\n        Returns:\n            int: The number of documents written.\n        \"\"\"\n        if policy in [DuplicatePolicy.NONE, DuplicatePolicy.OVERWRITE]:\n            return self._batch_write(documents)\n\n        return self._write(documents, policy)\n\n    def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore.\n\n        Args:\n            document_ids (list[str], optional): The IDs of documents to delete.\n            delete_all (bool): If True, delete all documents. Defaults to False.\n\n        Raises:\n            ValueError: If neither document_ids nor delete_all is provided.\n        \"\"\"\n        if delete_all:\n            weaviate_ids = [item.uuid for item in self._collection.iterator()]\n        elif document_ids:\n            weaviate_ids = [generate_uuid5(doc_id) for doc_id in document_ids]\n        else:\n            msg = \"Either 'document_ids' or 'delete_all' must be set.\"\n            raise ValueError(msg)\n        self._collection.data.delete_many(\n            where=Filter.by_id().contains_any(weaviate_ids)\n        )\n\n    def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore based on the provided filters.\n\n        Args:\n            filters (dict[str, Any]): The filters to apply to the document list.\n        \"\"\"\n        if filters:\n            self._collection.data.delete_many(where=convert_filters(filters))\n        else:\n            raise ValueError(\"No filters provided to delete documents.\")\n\n    def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n        \"\"\"\n        Delete documents from the DocumentStore based on the provided file_id.\n\n        Args:\n            file_id (str): The file ID to filter by.\n        \"\"\"\n        filters = create_file_id_filter(file_id)\n        self.delete_documents_by_filters(filters)\n\n    def _bm25_retrieval(\n        self,\n        query: str,\n        filters: dict[str, Any] | None = None,\n        top_k: int | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform BM25 retrieval on the documents.\n\n        Args:\n            query (str): The query string.\n            filters (dict[str, Any] | None): Filters to apply to the query.\n            top_k (int | None): The number of top results to return.\n\n        Returns:\n            list[Document]: A list of retrieved documents.\n        \"\"\"\n        properties = [p.name for p in self._collection.config.get().properties]\n        result = self._collection.query.bm25(\n            query=query,\n            filters=convert_filters(filters) if filters else None,\n            limit=top_k,\n            include_vector=True,\n            query_properties=[\"content\"],\n            return_properties=properties,\n            return_metadata=[\"score\"],\n        )\n\n        return [self._to_document(doc) for doc in result.objects]\n\n    def _embedding_retrieval(\n        self,\n        query_embedding: list[float],\n        filters: dict[str, Any] | None = None,\n        top_k: int | None = None,\n        exclude_document_embeddings=True,\n        distance: float | None = None,\n        certainty: float | None = None,\n    ) -&gt; list[Document]:\n        \"\"\"\n        Perform embedding-based retrieval on the documents.\n\n        Args:\n            query_embedding (list[float]): The query embedding.\n            filters (dict[str, Any] | None): Filters to apply to the query.\n            top_k (int | None): The number of top results to return.\n            exclude_document_embeddings (bool): Whether to exclude document embeddings in the result.\n            distance (float | None): The maximum distance for retrieval.\n            certainty (float | None): The minimum certainty for retrieval.\n\n        Returns:\n            list[Document]: A list of retrieved documents.\n\n        Raises:\n            ValueError: If both distance and certainty are provided.\n        \"\"\"\n        if distance is not None and certainty is not None:\n            msg = \"Can't use 'distance' and 'certainty' parameters together\"\n            raise ValueError(msg)\n\n        properties = [p.name for p in self._collection.config.get().properties]\n        result = self._collection.query.near_vector(\n            near_vector=query_embedding,\n            distance=distance,\n            certainty=certainty,\n            include_vector=not exclude_document_embeddings,\n            filters=convert_filters(filters) if filters else None,\n            limit=top_k,\n            return_properties=properties,\n            return_metadata=[\"certainty\"],\n        )\n\n        return [self._to_document(doc) for doc in result.objects]\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.__init__","title":"<code>__init__(connection=None, client=None, index_name='default', create_if_not_exist=False)</code>","text":"<p>Initialize a new instance of WeaviateDocumentStore and connect to the Weaviate instance.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Weaviate | None</code> <p>A Weaviate connection object. If None, a new one is created.</p> <code>None</code> <code>client</code> <code>Optional[WeaviateClient]</code> <p>A Weaviate client. If None, one is created from the connection.</p> <code>None</code> <code>index_name</code> <code>str</code> <p>The name of the index to use. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def __init__(\n    self,\n    connection: Weaviate | None = None,\n    client: Optional[\"WeaviateClient\"] = None,\n    index_name: str = \"default\",\n    create_if_not_exist: bool = False,\n):\n    \"\"\"\n    Initialize a new instance of WeaviateDocumentStore and connect to the Weaviate instance.\n\n    Args:\n        connection (Weaviate | None): A Weaviate connection object. If None, a new one is created.\n        client (Optional[WeaviateClient]): A Weaviate client. If None, one is created from the connection.\n        index_name (str): The name of the index to use. Defaults to \"default\".\n    \"\"\"\n    self.client = client\n    if self.client is None:\n        if connection is None:\n            connection = Weaviate()\n        self.client = connection.connect()\n\n    collection_settings = {\n        \"class\": index_name,\n        \"invertedIndexConfig\": {\"indexNullState\": True},\n    }\n\n    if not self.client.collections.exists(collection_settings[\"class\"]):\n        if create_if_not_exist:\n            self.client.collections.create_from_dict(collection_settings)\n        else:\n            raise ValueError(\n                f\"Collection '{collection_settings['class']}' does not exist.\"\n                \" Set 'create_if_not_exist' to True to create it.\"\n            )\n\n    self._collection_settings = collection_settings\n    self._collection = self.client.collections.get(collection_settings[\"class\"])\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.close","title":"<code>close()</code>","text":"<p>Close the connection to Weaviate.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def close(self):\n    \"\"\"Close the connection to Weaviate.\"\"\"\n    if self.client:\n        self.client.close()\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.count_documents","title":"<code>count_documents()</code>","text":"<p>Count the number of documents in the DocumentStore.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents in the store.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def count_documents(self) -&gt; int:\n    \"\"\"\n    Count the number of documents in the DocumentStore.\n\n    Returns:\n        int: The number of documents in the store.\n    \"\"\"\n    total = self._collection.aggregate.over_all(total_count=True).total_count\n    return total if total else 0\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.delete_documents","title":"<code>delete_documents(document_ids=None, delete_all=False)</code>","text":"<p>Delete documents from the DocumentStore.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>list[str]</code> <p>The IDs of documents to delete.</p> <code>None</code> <code>delete_all</code> <code>bool</code> <p>If True, delete all documents. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither document_ids nor delete_all is provided.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def delete_documents(self, document_ids: list[str] | None = None, delete_all: bool = False) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore.\n\n    Args:\n        document_ids (list[str], optional): The IDs of documents to delete.\n        delete_all (bool): If True, delete all documents. Defaults to False.\n\n    Raises:\n        ValueError: If neither document_ids nor delete_all is provided.\n    \"\"\"\n    if delete_all:\n        weaviate_ids = [item.uuid for item in self._collection.iterator()]\n    elif document_ids:\n        weaviate_ids = [generate_uuid5(doc_id) for doc_id in document_ids]\n    else:\n        msg = \"Either 'document_ids' or 'delete_all' must be set.\"\n        raise ValueError(msg)\n    self._collection.data.delete_many(\n        where=Filter.by_id().contains_any(weaviate_ids)\n    )\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.delete_documents_by_file_id","title":"<code>delete_documents_by_file_id(file_id)</code>","text":"<p>Delete documents from the DocumentStore based on the provided file_id.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The file ID to filter by.</p> required Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def delete_documents_by_file_id(self, file_id: str) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore based on the provided file_id.\n\n    Args:\n        file_id (str): The file ID to filter by.\n    \"\"\"\n    filters = create_file_id_filter(file_id)\n    self.delete_documents_by_filters(filters)\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.delete_documents_by_filters","title":"<code>delete_documents_by_filters(filters)</code>","text":"<p>Delete documents from the DocumentStore based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any]</code> <p>The filters to apply to the document list.</p> required Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def delete_documents_by_filters(self, filters: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Delete documents from the DocumentStore based on the provided filters.\n\n    Args:\n        filters (dict[str, Any]): The filters to apply to the document list.\n    \"\"\"\n    if filters:\n        self._collection.data.delete_many(where=convert_filters(filters))\n    else:\n        raise ValueError(\"No filters provided to delete documents.\")\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.filter_documents","title":"<code>filter_documents(filters=None)</code>","text":"<p>Filter documents based on the provided filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict[str, Any] | None</code> <p>The filters to apply to the document list.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of Documents that match the given filters.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def filter_documents(self, filters: dict[str, Any] | None = None) -&gt; list[Document]:\n    \"\"\"\n    Filter documents based on the provided filters.\n\n    Args:\n        filters (dict[str, Any] | None): The filters to apply to the document list.\n\n    Returns:\n        list[Document]: A list of Documents that match the given filters.\n    \"\"\"\n    if filters:\n        result = self._query_with_filters(filters)\n    else:\n        result = self._query()\n    return [self._to_document(doc) for doc in result]\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.list_documents","title":"<code>list_documents(include_embeddings=False)</code>","text":"<p>List all documents in the DocumentStore.</p> <p>Parameters:</p> Name Type Description Default <code>include_embeddings</code> <code>bool</code> <p>Whether to include document embeddings in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>list[Document]: A list of all documents in the store.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def list_documents(self, include_embeddings: bool = False) -&gt; list[Document]:\n    \"\"\"\n    List all documents in the DocumentStore.\n\n    Args:\n        include_embeddings (bool): Whether to include document embeddings in the result.\n\n    Returns:\n        list[Document]: A list of all documents in the store.\n    \"\"\"\n    documents = []\n    for item in self._collection.iterator(\n        include_vector=include_embeddings\n        # If using named vectors, you can specify ones to include e.g. ['title', 'body'], or True to include all\n    ):\n        document = self._to_document(item)\n        documents.append(document)\n    return documents\n</code></pre>"},{"location":"dynamiq/storages/vector/weaviate/weaviate/#dynamiq.storages.vector.weaviate.weaviate.WeaviateVectorStore.write_documents","title":"<code>write_documents(documents, policy=DuplicatePolicy.NONE)</code>","text":"<p>Write documents to Weaviate using the specified policy.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>The list of documents to write.</p> required <code>policy</code> <code>DuplicatePolicy</code> <p>The policy to use for handling duplicates.</p> <code>NONE</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of documents written.</p> Source code in <code>dynamiq/storages/vector/weaviate/weaviate.py</code> <pre><code>def write_documents(\n    self, documents: list[Document], policy: DuplicatePolicy = DuplicatePolicy.NONE\n) -&gt; int:\n    \"\"\"\n    Write documents to Weaviate using the specified policy.\n\n    Args:\n        documents (list[Document]): The list of documents to write.\n        policy (DuplicatePolicy): The policy to use for handling duplicates.\n\n    Returns:\n        int: The number of documents written.\n    \"\"\"\n    if policy in [DuplicatePolicy.NONE, DuplicatePolicy.OVERWRITE]:\n        return self._batch_write(documents)\n\n    return self._write(documents, policy)\n</code></pre>"},{"location":"dynamiq/types/document/","title":"Document","text":""},{"location":"dynamiq/types/document/#dynamiq.types.document.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document class for Dynamiq.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Callable[[], Any] | str | None</code> <p>Unique identifier. Defaults to UUID4 hex.</p> <code>content</code> <code>str</code> <p>Main content of the document.</p> <code>metadata</code> <code>dict | None</code> <p>Additional metadata. Defaults to None.</p> <code>embedding</code> <code>list | None</code> <p>Vector representation. Defaults to None.</p> <code>score</code> <code>float | None</code> <p>Relevance or similarity score. Defaults to None.</p> Source code in <code>dynamiq/types/document.py</code> <pre><code>class Document(BaseModel):\n    \"\"\"Document class for Dynamiq.\n\n    Attributes:\n        id (Callable[[], Any] | str | None): Unique identifier. Defaults to UUID4 hex.\n        content (str): Main content of the document.\n        metadata (dict | None): Additional metadata. Defaults to None.\n        embedding (list | None): Vector representation. Defaults to None.\n        score (float | None): Relevance or similarity score. Defaults to None.\n    \"\"\"\n    id: Callable[[], Any] | str | None = Field(default_factory=lambda: uuid.uuid4().hex)\n    content: str\n    metadata: dict | None = None\n    embedding: list | None = None\n    score: float | None = None\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert the Document object to a dictionary.\n\n        Returns:\n            dict: Dictionary representation of the Document.\n        \"\"\"\n        return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/document/#dynamiq.types.document.Document.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert the Document object to a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation of the Document.</p> Source code in <code>dynamiq/types/document.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert the Document object to a dictionary.\n\n    Returns:\n        dict: Dictionary representation of the Document.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/document/#dynamiq.types.document.DocumentCreationMode","title":"<code>DocumentCreationMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for document creation modes.</p> Source code in <code>dynamiq/types/document.py</code> <pre><code>class DocumentCreationMode(str, enum.Enum):\n    \"\"\"Enumeration for document creation modes.\"\"\"\n    ONE_DOC_PER_FILE = \"one-doc-per-file\"\n    ONE_DOC_PER_PAGE = \"one-doc-per-page\"\n    ONE_DOC_PER_ELEMENT = \"one-doc-per-element\"\n</code></pre>"},{"location":"dynamiq/types/streaming/","title":"Streaming","text":""},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingConfig","title":"<code>StreamingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for streaming.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether streaming is enabled. Defaults to False.</p> <code>event</code> <code>str</code> <p>Event name. Defaults to \"streaming\".</p> <code>timeout</code> <code>float | None</code> <p>Timeout for streaming. Defaults to None.</p> <code>input_queue</code> <code>Queue | None</code> <p>Input queue for streaming. Defaults to None.</p> <code>input_queue_done_event</code> <code>Event | None</code> <p>Event to signal input queue completion. Defaults to None.</p> <code>mode</code> <code>StreamingMode</code> <p>Streaming mode. Defaults to StreamingMode.ANSWER.</p> <code>by_tokens</code> <code>bool</code> <p>Whether to stream  by tokens. Defaults to False.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingConfig(BaseModel):\n    \"\"\"Configuration for streaming.\n\n    Attributes:\n        enabled (bool): Whether streaming is enabled. Defaults to False.\n        event (str): Event name. Defaults to \"streaming\".\n        timeout (float | None): Timeout for streaming. Defaults to None.\n        input_queue (Queue | None): Input queue for streaming. Defaults to None.\n        input_queue_done_event (Event | None): Event to signal input queue completion. Defaults to None.\n        mode (StreamingMode): Streaming mode. Defaults to StreamingMode.ANSWER.\n        by_tokens (bool): Whether to stream  by tokens. Defaults to False.\n    \"\"\"\n    enabled: bool = False\n    event: str = STREAMING_EVENT\n    timeout: float | None = None\n    input_queue: Queue | None = None\n    input_queue_done_event: Event | None = None\n    mode: StreamingMode = StreamingMode.FINAL\n    by_tokens: bool = True\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @cached_property\n    def input_streaming_enabled(self) -&gt; bool:\n        \"\"\"Check if input streaming is enabled.\n\n        Returns:\n            bool: True if input streaming is enabled, False otherwise.\n        \"\"\"\n        return self.enabled and self.input_queue\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingConfig.input_streaming_enabled","title":"<code>input_streaming_enabled: bool</code>  <code>cached</code> <code>property</code>","text":"<p>Check if input streaming is enabled.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if input streaming is enabled, False otherwise.</p>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage","title":"<code>StreamingEventMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message for streaming events.</p> <p>Attributes:</p> Name Type Description <code>run_id</code> <code>str | None</code> <p>Run ID.</p> <code>wf_run_id</code> <code>str | None</code> <p>Workflow run ID. Defaults to a generated UUID.</p> <code>entity_id</code> <code>str</code> <p>Entity ID.</p> <code>data</code> <code>Any</code> <p>Data associated with the event.</p> <code>event</code> <code>str | None</code> <p>Event name. Defaults to \"streaming\".</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingEventMessage(BaseModel):\n    \"\"\"Message for streaming events.\n\n    Attributes:\n        run_id (str | None): Run ID.\n        wf_run_id (str | None): Workflow run ID. Defaults to a generated UUID.\n        entity_id (str): Entity ID.\n        data (Any): Data associated with the event.\n        event (str | None): Event name. Defaults to \"streaming\".\n    \"\"\"\n    run_id: str | None = None\n    wf_run_id: str | None = Field(default_factory=generate_uuid)\n    entity_id: str\n    data: Any\n    event: str | None = None\n\n    @field_validator(\"event\")\n    @classmethod\n    def set_event(cls, value: str | None) -&gt; str:\n        \"\"\"Set the event name.\n\n        Args:\n            value (str | None): Event name.\n\n        Returns:\n            str: Event name or default.\n        \"\"\"\n        return value or STREAMING_EVENT\n\n    def to_dict(self, **kwargs) -&gt; dict:\n        \"\"\"Convert to dictionary.\n\n        Returns:\n            dict: Dictionary representation.\n        \"\"\"\n        return self.model_dump(**kwargs)\n\n    def to_json(self, **kwargs) -&gt; str:\n        \"\"\"Convert to JSON string.\n\n        Returns:\n            str: JSON string representation.\n        \"\"\"\n        return self.model_dump_json(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage.set_event","title":"<code>set_event(value)</code>  <code>classmethod</code>","text":"<p>Set the event name.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>Event name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Event name or default.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>@field_validator(\"event\")\n@classmethod\ndef set_event(cls, value: str | None) -&gt; str:\n    \"\"\"Set the event name.\n\n    Args:\n        value (str | None): Event name.\n\n    Returns:\n        str: Event name or default.\n    \"\"\"\n    return value or STREAMING_EVENT\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage.to_dict","title":"<code>to_dict(**kwargs)</code>","text":"<p>Convert to dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary representation.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>def to_dict(self, **kwargs) -&gt; dict:\n    \"\"\"Convert to dictionary.\n\n    Returns:\n        dict: Dictionary representation.\n    \"\"\"\n    return self.model_dump(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingEventMessage.to_json","title":"<code>to_json(**kwargs)</code>","text":"<p>Convert to JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string representation.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>def to_json(self, **kwargs) -&gt; str:\n    \"\"\"Convert to JSON string.\n\n    Returns:\n        str: JSON string representation.\n    \"\"\"\n    return self.model_dump_json(**kwargs)\n</code></pre>"},{"location":"dynamiq/types/streaming/#dynamiq.types.streaming.StreamingMode","title":"<code>StreamingMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for streaming modes.</p> Source code in <code>dynamiq/types/streaming.py</code> <pre><code>class StreamingMode(Enum):\n    \"\"\"Enumeration for streaming modes.\"\"\"\n\n    FINAL = \"final\"  # Only final output\n    ALL = \"all\"  # All intermediate steps and final output\n</code></pre>"},{"location":"dynamiq/utils/chat/","title":"Chat","text":""},{"location":"dynamiq/utils/chat/#dynamiq.utils.chat.format_chat_history","title":"<code>format_chat_history(chat_history)</code>","text":"<p>Format chat history for the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>list[dict[str, str]]</code> <p>List of chat entries.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted chat history.</p> Source code in <code>dynamiq/utils/chat.py</code> <pre><code>def format_chat_history(chat_history: list[dict[str, str]]) -&gt; str:\n    \"\"\"Format chat history for the orchestrator.\n\n    Args:\n        chat_history (list[dict[str, str]]): List of chat entries.\n\n    Returns:\n        str: Formatted chat history.\n    \"\"\"\n    formatted_history = \"\"\n    for entry in chat_history:\n        role = entry[\"role\"].title()\n        content = entry[\"content\"]\n        formatted_history += f\"{role}: {content}\\n\"\n    return formatted_history\n</code></pre>"},{"location":"dynamiq/utils/duration/","title":"Duration","text":""},{"location":"dynamiq/utils/duration/#dynamiq.utils.duration.format_duration","title":"<code>format_duration(start, end)</code>","text":"<p>Format the duration between two datetime objects into a human-readable string.</p> <p>This function calculates the time difference between the start and end datetimes and returns a formatted string representing the duration in milliseconds, seconds, minutes, or hours, depending on the length of the duration.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>The starting datetime.</p> required <code>end</code> <code>datetime</code> <p>The ending datetime.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string representing the duration.  - For durations less than 1 second: \"Xms\" (milliseconds)  - For durations between 1 second and 1 minute: \"Xs\" (seconds)  - For durations between 1 minute and 1 hour: \"Xm\" (minutes)  - For durations of 1 hour or more: \"Xh\" (hours)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime, timedelta\n&gt;&gt;&gt; start = datetime(2023, 1, 1, 12, 0, 0)\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(milliseconds=500)))\n500ms\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(seconds=45)))\n45.0s\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(minutes=30)))\n30.0m\n&gt;&gt;&gt; print(format_duration(start, start + timedelta(hours=2)))\n2.0h\n</code></pre> Source code in <code>dynamiq/utils/duration.py</code> <pre><code>def format_duration(start: datetime, end: datetime) -&gt; str:\n    \"\"\"\n    Format the duration between two datetime objects into a human-readable string.\n\n    This function calculates the time difference between the start and end datetimes and\n    returns a formatted string representing the duration in milliseconds, seconds, minutes,\n    or hours, depending on the length of the duration.\n\n    Args:\n        start (datetime): The starting datetime.\n        end (datetime): The ending datetime.\n\n    Returns:\n        str: A formatted string representing the duration.\n             - For durations less than 1 second: \"Xms\" (milliseconds)\n             - For durations between 1 second and 1 minute: \"Xs\" (seconds)\n             - For durations between 1 minute and 1 hour: \"Xm\" (minutes)\n             - For durations of 1 hour or more: \"Xh\" (hours)\n\n    Examples:\n        &gt;&gt;&gt; from datetime import datetime, timedelta\n        &gt;&gt;&gt; start = datetime(2023, 1, 1, 12, 0, 0)\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(milliseconds=500)))\n        500ms\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(seconds=45)))\n        45.0s\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(minutes=30)))\n        30.0m\n        &gt;&gt;&gt; print(format_duration(start, start + timedelta(hours=2)))\n        2.0h\n    \"\"\"\n    delta = end - start\n    total_seconds = delta.total_seconds()\n\n    if total_seconds &lt; 1:\n        return f\"{total_seconds * 1000:.0f}ms\"\n    elif total_seconds &lt; 60:\n        return f\"{round(total_seconds, 1)}s\"\n    elif total_seconds &lt; 3600:\n        return f\"{round(total_seconds / 60, 1)}m\"\n    else:\n        return f\"{round(total_seconds / 3600, 1)}h\"\n</code></pre>"},{"location":"dynamiq/utils/env/","title":"Env","text":""},{"location":"dynamiq/utils/env/#dynamiq.utils.env.get_env_var","title":"<code>get_env_var(var_name, default_value=None)</code>","text":"<p>Retrieves the value of an environment variable.</p> <p>This function attempts to retrieve the value of the specified environment variable. If the variable is not found and no default value is provided, it raises a ValueError.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>The name of the environment variable to retrieve.</p> required <code>default_value</code> <code>str</code> <p>The default value to return if the environment variable is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The value of the environment variable.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the environment variable is not found and no default value is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_env_var(\"HOME\")\n'/home/user'\n&gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\", \"default\")\n'default'\n&gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\")\nTraceback (most recent call last):\n    ...\nValueError: Environment variable 'NONEXISTENT_VAR' not found.\n</code></pre> Source code in <code>dynamiq/utils/env.py</code> <pre><code>def get_env_var(var_name: str, default_value: Any = None):\n    \"\"\"Retrieves the value of an environment variable.\n\n    This function attempts to retrieve the value of the specified environment variable. If the\n    variable is not found and no default value is provided, it raises a ValueError.\n\n    Args:\n        var_name (str): The name of the environment variable to retrieve.\n        default_value (str, optional): The default value to return if the environment variable\n            is not found. Defaults to None.\n\n    Returns:\n        str: The value of the environment variable.\n\n    Raises:\n        ValueError: If the environment variable is not found and no default value is provided.\n\n    Examples:\n        &gt;&gt;&gt; get_env_var(\"HOME\")\n        '/home/user'\n        &gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\", \"default\")\n        'default'\n        &gt;&gt;&gt; get_env_var(\"NONEXISTENT_VAR\")\n        Traceback (most recent call last):\n            ...\n        ValueError: Environment variable 'NONEXISTENT_VAR' not found.\n    \"\"\"\n    value = os.environ.get(var_name, default_value)\n\n    if value is None:\n        logger.warning(f\"Environment variable '{var_name}' not found\")\n\n    return value\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/","title":"Jsonpath","text":""},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.filter","title":"<code>filter(json, filter, node_id)</code>","text":"<p>Filter a JSON object based on a JSONPath expression.</p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>dict</code> <p>The input JSON object to be filtered.</p> required <code>filter</code> <code>str</code> <p>A JSONPath expression used to filter the JSON object.</p> required <code>node_id</code> <code>str</code> <p>An identifier for the current node being processed.</p> required <p>Returns:</p> Type Description <p>The filtered data, which can be a single value, a list of values, or None if no match is found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filter is not a valid JSONPath expression or if there's an error in parsing.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def filter(json: dict, filter: str, node_id: str):\n    \"\"\"\n    Filter a JSON object based on a JSONPath expression.\n\n    Args:\n        json (dict): The input JSON object to be filtered.\n        filter (str): A JSONPath expression used to filter the JSON object.\n        node_id (str): An identifier for the current node being processed.\n\n    Returns:\n        The filtered data, which can be a single value, a list of values, or None if no match is found.\n\n    Raises:\n        ValueError: If the filter is not a valid JSONPath expression or if there's an error in parsing.\n    \"\"\"\n    if not filter:\n        return json\n    if not is_jsonpath(filter):\n        raise ValueError(f\"Invalid filter of node {node_id}: filter must be a jsonpath\")\n\n    filtered_data = None\n    try:\n        value = parse(filter).find(json)\n        if value:\n            filtered_data = [v.value for v in value]\n            if len(filtered_data) == 1:\n                filtered_data = filtered_data[0]\n        else:\n            filtered_data = None\n    except Exception as e:\n        raise ValueError(f\"Error in path parsing of node {node_id}: {e}\")\n\n    return filtered_data\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.is_jsonpath","title":"<code>is_jsonpath(path)</code>","text":"<p>Check if the given string is a valid JSONPath expression.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The string to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string is a valid JSONPath expression, False otherwise.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def is_jsonpath(path: str) -&gt; bool:\n    \"\"\"\n    Check if the given string is a valid JSONPath expression.\n\n    Args:\n        path (str): The string to be checked.\n\n    Returns:\n        bool: True if the string is a valid JSONPath expression, False otherwise.\n    \"\"\"\n    try:\n        parse(path)\n        return True\n    except JsonPathParserError:\n        return False\n</code></pre>"},{"location":"dynamiq/utils/jsonpath/#dynamiq.utils.jsonpath.mapper","title":"<code>mapper(json, map, node_id)</code>","text":"<p>Map values from a JSON object or list to a new dictionary based on a mapping configuration.</p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>dict | list</code> <p>The input JSON object or list to be mapped.</p> required <code>map</code> <code>dict</code> <p>A dictionary defining the mapping configuration.</p> required <code>node_id</code> <code>str</code> <p>An identifier for the current node being processed.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A new dictionary with mapped values according to the provided configuration.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the map is not a dictionary or if the json is neither a dictionary nor a list.</p> <code>ValueError</code> <p>If there's an error in JSONPath parsing.</p> Source code in <code>dynamiq/utils/jsonpath.py</code> <pre><code>def mapper(json: dict | list, map: dict, node_id: str) -&gt; dict:\n    \"\"\"\n    Map values from a JSON object or list to a new dictionary based on a mapping configuration.\n\n    Args:\n        json (dict | list): The input JSON object or list to be mapped.\n        map (dict): A dictionary defining the mapping configuration.\n        node_id (str): An identifier for the current node being processed.\n\n    Returns:\n        dict: A new dictionary with mapped values according to the provided configuration.\n\n    Raises:\n        TypeError: If the map is not a dictionary or if the json is neither a dictionary nor a list.\n        ValueError: If there's an error in JSONPath parsing.\n    \"\"\"\n    if not map:\n        return json\n    if not isinstance(map, dict):\n        raise TypeError(f\"Invalid map of node {node_id}: map must be a dictionary\")\n    if not isinstance(json, dict) and not isinstance(json, list):\n        raise TypeError(\n            f\"Invalid json of node {node_id}: json must be a dictionary or a list\"\n        )\n\n    new_json = {}\n    for key, path in map.items():\n        if not is_jsonpath(path):\n            new_json[key] = path\n            continue\n        try:\n            found = parse(path).find(json)\n            if not found:\n                new_json[key] = None\n            elif len(found) == 1:\n                new_json[key] = found[0].value\n            else:\n                new_json[key] = [v.value for v in found]\n        except Exception as e:\n            raise ValueError(f\"Error in jsonpath parsing of node {node_id}: {e}\")\n\n    return new_json\n</code></pre>"},{"location":"dynamiq/utils/logger/","title":"Logger","text":""},{"location":"dynamiq/utils/utils/","title":"Utils","text":""},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.JsonWorkflowEncoder","title":"<code>JsonWorkflowEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>A custom JSON encoder for handling specific object types in workflow serialization.</p> <p>This encoder extends the default JSONEncoder to provide custom serialization for Enum, UUID, and datetime objects.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>class JsonWorkflowEncoder(JSONEncoder):\n    \"\"\"\n    A custom JSON encoder for handling specific object types in workflow serialization.\n\n    This encoder extends the default JSONEncoder to provide custom serialization for Enum, UUID,\n    and datetime objects.\n    \"\"\"\n\n    def default(self, obj: Any) -&gt; Any:\n        \"\"\"\n        Encode the given object into a JSON-serializable format.\n\n        Args:\n            obj (Any): The object to be encoded.\n\n        Returns:\n            Any: A JSON-serializable representation of the object.\n\n        Raises:\n            TypeError: If the object type is not handled by this encoder or the default encoder.\n        \"\"\"\n        if isinstance(obj, Enum):\n            return obj.value\n        if isinstance(obj, UUID):\n            return str(obj)\n        if isinstance(obj, (datetime, date)):\n            return obj.isoformat()\n        if isinstance(obj, (BytesIO, bytes, Exception)) or callable(obj):\n            return format_value(obj)\n        return JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.JsonWorkflowEncoder.default","title":"<code>default(obj)</code>","text":"<p>Encode the given object into a JSON-serializable format.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to be encoded.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A JSON-serializable representation of the object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object type is not handled by this encoder or the default encoder.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def default(self, obj: Any) -&gt; Any:\n    \"\"\"\n    Encode the given object into a JSON-serializable format.\n\n    Args:\n        obj (Any): The object to be encoded.\n\n    Returns:\n        Any: A JSON-serializable representation of the object.\n\n    Raises:\n        TypeError: If the object type is not handled by this encoder or the default encoder.\n    \"\"\"\n    if isinstance(obj, Enum):\n        return obj.value\n    if isinstance(obj, UUID):\n        return str(obj)\n    if isinstance(obj, (datetime, date)):\n        return obj.isoformat()\n    if isinstance(obj, (BytesIO, bytes, Exception)) or callable(obj):\n        return format_value(obj)\n    return JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.encode_bytes","title":"<code>encode_bytes(value)</code>","text":"<p>Encode a bytes object to an encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The bytes object to be encoded.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>encoded string representation of the bytes object.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def encode_bytes(value: bytes) -&gt; str:\n    \"\"\"\n    Encode a bytes object to an encoded string.\n\n    Args:\n        value (bytes): The bytes object to be encoded.\n\n    Returns:\n        str: encoded string representation of the bytes object.\n    \"\"\"\n    try:\n        return value.decode()\n    except UnicodeDecodeError:\n        return base64.b64encode(value).decode()\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.format_value","title":"<code>format_value(value, skip_format_types=None, force_format_types=None, **kwargs)</code>","text":"<p>Format a value for serialization.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to format.</p> required <code>skip_format_types</code> <code>set</code> <p>Types to skip formatting.</p> <code>None</code> <code>force_format_types</code> <code>set</code> <p>Types to force formatting.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Formatted value.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def format_value(value: Any, skip_format_types: set = None, force_format_types: set = None, **kwargs) -&gt; Any:\n    \"\"\"Format a value for serialization.\n\n    Args:\n        value (Any): The value to format.\n        skip_format_types (set, optional): Types to skip formatting.\n        force_format_types (set, optional): Types to force formatting.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Any: Formatted value.\n    \"\"\"\n    from dynamiq.nodes.tools.python import PythonInputSchema\n    from dynamiq.runnables import RunnableResult\n\n    if skip_format_types is None:\n        skip_format_types = set()\n    if force_format_types is None:\n        force_format_types = set()\n\n    if not isinstance(value, tuple(force_format_types)) and isinstance(\n        value, tuple(skip_format_types)\n    ):\n        return value\n\n    if isinstance(value, BytesIO):\n        return getattr(value, \"name\", None) or encode_bytes(value.getvalue())\n    if isinstance(value, bytes):\n        return encode_bytes(value)\n    if isinstance(value, dict):\n        return {\n            k: format_value(v, skip_format_types, force_format_types)\n            for k, v in value.items()\n        }\n    if isinstance(value, (list, tuple, set)):\n        return type(value)(\n            format_value(v, skip_format_types, force_format_types) for v in value\n        )\n    if isinstance(value, (RunnableResult, PythonInputSchema)):\n        return value.to_dict(skip_format_types=skip_format_types, force_format_types=force_format_types)\n    if isinstance(value, BaseModel):\n        return value.to_dict() if hasattr(value, \"to_dict\") else value.model_dump()\n    if isinstance(value, Exception):\n        recoverable = bool(kwargs.get(\"recoverable\"))\n        return {\"content\": f\"{str(value)}\", \"error_type\": type(value).__name__, \"recoverable\": recoverable}\n    if callable(value):\n        return f\"func: {getattr(value, '__name__', str(value))}\"\n\n    try:\n        return RootModel[type(value)](value).model_dump()\n    except PydanticUserError:\n        return str(value)\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.generate_uuid","title":"<code>generate_uuid()</code>","text":"<p>Generate a UUID4 string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of a UUID4.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def generate_uuid() -&gt; str:\n    \"\"\"\n    Generate a UUID4 string.\n\n    Returns:\n        str: A string representation of a UUID4.\n    \"\"\"\n    return str(uuid4())\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.merge","title":"<code>merge(a, b)</code>","text":"<p>Merge two dictionaries or objects.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Any</code> <p>The first dictionary or object.</p> required <code>b</code> <code>Any</code> <p>The second dictionary or object.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A new dictionary containing the merged key-value pairs from both inputs.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def merge(a: Any, b: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Merge two dictionaries or objects.\n\n    Args:\n        a (Any): The first dictionary or object.\n        b (Any): The second dictionary or object.\n\n    Returns:\n        dict[str, Any]: A new dictionary containing the merged key-value pairs from both inputs.\n    \"\"\"\n    return {**a, **b}\n</code></pre>"},{"location":"dynamiq/utils/utils/#dynamiq.utils.utils.serialize","title":"<code>serialize(obj)</code>","text":"<p>Serialize an object to a JSON-compatible dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to be serialized.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary representation of the object, suitable for JSON serialization.</p> Source code in <code>dynamiq/utils/utils.py</code> <pre><code>def serialize(obj: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize an object to a JSON-compatible dictionary.\n\n    Args:\n        obj (Any): The object to be serialized.\n\n    Returns:\n        dict[str, Any]: A dictionary representation of the object, suitable for JSON serialization.\n    \"\"\"\n    import jsonpickle\n\n    return loads(jsonpickle.encode(obj, unpicklable=False))\n</code></pre>"},{"location":"dynamiq/workflow/workflow/","title":"Workflow","text":""},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow","title":"<code>Workflow</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Runnable</code></p> <p>Workflow class for managing and running workflows.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the workflow.</p> <code>flow</code> <code>BaseFlow</code> <p>The flow associated with the workflow.</p> <code>version</code> <code>str | None</code> <p>Version of the workflow.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>class Workflow(BaseModel, Runnable):\n    \"\"\"Workflow class for managing and running workflows.\n\n    Attributes:\n        id (str): Unique identifier for the workflow.\n        flow (BaseFlow): The flow associated with the workflow.\n        version (str | None): Version of the workflow.\n    \"\"\"\n    id: str = Field(default_factory=generate_uuid)\n    flow: BaseFlow = Flow()\n    version: str | None = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    @classmethod\n    def from_yaml_file(\n        cls,\n        file_path: str,\n        wf_id: str = None,\n        connection_manager: ConnectionManager | None = None,\n        init_components: bool = False,\n    ):\n        \"\"\"Load workflow from a YAML file.\n\n        Args:\n            file_path (str): Path to the YAML file.\n            wf_id (str, optional): Workflow ID. Defaults to None.\n            connection_manager (ConnectionManager | None, optional): Connection manager. Defaults to None.\n            init_components (bool, optional): Whether to initialize components. Defaults to False.\n\n        Returns:\n            Workflow: Loaded workflow instance.\n        \"\"\"\n        from dynamiq.loaders.yaml import WorkflowYAMLLoader\n\n        try:\n            wf_data = WorkflowYAMLLoader.load(\n                file_path, connection_manager, init_components\n            )\n        except Exception as e:\n            logger.error(f\"Failed to load workflow from YAML. {e}\")\n            raise\n\n        return cls.from_yaml_file_data(wf_data, wf_id)\n\n    @classmethod\n    def from_yaml_file_data(\n        cls, file_data: \"WorkflowYamlLoaderData\", wf_id: str = None\n    ):\n        \"\"\"Load workflow from YAML file data.\n\n        Args:\n            file_data (WorkflowYamlLoaderData): Data loaded from the YAML file.\n            wf_id (str, optional): Workflow ID. Defaults to None.\n\n        Returns:\n            Workflow: Loaded workflow instance.\n        \"\"\"\n        try:\n            if wf_id is None:\n                if len(file_data.workflows) &gt; 1:\n                    raise ValueError(\n                        \"Multiple workflows found in YAML. Please specify 'wf_id'\"\n                    )\n                return list(file_data.workflows.values())[0]\n\n            if wf := file_data.workflows.get(wf_id):\n                return wf\n            else:\n                raise ValueError(f\"Workflow '{wf_id}' not found in YAML\")\n        except Exception as e:\n            logger.error(f\"Failed to load workflow from YAML. {e}\")\n            raise\n\n    def run(\n        self, input_data: Any, config: RunnableConfig = None, **kwargs\n    ) -&gt; RunnableResult:\n        \"\"\"Run the workflow with given input data and configuration.\n\n        Args:\n            input_data (Any): Input data for the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            RunnableResult: Result of the workflow execution.\n        \"\"\"\n        run_id = uuid4()\n        logger.info(f\"Workflow {self.id}: execution started.\")\n\n        # update kwargs with run_id\n        merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"wf_run_id\": getattr(config, \"run_id\", None)})\n        self.run_on_workflow_start(input_data, config, **merged_kwargs)\n        time_start = datetime.now()\n\n        result = self.flow.run(input_data, config, **merge(merged_kwargs, {\"parent_run_id\": run_id}))\n        if result.status == RunnableStatus.SUCCESS:\n            self.run_on_workflow_end(result.output, config, **merged_kwargs)\n            logger.info(\n                f\"Workflow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n            )\n        else:\n            self.run_on_workflow_error(result.output, config, **merged_kwargs)\n            logger.error(\n                f\"Workflow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\"\n            )\n\n        return RunnableResult(\n            status=result.status, input=input_data, output=result.output\n        )\n\n    def run_on_workflow_start(self, input_data: Any, config: RunnableConfig = None, **kwargs: Any):\n        \"\"\"Run callbacks on workflow start.\n\n        Args:\n            input_data (Any): Input data for the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                callback.on_workflow_start(self.model_dump(), input_data, **kwargs)\n\n    def run_on_workflow_end(\n        self, output: Any, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"Run callbacks on workflow end.\n\n        Args:\n            output (Any): Output data from the workflow.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                callback.on_workflow_end(self.model_dump(), output, **kwargs)\n\n    def run_on_workflow_error(\n        self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n    ):\n        \"\"\"Run callbacks on workflow error.\n\n        Args:\n            error (BaseException): The error that occurred.\n            config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        if config and config.callbacks:\n            for callback in config.callbacks:\n                callback.on_workflow_error(self.model_dump(), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.from_yaml_file","title":"<code>from_yaml_file(file_path, wf_id=None, connection_manager=None, init_components=False)</code>  <code>classmethod</code>","text":"<p>Load workflow from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the YAML file.</p> required <code>wf_id</code> <code>str</code> <p>Workflow ID. Defaults to None.</p> <code>None</code> <code>connection_manager</code> <code>ConnectionManager | None</code> <p>Connection manager. Defaults to None.</p> <code>None</code> <code>init_components</code> <code>bool</code> <p>Whether to initialize components. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Workflow</code> <p>Loaded workflow instance.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>@classmethod\ndef from_yaml_file(\n    cls,\n    file_path: str,\n    wf_id: str = None,\n    connection_manager: ConnectionManager | None = None,\n    init_components: bool = False,\n):\n    \"\"\"Load workflow from a YAML file.\n\n    Args:\n        file_path (str): Path to the YAML file.\n        wf_id (str, optional): Workflow ID. Defaults to None.\n        connection_manager (ConnectionManager | None, optional): Connection manager. Defaults to None.\n        init_components (bool, optional): Whether to initialize components. Defaults to False.\n\n    Returns:\n        Workflow: Loaded workflow instance.\n    \"\"\"\n    from dynamiq.loaders.yaml import WorkflowYAMLLoader\n\n    try:\n        wf_data = WorkflowYAMLLoader.load(\n            file_path, connection_manager, init_components\n        )\n    except Exception as e:\n        logger.error(f\"Failed to load workflow from YAML. {e}\")\n        raise\n\n    return cls.from_yaml_file_data(wf_data, wf_id)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.from_yaml_file_data","title":"<code>from_yaml_file_data(file_data, wf_id=None)</code>  <code>classmethod</code>","text":"<p>Load workflow from YAML file data.</p> <p>Parameters:</p> Name Type Description Default <code>file_data</code> <code>WorkflowYamlLoaderData</code> <p>Data loaded from the YAML file.</p> required <code>wf_id</code> <code>str</code> <p>Workflow ID. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Workflow</code> <p>Loaded workflow instance.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>@classmethod\ndef from_yaml_file_data(\n    cls, file_data: \"WorkflowYamlLoaderData\", wf_id: str = None\n):\n    \"\"\"Load workflow from YAML file data.\n\n    Args:\n        file_data (WorkflowYamlLoaderData): Data loaded from the YAML file.\n        wf_id (str, optional): Workflow ID. Defaults to None.\n\n    Returns:\n        Workflow: Loaded workflow instance.\n    \"\"\"\n    try:\n        if wf_id is None:\n            if len(file_data.workflows) &gt; 1:\n                raise ValueError(\n                    \"Multiple workflows found in YAML. Please specify 'wf_id'\"\n                )\n            return list(file_data.workflows.values())[0]\n\n        if wf := file_data.workflows.get(wf_id):\n            return wf\n        else:\n            raise ValueError(f\"Workflow '{wf_id}' not found in YAML\")\n    except Exception as e:\n        logger.error(f\"Failed to load workflow from YAML. {e}\")\n        raise\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run","title":"<code>run(input_data, config=None, **kwargs)</code>","text":"<p>Run the workflow with given input data and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RunnableResult</code> <code>RunnableResult</code> <p>Result of the workflow execution.</p> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run(\n    self, input_data: Any, config: RunnableConfig = None, **kwargs\n) -&gt; RunnableResult:\n    \"\"\"Run the workflow with given input data and configuration.\n\n    Args:\n        input_data (Any): Input data for the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RunnableResult: Result of the workflow execution.\n    \"\"\"\n    run_id = uuid4()\n    logger.info(f\"Workflow {self.id}: execution started.\")\n\n    # update kwargs with run_id\n    merged_kwargs = merge(kwargs, {\"run_id\": run_id, \"wf_run_id\": getattr(config, \"run_id\", None)})\n    self.run_on_workflow_start(input_data, config, **merged_kwargs)\n    time_start = datetime.now()\n\n    result = self.flow.run(input_data, config, **merge(merged_kwargs, {\"parent_run_id\": run_id}))\n    if result.status == RunnableStatus.SUCCESS:\n        self.run_on_workflow_end(result.output, config, **merged_kwargs)\n        logger.info(\n            f\"Workflow {self.id}: execution succeeded in {format_duration(time_start, datetime.now())}.\"\n        )\n    else:\n        self.run_on_workflow_error(result.output, config, **merged_kwargs)\n        logger.error(\n            f\"Workflow {self.id}: execution failed in {format_duration(time_start, datetime.now())}.\"\n        )\n\n    return RunnableResult(\n        status=result.status, input=input_data, output=result.output\n    )\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_on_workflow_end","title":"<code>run_on_workflow_end(output, config=None, **kwargs)</code>","text":"<p>Run callbacks on workflow end.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>Output data from the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_on_workflow_end(\n    self, output: Any, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"Run callbacks on workflow end.\n\n    Args:\n        output (Any): Output data from the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            callback.on_workflow_end(self.model_dump(), output, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_on_workflow_error","title":"<code>run_on_workflow_error(error, config=None, **kwargs)</code>","text":"<p>Run callbacks on workflow error.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_on_workflow_error(\n    self, error: BaseException, config: RunnableConfig = None, **kwargs: Any\n):\n    \"\"\"Run callbacks on workflow error.\n\n    Args:\n        error (BaseException): The error that occurred.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            callback.on_workflow_error(self.model_dump(), error, **kwargs)\n</code></pre>"},{"location":"dynamiq/workflow/workflow/#dynamiq.workflow.workflow.Workflow.run_on_workflow_start","title":"<code>run_on_workflow_start(input_data, config=None, **kwargs)</code>","text":"<p>Run callbacks on workflow start.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data for the workflow.</p> required <code>config</code> <code>RunnableConfig</code> <p>Configuration for the run. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>dynamiq/workflow/workflow.py</code> <pre><code>def run_on_workflow_start(self, input_data: Any, config: RunnableConfig = None, **kwargs: Any):\n    \"\"\"Run callbacks on workflow start.\n\n    Args:\n        input_data (Any): Input data for the workflow.\n        config (RunnableConfig, optional): Configuration for the run. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    if config and config.callbacks:\n        for callback in config.callbacks:\n            callback.on_workflow_start(self.model_dump(), input_data, **kwargs)\n</code></pre>"},{"location":"tutorials/agents/","title":"Agents Tutorial","text":""},{"location":"tutorials/agents/#simple-react-agent","title":"Simple ReAct Agent","text":"<p>An agent that has access to the E2B Code Interpreter and is capable of solving complex coding tasks.</p>"},{"location":"tutorials/agents/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection, E2B as E2BConnection\nfrom dynamiq.nodes.agents.react import ReActAgent\nfrom dynamiq.nodes.tools.e2b_sandbox import E2BInterpreterTool\n</code></pre> <p>Initialize the E2B Tool</p> <p>Set up the E2B tool with the necessary API key.</p> <pre><code>e2b_tool = E2BInterpreterTool(\n    connection=E2BConnection(api_key=\"$API_KEY\")\n)\n</code></pre> <p>Setup Your LLM</p> <p>Configure the Large Language Model (LLM) with the necessary parameters such as the model, temperature, and maximum tokens.</p> <pre><code>llm = OpenAI(\n    id=\"openai\",\n    connection=OpenAIConnection(api_key=\"$API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.3,\n    max_tokens=1000,\n)\n</code></pre> <p>Create the ReAct Agent</p> <p>Create an agent that uses the LLM and the E2B tool to solve coding tasks.</p> <pre><code>agent = ReActAgent(\n    name=\"react-agent\",\n    llm=llm,\n    tools=[e2b_tool],\n    role=\"Senior Data Scientist\",\n    max_loops=10,\n)\n</code></pre> <p>Run the Agent with an Input</p> <p>Execute the agent with a specific input task.</p> <pre><code>result = agent.run(\n    input_data={\n        \"input\": \"Add the first 10 numbers and tell if the result is prime.\",\n    }\n)\n\nprint(result.output.get(\"content\"))\n</code></pre>"},{"location":"tutorials/agents/#multi-agent-orchestration","title":"Multi-agent Orchestration","text":""},{"location":"tutorials/agents/#step-by-step-guide_1","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from dynamiq.connections import (OpenAI as OpenAIConnection,\n                                 ScaleSerp as ScaleSerpConnection,\n                                 E2B as E2BConnection)\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.nodes.agents.orchestrators.adaptive import AdaptiveOrchestrator\nfrom dynamiq.nodes.agents.orchestrators.adaptive_manager import AdaptiveAgentManager\nfrom dynamiq.nodes.agents.react import ReActAgent\nfrom dynamiq.nodes.agents.reflection import ReflectionAgent\nfrom dynamiq.nodes.tools.e2b_sandbox import E2BInterpreterTool\nfrom dynamiq.nodes.tools.scale_serp import ScaleSerpTool\n</code></pre> <p>Initialize Tools</p> <p>Set up the tools required for coding and web search tasks.</p> <pre><code>python_tool = E2BInterpreterTool(\n    connection=E2BConnection(api_key=\"$E2B_API_KEY\")\n)\nsearch_tool = ScaleSerpTool(\n    connection=ScaleSerpConnection(api_key=\"$SCALESERP_API_KEY\")\n)\n</code></pre> <p>Initialize LLM</p> <p>Configure the Large Language Model (LLM) with the necessary parameters.</p> <pre><code>llm = OpenAI(\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    temperature=0.2,\n)\n</code></pre> <p>Define Agents</p> <p>Create agents with specific roles and goals.</p> <pre><code>coding_agent = ReActAgent(\n    name=\"coding-agent\",\n    llm=llm,\n    tools=[python_tool],\n    role=(\"Expert agent with coding skills.\"\n          \"Goal is to provide the solution to the input task\"\n          \"using Python software engineering skills.\"),\n    max_loops=15,\n)\n\nplanner_agent = ReflectionAgent(\n    name=\"planner-agent\",\n    llm=llm,\n    role=(\"Expert agent with planning skills.\"\n          \"Goal is to analyze complex requests\" \n          \"and provide a detailed action plan.\"),\n)\n\nsearch_agent = ReActAgent(\n    name=\"search-agent\",\n    llm=llm,\n    tools=[search_tool],\n    role=(\"Expert agent with web search skills.\"\n          \"Goal is to provide the solution to the input task\"\n          \"using web search and summarization skills.\"),\n    max_loops=10,\n)\n</code></pre> <p>Initialize the Adaptive Agent Manager</p> <p>Set up the manager to handle the orchestration of multiple agents.</p> <pre><code>agent_manager = AdaptiveAgentManager(llm=llm)\n</code></pre> <p>Create the Orchestrator</p> <p>Create an orchestrator to manage the execution of multiple agents.</p> <pre><code>orchestrator = AdaptiveOrchestrator(\n    name=\"adaptive-orchestrator\",\n    agents=[coding_agent, planner_agent, search_agent],\n    manager=agent_manager,\n)\n</code></pre> <p>Define the Input Task</p> <p>Specify the task that the orchestrator will handle.</p> <pre><code>input_task = (\n    \"Use coding skills to gather data about Nvidia and Intel stock prices for the last 10 years, \"\n    \"calculate the average per year for each company, and create a table. Then craft a report \"\n    \"and add a conclusion: what would have been better if I had invested $100 ten years ago?\"\n)\n</code></pre> <p>Run the Orchestrator</p> <p>Execute the orchestrator with the defined input task.</p> <pre><code>result = orchestrator.run(\n    input_data={\"input\": input_task},\n)\n\n# Print the result\nprint(result.output.get(\"content\"))\n</code></pre> <p>This tutorial provides a comprehensive guide to setting up and running agents using Dynamiq. By following these steps, you can create agents capable of solving complex tasks and orchestrate multiple agents to handle more sophisticated workflows.</p>"},{"location":"tutorials/quickstart/","title":"Quickstart Tutorial","text":""},{"location":"tutorials/quickstart/#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Here's how you can get started with Dynamiq:</p>"},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<p>First, let's get Dynamiq installed. You'll need Python, so make sure that's set up on your machine. Then run:</p> <pre><code>pip install dynamiq\n</code></pre> <p>Or build the Python package from the source code:</p> <pre><code>git clone https://github.com/dynamiq-ai/dynamiq.git\ncd dynamiq\npoetry install\n</code></pre>"},{"location":"tutorials/quickstart/#examples","title":"Examples","text":""},{"location":"tutorials/quickstart/#simple-llm-flow","title":"Simple LLM Flow","text":"<p>Here's a simple example to get you started with Dynamiq:</p> <p>Import Necessary Libraries</p> <pre><code>from dynamiq.nodes.llms.openai import OpenAI\nfrom dynamiq.connections import OpenAI as OpenAIConnection\nfrom dynamiq import Workflow\nfrom dynamiq.prompts import Prompt, Message\n</code></pre> <p>Define the Prompt Template for Translation</p> <p>Create a template for the prompt that will be used to translate text into English.</p> <pre><code>prompt_template = \"\"\"\nTranslate the following text into English: {{ text }}\n\"\"\"\n</code></pre> <p>Create a Prompt Object with the Defined Template</p> <pre><code>prompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n</code></pre> <p>Setup Your LLM (Large Language Model) Node</p> <p>Configure the LLM node with the necessary parameters such as the model, temperature, and maximum tokens.</p> <pre><code>llm = OpenAI(\n    id=\"openai\",  # Unique identifier for the node\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),  # Connection using API key\n    model=\"gpt-4o\",  # Model to be used\n    temperature=0.3,  # Sampling temperature for the model\n    max_tokens=1000,  # Maximum number of tokens in the output\n    prompt=prompt  # Prompt to be used for the model\n)\n</code></pre> <p>Create a Workflow Object</p> <p>Initialize a workflow to manage the nodes and their execution.</p> <pre><code>workflow = Workflow()\n</code></pre> <p>Add the LLM Node to the Workflow</p> <p>Add the configured LLM node to the workflow.</p> <pre><code>workflow.flow.add_nodes(llm)\n</code></pre> <p>Run the Workflow with the Input Data</p> <p>Execute the workflow with the input data that needs to be translated.</p> <pre><code>result = workflow.run(\n    input_data={\n        \"text\": \"Hola Mundo!\"  # Text to be translated\n    }\n)\n</code></pre> <p>Print the Result of the Translation</p> <p>Output the result of the translation to the console.</p> <pre><code>print(result.output)\n</code></pre> <p>This tutorial provides a quick and easy way to get started with Dynamiq. By following these steps, you can set up a simple workflow to translate text using a large language model.</p>"},{"location":"tutorials/rag/","title":"RAG Tutorial","text":""},{"location":"tutorials/rag/#rag-document-indexing-flow","title":"RAG - Document Indexing Flow","text":"<p>This workflow takes input PDF files, pre-processes them, converts them to vector embeddings, and stores them in the Pinecone vector database.</p>"},{"location":"tutorials/rag/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from io import BytesIO\nfrom dynamiq import Workflow\nfrom dynamiq.nodes import InputTransformer\nfrom dynamiq.connections import OpenAI as OpenAIConnection, Pinecone as PineconeConnection\nfrom dynamiq.nodes.converters import PyPDFConverter\nfrom dynamiq.nodes.splitters.document import DocumentSplitter\nfrom dynamiq.nodes.embedders import OpenAIDocumentEmbedder\nfrom dynamiq.nodes.writers import PineconeDocumentWriter\n</code></pre> <p>Initialize the RAG Workflow</p> <pre><code>rag_wf = Workflow()\n</code></pre> <p>PyPDF Document Converter</p> <p>Convert the PDF documents into a format suitable for processing.</p> <pre><code>converter = PyPDFConverter(document_creation_mode=\"one-doc-per-page\")\nrag_wf.flow.add_nodes(converter)  # Add node to the DAG\n</code></pre> <p>Document Splitter</p> <p>Split the documents into smaller chunks for better processing.</p> <pre><code>document_splitter = DocumentSplitter(\n    split_by=\"sentence\",\n    split_length=10,\n    split_overlap=1,\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[converter.id]}.output.documents\",\n        },  # Map output of the previous node to the expected input of the current node\n    ),\n).depends_on(converter)\nrag_wf.flow.add_nodes(document_splitter)\n</code></pre> <p>OpenAI Vector Embeddings</p> <p>Convert the document chunks into vector embeddings using OpenAI.</p> <pre><code>embedder = OpenAIDocumentEmbedder(\n    connection=OpenAIConnection(api_key=\"$OPENAI_API_KEY\"),\n    model=\"text-embedding-3-small\",\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[document_splitter.id]}.output.documents\",\n        },\n    ),\n).depends_on(document_splitter)\nrag_wf.flow.add_nodes(embedder)\n</code></pre> <p>Pinecone Vector Storage</p> <p>Store the vector embeddings in the Pinecone vector database. If you already have a created index for your database, you can simply connect to it.</p> <pre><code>vector_store = PineconeDocumentWriter(\n    connection=PineconeConnection(api_key=\"$PINECONE_API_KEY\"),\n    index_name=\"default\",\n    dimension=1536,\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[embedder.id]}.output.documents\",\n        },\n    ),\n).depends_on(embedder)\nrag_wf.flow.add_nodes(vector_store)\n</code></pre> <p>If you don't have an index in the database and want to create it programmatically, you need to specify the parameter <code>create_if_not_exist=True</code> and, depending on your deployment type, specify the additional parameters needed for index creation.</p> <p>If you have a <code>serverless</code> Pinecone deployment, your vector store initialization might look like this:</p> <pre><code># Pinecone vector storage\nvector_store = (\n    PineconeDocumentWriter(\n        connection=PineconeConnection(),\n        index_name=\"quickstart\",  # your new index\n        dimension=1536,\n        create_if_not_exist=True,\n        index_type=\"serverless\",\n        cloud=\"aws\",\n        region=\"us-east-1\"\n    )\n    .inputs(documents=embedder.outputs.documents)\n    .depends_on(embedder)\n)\n</code></pre> <p>If you have a pod-based deployment, your vector store initialization could look like this:</p> <pre><code># Pinecone vector storage\nvector_store = (\n    PineconeDocumentWriter(\n        connection=PineconeConnection(),\n        index_name=\"quickstart\",  # your new index\n        dimension=1536,\n        create_if_not_exist=True,\n        index_type=\"pod\",\n        environment=\"us-west1-gcp\",\n        pod_type=\"p1.x1\",\n        pods=1\n    )\n    .inputs(documents=embedder.outputs.documents)\n    .depends_on(embedder)\n)\n</code></pre> <p>Prepare Input PDF Files</p> <p>Prepare the input PDF files for processing.</p> <pre><code>file_paths = [\"example.pdf\"]\ninput_data = {\n    \"files\": [\n        BytesIO(open(path, \"rb\").read()) for path in file_paths\n    ],\n    \"metadata\": [\n        {\"filename\": path} for path in file_paths\n    ],\n}\n</code></pre> <p>Run RAG Indexing Flow</p> <p>Execute the workflow to process and store the documents.</p> <pre><code>rag_wf.run(input_data=input_data)\n</code></pre>"},{"location":"tutorials/rag/#rag-document-retrieval-flow","title":"RAG - Document Retrieval Flow","text":"<p>This simple retrieval RAG flow searches for relevant documents and answers the original user question using the retrieved documents.</p>"},{"location":"tutorials/rag/#step-by-step-guide_1","title":"Step-by-Step Guide","text":"<p>Import Necessary Libraries</p> <pre><code>from dynamiq import Workflow\nfrom dynamiq.nodes import InputTransformer\nfrom dynamiq.connections import OpenAI as OpenAIConnection, Pinecone as PineconeConnection\nfrom dynamiq.nodes.embedders import OpenAITextEmbedder\nfrom dynamiq.nodes.retrievers import PineconeDocumentRetriever\nfrom dynamiq.nodes.llms import OpenAI\nfrom dynamiq.prompts import Message, Prompt\n</code></pre> <p>Initialize the RAG Retrieval Workflow</p> <pre><code>retrieval_wf = Workflow()\n</code></pre> <p>Shared OpenAI Connection</p> <p>Set up a shared connection to OpenAI.</p> <pre><code>openai_connection = OpenAIConnection(api_key=\"$OPENAI_API_KEY\")\n</code></pre> <p>OpenAI Text Embedder for Query Embedding</p> <p>Embed the user query into a vector format.</p> <pre><code>embedder = OpenAITextEmbedder(\n    connection=openai_connection,\n    model=\"text-embedding-3-small\",\n)\nretrieval_wf.flow.add_nodes(embedder)\n</code></pre> <p>Pinecone Document Retriever</p> <p>Retrieve relevant documents from the Pinecone vector database.</p> <pre><code>document_retriever = PineconeDocumentRetriever(\n    connection=PineconeConnection(api_key=\"$PINECONE_API_KEY\"),\n    index_name=\"default\",\n    dimension=1536,\n    top_k=5,\n    input_transformer=InputTransformer(\n        selector={\n            \"embedding\": f\"${[embedder.id]}.output.embedding\",\n        },\n    ),\n).depends_on(embedder)\nretrieval_wf.flow.add_nodes(document_retriever)\n</code></pre> <p>Define the Prompt Template</p> <p>Create a template for generating answers based on the retrieved documents.</p> <pre><code>prompt_template = \"\"\"\nPlease answer the question based on the provided context.\n\nQuestion: {{ query }}\n\nContext:\n{% for document in documents %}\n- {{ document.content }}\n{% endfor %}\n\"\"\"\n</code></pre> <p>OpenAI LLM for Answer Generation</p> <p>Generate an answer to the user query using OpenAI's language model.</p> <pre><code>prompt = Prompt(messages=[Message(content=prompt_template, role=\"user\")])\n\nanswer_generator = OpenAI(\n    connection=openai_connection,\n    model=\"gpt-4o\",\n    prompt=prompt,\n    input_transformer=InputTransformer(\n        selector={\n            \"documents\": f\"${[document_retriever.id]}.output.documents\",\n            \"query\": f\"${[embedder.id]}.output.query\",\n        },  # Take documents from the vector store node and query from the embedder\n    ),\n).depends_on([embedder, document_retriever])\nretrieval_wf.flow.add_nodes(answer_generator)\n</code></pre> <p>Run the RAG Retrieval Flow</p> <p>Execute the workflow to retrieve and answer the user query.</p> <pre><code>question = \"What are the line items provided in the invoice?\"\nresult = retrieval_wf.run(input_data={\"query\": question})\n\n# Print the answer\nanswer = result.output.get(answer_generator.id).get(\"output\", {}).get(\"content\")\nprint(answer)\n</code></pre>"}]}